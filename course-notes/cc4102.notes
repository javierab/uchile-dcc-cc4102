#+DRAWERS: PROPERTIES LOGBOOK CLOCK PROOF SOLUTION CONTEXT
#+TODO: NEXT(n) MAYB(m) TODO(t) ACTF(a) PAUS(p) WAIT(w) | DONE(d) CANC(c)
#+TAGS: CQ(C) READING(R) TALK(T)
#+LATEX_HEADER: \usepackage{fullpage}
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+OPTIONS: LaTeX:dvipng H:5 num:t
#+TITLE: CC4102/CC40A/CC53A - Diseno y Analisis de Algoritmos
#+AUTHOR: Jeremy Barbay


* Examples
** Example of Mathematical formulas seen with C-c C-x C-l 
*** Basic formulas
    $$\log_2 n$$
    $$\frac{n}{\lg n}$$
**** NEXT Find a command to put in latex header to make latex math larger
*** Formulas in array (note the misalignments :( )
    | $n$                   | $n^2$    | $\lfloor n/2 \rfloor$ |
    | $\lfloor n/2 \rfloor$ | $n\lg n$ | $n$                   |
** Example of time planification
   :PROPERTIES:
   :COLUMNS:  %80ITEM(TASK)  %5Effort(TIME){:} %5CLOCKSUM
   :Effort_ALL: 30 45 60 90 2*90 3*90 4*90
   :END:
   Use C-c C-x C-c to switch to column mode, and "q" while in a column to quit.
*** A unit
**** a topic for 1/2 talk (45mn)
     :PROPERTIES:
     :Effort:   45
     :END:
**** a topic of 3 talks
     :PROPERTIES:
     :Effort:   3*90
     :END: 

** Example of Structure of Concept Question subsection 				 
**** PREGUNTAS [0/0]						 :PREGUNTAS:
***** 
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      :END:
      
      1. [ ] 
      2. [ ] 
      3. [ ] 
      4. [ ] 
      5. [ ] otra respuesta
***** Title of the question
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:
      Text of the question
      1. [ ] Answer 1
      2. [ ] Answer 2
      3. [ ] Answer 3
      4. [ ] Answer 4
      5. [ ] none of the above


* Lecture Notes CC4102
  :CLOCK:
  CLOCK: [2011-03-08 Tue 11:18]--[2011-03-08 Tue 12:23] =>  1:05
  :END:
  :PROPERTIES:
  :ID:       8083f75f-15d1-4cf8-a992-8b787d182c26
  :COLUMNS:  %100ITEM(TASK)  %5Effort(TIME){:} %5CLOCKSUM
  :Effort_ALL: 10 20 30 45 60 90 2*90 3*90 4*90 " "
  :END:
** Presentación y Evaluación (1 charla = 90mns)
*** Presentaciones
    :PROPERTIES:
    :Effort:   10
    :END:
**** Quien somos?
     - franco-ingles-castellano
**** Quien son? 
     * Quien
	 - tomo el curso CC40A o CC4102 en los últimos semestres?
	 - toma CC53A?
       * Quien
	 - piensa seguir en la universidad después de magíster?
**** El curso
***** Contrato Alumno Profesor
****** Citacion de Randy Pausch (en ingles)

       "I’d compare college tuition to paying for a personal trainer
       at an athletic club. We professors play the roles of trainers,
       giving people access to the equipment (books, labs, our
       expertise) and after that, it is our job to be demanding. We
       need to make sure that our students are exerting themselves. We
       need to praise them when they deserve it and to tell them
       honestly when they have it in them to work harder.

       Most importantly, we need to let them know how to judge for
       themselves how they’re coming along. The great thing about
       working in a gym is that if you put in effort, you get very
       obvious results. The same should be true of college. A
       professor’s job is to teach students how to see their minds
       growing in the same way they can see their muscles grow when
       they look in the mirror."

****** Estudiando
       
       * el deber del profesor, como un ayudante en un gimnasio, es de
         ayudar el alumno a
	 - elegir el mejor camino para apprender
	 - saber cuanto progreso hizo
	 - saber cuanto efuerzo le queda para su objetivo
       * el deber del alumno es 
	 - de caminar el camino
	 - dar feedback al profesor sobre sus debilidades.



***** =Temáticas= 
    1. Conceptos básicos y complejidad (3 semanas = 6 charlas)
    2. Algoritmos y Estructuras de Datos para Memoria Secundaria (3 semanas = 6 charlas)
    3. Técnicas avanzadas de diseño y análisis de algoritmos (4 semanas = 8 charlas)
    4. Algoritmos no convencionales  (5 semanas = 10 charlas)
***** =Modo=
  - *Clases expositivas* del profesor de cátedra
    - buscando la participación de los alumnos en pequeños
      problemas que se van proponiendo durante la exposición.
  - *Clases auxiliares* dedicadas a explicar ejemplos mas
    extensos, resolver ejercicios propuestos, y preparación pre y
    post controles.
  - *Exposición* de las mejores tareas de los alumnos, como casos
    de estudio de implementación y experimentación.
***** =Evaluación=
****** 2011 con 6 tareas y 3 controles
     - [1/3] Tareas
       - [1/18] Tarea 1
       - [1/18] Tarea 2
       - [1/18] Tarea 3
       - [1/18] Tarea 4
       - [1/18] Tarea 5
       - [1/18] Tarea 6
     - [4/9] Controles
       - [1/9] Control 1 (unidad 1)
       - [2/9] Control 2 (unidades 2 y 3)
       - [1/9] Control 3 (unidad 4)
     - [2/9] Examen (todas unidades)
****** =Nota Final=      
   - controles se promedian a partes iguales
   - el examen reemplaza el peor control si la nota del examen es
     mayor.
   - tareas se promedian a partes iguales
*** Preguntas sobre la Programación				       :TALK:
    :PROPERTIES:
    :Effort:   20
    :END:
   - Cuanto memoria hay en un computador? Como se maneja?
   - Cual es la diferencia entre el disco duro y la memoria?
   - Cuanto procesadores hay en un computador? Como se programan?
   - Cual algoritmo elegir a implementar para un problema dicho?
   - Cual es la diferencia entre programación imperativa y funcional?
*** Introduciendo "Concept questions": "Hanoi Tower" y "Disk Pile"
    :LOGBOOK:
    - State "DONE"       from "NEXT"       [2011-03-03 Thu 10:17]
    :END:
    :PROPERTIES:
    :Effort:   20
    :Energy:   ZERO
    :Span:     hour
    :END:
**** Una corta historia sobre "Concept Questions" 		       :TALK:
     - Charla de Eric Mazur:
       - http://www.youtube.com/watch?v=WwslBPj8GgI
     - Grupo de Investigacion sobre "Peer Instruction"
       - http://mazur-www.harvard.edu/research/detailspage.php?rowid=8
**** Torre de Hanoi de altura 4 					 :PREGUNTAS:
     :PROPERTIES:
     :END:
     :SOLUTION: 
     none of the above
     :END:
     :PROOF: 
     - The complexity is *not* $2^{n}$ (think about the complexity to move a single disk).
     - f(1)=1; f(n+1) = 2*f(n)+1 yields
       | 1 | 2 | 3 |  4 |  5 |  6 |   7 |   8 |
       |---+---+---+----+----+----+-----+-----|
       | 1 | 3 | 7 | 15 | 31 | 63 | 127 | 253 |
     :END:
     What is the minimum number of moves required to move a Hanoi Tower of height $4$?
       1) [ ] $4$
       2) [ ] $4*\lg 4=4*2=8$
       3) [ ] $4!=4*3*2*1 = 24$
       4) [ ] $2^4 = 32$
       5) [ ] none of the above

**** Torre de Hanoi de altura 8						 :PREGUNTAS:
     :PROPERTIES:
     :END:
     :SOLUTION: 
     none of the above
     :END:
     :PROOF: 
     - The complexity is *not* $2^{n}$ (think about the complexity to move a single disk).
     - f(1)=1; f(n+1) = 2*f(n)+1 yields
       | 1 | 2 | 3 |  4 |  5 |  6 |   7 |   8 |
       |---+---+---+----+----+----+-----+-----|
       | 1 | 3 | 7 | 15 | 31 | 63 | 127 | 253 |
     :END:
     What is the minimum number of moves required to move a Hanoi Tower of height $8$?
       1) [ ] $8$
       2) [ ] $8*\lg 8=8*3=24$
       3) [ ] $2^8 = 254$
       4) [ ] $8! = 8*7*6*5*4*3*2*1 = 40320$
       5) [ ] none of the above

**** Disk Pile of height 8						 :PREGUNTAS:
     :PROPERTIES:
     :SOLUTION: 
     8   
     :END:
     :SOLUTION: 
     $8$
     :END:
     :PROOF: 
     The "minimum" can refer not only to the optimality of the
     solution, but also to the choice of the instance.  The easiest
     instance of moving a disk pile is when all the disks have the
     same size, and hence can be moved in linear time.
     :END:
     What is the minimum number of moves required to move a disk pile
     of height $8$?
       1) [ ] $8$
       2) [ ] $8*\lg 8=8*3=24$
       3) [ ] $2^8$
       4) [ ] $8!=8*7*6*5*4*3*2*1=?$
       5) [ ] none of the above

**** Disk Pile of height 8 with 2 disk sizes				 :PREGUNTAS:
     :PROPERTIES:
     :SOLUTION:     
     :END:
     :PROOF: 
     :END:
     What is the minimum number of moves required to move a disk pile
     of height $8$ in the worst case over the instances with exactly
     two distinct sizes of disc?
       1) [ ] $8$
       2) [ ] $8*\lg 8=8*3=24$
       3) [ ] $2^8$
       4) [ ] $8!=8*7*6*5*4*3*2*1=?$
       5) [ ] none of the above
*** Conceptos básicos (recuerdan CC3001)
    :PROPERTIES:
    :Effort:   20
    :END:
**** Notaciones
     - $O(), o(), \Omega(), \omega(), \Theta(), \theta()$
**** Definiciones
     - Complejidad en el peor caso
     - Complejidad en promedio
     - Otros modelos computacionales?
**** Complejidad Computacional
     - Cual Algoritmos conocen? Cual son sus complejidades?
       - para buscar en un arreglo (ordenado? no ordenado?)
       - para ordenar un arreglo (en el modelo de comparaciones o no?)
     - Cuales cotas inferiores conocen para...
       - buscar?
       - ordenar?
     - Que problemas difíciles conocen?
       - elegir sus cursos
       - asignar salas y horarios a los cursos
       - asignar enfermeros a hospitales
**** Concept Questions [0/7] 
***** TODO Asintóticas
       :LOGBOOK:
       - State "TODO"       from ""           [2011-03-10 Thu 11:55]
       :END:

| $f(n)$            | $g(n)$          | $f(n)\in O(g(n)$ | $f(n)\in \Omega(g(n)$ | $f(n)\in Theta(g(n)$ |
|-------------------+-----------------+------------------+-----------------------+----------------------|
| $3n+6$            | $100n-50$       |                  |                       |                      |
| $n^{\frac{1}{2}}$ | $n^\frac{2}{3}$ |                  |                       |                      |

***** TODO Cantidad de arboles binarios distintos con 3 nodos internos.
      :LOGBOOK:
      - State "TODO"       from ""           [2011-03-10 Thu 11:55]
      :END:
      ¿Cuántos árboles binarios distintos se pueden construir con 3 nodos internos?

      1. [ ] 1
      2. [ ] 3
      3. [ ] 4
      4. [ ] 6
      5. [ ] otra

***** TODO Arboles Binarios, nodos internos externos
      Si se define 
      - $i =$ número de nodos internos,
      - $e =$ número de nodos externos, 
      entonces se tiene que:
      :LOGBOOK:
      - State "TODO"       from ""           [2011-03-10 Thu 11:55]
      :END:
      1. [ ] $i = e$
      2. [ ] $e = i+1$
      3. [ ] $i = e+1$
      4. [ ] $e = 2^i$
      5. [ ] sin relación
***** TODO Sumas de largos de caminos en arboles.
      :LOGBOOK:
      - State "TODO"       from ""           [2011-03-10 Thu 11:55]
      :END:
Sea $n =$ número de nodos internos. Se define:
 * $I_n =$ suma del largo de los caminos desde la raíz a cada nodo
   interno (largo de caminos internos).
 * $E_n =$ suma del largo de los caminos desde la raíz a cada nodo
   externo (largo de caminos externos).
Se tiene que:
     1. [ ] $E_n = I_n$
     2. [ ] $E_n = I_n+1$
     3. [ ] $E_n = I_n+n$
     4. [ ] $E_n = I_n+2n$
     5. [ ] sin relación
***** TODO Heap
      :LOGBOOK:
      - State "TODO"       from ""           [2011-03-10 Thu 11:55]
      :END:
      La característica que permite que un heap se pueda almacenar
      sin punteros es que, si se utiliza la numeración por niveles
      indicada, entonces la(s) relación(es) entre padres e hijos es
      (son):

      1. [ ] Hijos del nodo $j = \{2*j, 2*j+1\}$
      2. [ ] Padre del nodo $k = \lfloor k/2 \rfloor$ 
      3. [ ] Hijos del nodo $j = \{2*j-1, 2*j\}$
      4. [ ] Padre del nodo $k = \lfloor k/2 \rfloor+1$ 
      5. [ ] ningunos
***** TODO altura de un AVL
   :SOLUTION:     
   en $\Theta(\lg n)$
   :END:
   :PROOF: 
   cf apuntes de CC3001. Se muestra por inducion, construiando los
   arboles AVL de altura fijada con un minimo/maximo de nodes.
   :END:
      La altura de un AVL con $n$ elementos es  
      1. [ ] $\log_\phi(n+1)+\Theta(1)$
      2. [ ] en $O(\lg n)$
      3. [ ] en $\Omega(\lg n)$
      4. [ ] en $\Theta(\lg n)$
      5. [ ] ningunos o mas que dos
***** TODO AVL h-> n
      para una altura $h$ dada, cuantos nodos tiene un árbol AVL con
      *mínimo* número de nodos que alcanza esa altura?
      1. [ ] $h$
      2. [ ] $2h$
      3. [ ] $2^h$
      4. [ ] $2^h-1$
      5. [ ] ningunas de las respuestas  previas.

*** Búsqueda y Codificación de Enteros (BONUS)
    :PROPERTIES:
    :Effort:   20
    :END:
**** Relación entre búsqueda ordenada y códigos			       :TALK:

     | Algoritmo de búsqueda      | Código por enteros |
     |----------------------------+--------------------|
     | Búsqueda secuencial        | Código Unario      |
     | Búsqueda binaria           | Codigo Binario     |
     | Búsqueda doblada           | ???                |
     | Búsqueda por interpolación | ???                |
     | ???                        | Huffman Código       |

     - A cuales algoritmos de búsqueda ordenada corresponden códigos?
     - A cuales códigos corresponden algoritmos de búsqueda?

** Conceptos básicos y complejidad (3 semanas = 6 charlas = 540mns)
*** DESCRIPCIÓN de la Unidad:
**** Resultados de Aprendizajes de la Unidad
     - Comprender el concepto de complejidad de un problema como cota inferior
     - conocer técnicas elementales para demostrar cotas inferiores
     - Conocer algunos casos de estudio relevantes
     - Adquirir nociones básicas de experimentación en algoritmos. 
**** Principales casos de estudio 
     - Cota inferior para minimo y maximo de un arreglo
     - Caso promedio del "Quicksort"
     - Cota inferior para búsqueda en un arreglo con distintas probabilidades de acceso
*** PRERREQUISITOS DE UNIDAD 
**** Lista de temas en apuntes de CC3001 			    :READING:
    http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/
*** Repaso 
**** PRERREQUISITOS
***** "Bases de la programación" en apuntes de CC3001		    :READING:
      - http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/
***** Notaciones Asintóticas					    :READING:
	- $O()$  http://es.wikipedia.org/wiki/Cota_superior_asint%C3%B3tica
	- $\Omega()$ http://es.wikipedia.org/wiki/Cota_inferior_asint%C3%B3tica
	- $\Theta()$ http://es.wikipedia.org/wiki/Cota_ajustada_asint%C3%B3tica
	- $o()$ (y $O()$) http://es.wikipedia.org/wiki/Notaci%C3%B3n_de_Landau
	- $\omega()$
	- $\theta()$
***** AVL en  apuntes de CC3001					    :READING:
      - http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Diccionario/#3
**** Problema de la Torre de Hanoi				       :TALK:
     - Definición
     - Cota superior
     - Cota inferior
**** Variantes de la Torre de Hanoi				       :TALK:
    - pesos distintos?
    - Disk Pile Problema ($n$ discos pero $h<n$ tamaños)
      - Cota inferior en función de $n$, en función de $n,h$ 
      - Cota superior en función de $n$, en función de $n,h$
    - Precise Disk Pile Problema ($n$ discos con $n_i$ de tamaño $i$, $\forall i\in[1..h]$)
      - cota inferior en función de $n$, en función de $h,n_1,...,n_h$ 
      - cota superior en función de $n$, en función de $h,n_1,...,n_h$
**** PREGUNTAS [7/7] 						 :PREGUNTAS:
***** DONE [#A] Asintóticas						 :CP:
       :LOGBOOK:
       - State "DONE"       from "TODO"       [2011-03-15 Tue 11:05]
       - State "TODO"       from ""           [2011-03-10 Thu 11:55]
       :END:
| $f(n)$            | $g(n)$          | $f(n)\in O(g(n)$ | $f(n)\in \Omega(g(n)$ | $f(n)\in Theta(g(n)$ |
|-------------------+-----------------+------------------+-----------------------+----------------------|
| $3n+6$            | $100n-50$       |                  |                       |                      |
| $n^{\frac{1}{2}}$ | $n^\frac{2}{3}$ |                  |                       |                      |
***** DONE [#A] ¿Cuántos árboles binarios distintos se pueden construir con 3 nodos internos?
      :LOGBOOK:
      - State "DONE"       from "TODO"       [2011-03-15 Tue 11:06]
      - State "TODO"       from ""           [2011-03-10 Thu 11:55]
      :END:
      1. [ ] 1
      2. [ ] 3
      3. [ ] 4
      4. [ ] 6
      5. [ ] otra
***** DONE Arboles Binarios, nodos internos externos
      Si se define i = número de nodos internos, e = número de nodos
      externos, entonces se tiene que:
      :LOGBOOK:
      - State "DONE"       from "TODO"       [2011-03-15 Tue 11:06]
      - State "TODO"       from ""           [2011-03-10 Thu 11:55]
      :END:
      1. [ ] i = e
      2. [ ] e = i+1
      3. [ ] i = e+1
      4. [ ] e = 2^i
      5. [ ] sin relación
***** DONE Sea n = número de nodos internos. Se define:
      :LOGBOOK:
      - State "DONE"       from "TODO"       [2011-03-15 Tue 11:06]
      - State "TODO"       from ""           [2011-03-10 Thu 11:55]
      :END:
   * In = suma del largo de los caminos desde la raíz a cada nodo
     interno (largo de caminos internos).
   * En = suma del largo de los caminos desde la raíz a cada nodo
     externo (largo de caminos externos).
     Se tiene que:
     1. [ ] En = In
     2. [ ] En = In+1
     3. [ ] En = In+n
     4. [ ] En = In+2n
     5. [ ] sin relación
***** DONE Heap
      :LOGBOOK:
      - State "DONE"       from "TODO"       [2011-03-15 Tue 11:06]
      - State "TODO"       from ""           [2011-03-10 Thu 11:55]
      :END:
      La característica que permite que un heap se pueda almacenar
      sin punteros es que, si se utiliza la numeración por niveles
      indicada, entonces la(s) relación(es) entre padres e hijos es
      (son):
      1. [ ] Hijos del nodo $j = \{2*j, 2*j+1\}$
      2. [ ] Padre del nodo $k = \lfloor k/2 \rfloor$ 
      3. [ ] Hijos del nodo $j = \{2*j-1, 2*j\}$
      4. [ ] Padre del nodo $k = \lfloor k/2 \rfloor +1$
      5. [ ] ningunos
***** DONE AVL
      :LOGBOOK:
      - State "DONE"       from "TODO"       [2011-03-15 Tue 11:06]
      :END:
      La altura de un AVL con $n$ elementos es  
      1. [ ] $\log_\phi(n+1)+\Theta(1)$
      2. [ ] en $O(\lg n)$
      3. [ ] en $\Omega(\lg n)$
      4. [ ] en $\Theta(\lg n)$
      5. [ ] ningunos o mas que dos
***** DONE [#A] AVL h-> n
      :SOLUTION:
      5. otra respuesta
      :END:
      :PROOF:
      (ref http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Diccionario/#3  )
      Sea $n_h$ el tamaño del árbol AVL mas pequeño de altura $h$.
      Experimentalmente, 
      | $h$   | 1 | 2 | 3 | 4 |  5 |
      |-------+---+---+---+---+----|
      | $n_h$ | 2 | 3 | 5 | 8 | 13 |
      Recursivamente, un tal árbol tiene
      - un hijo de altura $h-1$ y con $n_{h-1}$ nodos
      - un hijo de altura $h-2$ y con $n_{h-2}$ nodos 

      Así $n_h = 1+n_{h-1}+n_{h-2}$: es la forma de la secuencia de
      Fibonacci (con distintas condiciones iniciales)!  

      La solución es $n_h=\frac{\phi^{h+2}}{\sqrt{5}}$ (que no es en
      la lista).
      :END:
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-15 Tue 12:08]
      - State "NEXT"       from "TODO"       [2011-03-15 Tue 11:06]
      :END:
      para una altura $h$ dada, cuantos nodos tiene un árbol AVL con
      *mínimo* número de nodos que alcanza esa altura?
      1. [ ] $h$
      2. [ ] $2h$
      3. [ ] $2^h$
      4. [ ] $2^h-1$
      5. [ ] otra respuesta
*** Metodología de experimentación
**** PRERREQUISITOS
     - de las apuntes de CC3001:
       - Nociones básicas de programación http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Programacion/
	 (sin estudiarlas)
     - de las apuntes de CC4102:
       - 1.4 Metodología de experimentación
     - en la red (y en ingles)
       - "Research Design: Qualitative, Quantitative, and Mixed Methods Approaches" 
	 John W. Creswell
	 http://www.amazon.com/Research-Design-Qualitative-Quantitative-Approaches/
**** Al inicio, Ciencias de la computación fue solamente experimentación.
***** Turing y el código Enigma

      - el importante estaba de solucionar la instancia del día
	(romper la llave del día, basado en los mensajes de
	meteorológico, para descifrar los messages mas importantes)
	 - no mucho focus en el problema, aunque Turing si escribio
	   la definicion de la Maquina universal.

***** Alan Turing [http://www.museointernacionaldechile.cl/artistas-amistosos-de-neukolln-presentan-homenaje-a-alan-turing/ [2011-04-09 Sat] ]

Alan Turing (1912 - 1954). Matemático, informático teórico,
criptógrafo y filósofo inglés. Es considerado uno de los padres de la
ciencia de la computación, siendo el precursor de la informática
moderna. Proporcionó una influyente formalización de los conceptos de
algoritmo y computación: la máquina de Turing. Durante la Segunda
Guerra Mundial, trabajó en romper los códigosnazis, particularmente
los de la máquina Enigma; durante un tiempo fue el director de la
sección Naval Enigma del Bletchley Park. Tras la guerra diseñó uno de
los primeros computadores electrónicos programables digitales en el
Laboratorio Nacional de Física del Reino Unido y poco tiempo después
construyó otra de las primeras máquinas en la Universidad de
Manchester. Entre otras muchas cosas, también contribuyó de forma
particular e incluso provocativa al enigma de si las máquinas pueden
pensar, es decir a la Inteligencia Artificial.

La carrera de Turing terminó súbitamente cuando fue procesado por su
condición de homosexual. No se defendió de los cargos y se le dio a
escoger entre la castración química o ir a la cárcel. Eligió lo
primero y sufrió importantes consecuencias físicas, entre ellas la
impotencia. Dos años después del juicio, en 1954, se suicidó,
aparentemente tras comerse una manzana envenenada con cianuro. Su
misteriosa muerte ha dado lugar a diversas hipótesis, incluso la del
asesinato. El año 2009 el primer ministro delReino Unido, Gordon
Brown, emitió un comunicado declarando sus disculpas en nombre del
gobierno por el trato que recibió Alan Turing durante sus últimos años
de vida. Este comunicado fue consecuencia de una movilización pública
solicitando al Gobierno que pidiera disculpas oficialmente por la
persecución sufrida por Alan Turing.

***** Experimentacion basica 
      - correga hasta que fonciona (o parece foncionar)
	- correga hasta que entrega resultados correctos (o que parecen correctos)
	- mejora hasta que fonciona en tiempo razonable (en las instancias que tenemos)
***** Problemas:
      - Demasiado "Ah Hoc"
       - falta de rigor, de reproducibilidad
       - desde el inicio, no "test bed" estandard, cada uno tiene sus tests.
       - mas tarde, no estandard de maquinas 
***** Respuestas: Knuth et al. 
      - complejidad asymptotica: independancia de la maquina
      - complejidad en el peor caso y promedio: independancia del "test bed"
      - todavia es necesario de completar las estudias teoricas
	con programacion y experimentacion: el modelo teorico es
	solamente una simplificacion.
***** Theoreticos desarollaron un lado "mathematico" de ciencias
      de la computacion, con resultados importantes tal que
      - NP-hardness
     - "Polynomial Hierarchy" (http://en.wikipedia.org/wiki/Polynomial\_hierarchy)
***** Theoria y Practica se completen, pero hay conflictos en ambos lados:
      - demasiado theorias sin implementaciones (resultado del
	ambiante social tambien).
    - todavia hay estudios experimentales "no reproducibles"
**** Sobre la "buena" manera de experimentar
     ("A Theoretician's Guide to the Experimental Analysis of Algorithms", David S. Johnson, 2001)
***** Fija una hipothesis *antes* de programar.
      - aunque el objetivo sea de programar un software
	completo, solamente es necesario de implementar de
	manera eficiente la partes relevantes. El resto se puede
	implementar de manera "brutal". (E.g. "Intersection Problem")
***** "Incremental Programming"
      - busca en la red "Agile Programming", "Software Engineering".
	 - una experimentacion es tambien un proyecto de software, y
	   las tecnicas de ingeniera de software se aplican tambien.
	 - Construe un simulador en etapas, donde a cada etapa
	   fonctiona el simulador entero.	   
***** "Modular Programming"
      - Experimentacion es Investigacion, nunca se sabe por
	seguro que se va a medir despues.
	 - Hay que programar de manera modular por salgar tiempo en
	   el futuro.

**** Sobre la "buena" manera de presentar sus resultados experimentales.
     ("Presenting Data from Experiments in Algorithmics", Peter Sanders, 2002)
***** El proceso:
      * Experimentacion tiene un ciclo:
	  1. "Experimental Design" (inclue la eleccion de la hypothesis)
	  2. "Description of Measurement"
	  3. "Interpretation"
	  4. vuelve al paso 1.
	* La presentacion sigue la misma estructura, pero solamente
	  exceptionalemente describe mas que una iteracion (la
	  mejor, no necesaramiente la ultima) del ciclo.

***** Eliges que quieres comunicar.
      - el mismo dato se presenta diferamente en funcion de la
	emfasis del reporte.
       - pero, siempre la descripcion debe permitir la
	 *reproducibilidad* de la experimentacion.

***** Tables vs 2d vs 3d plot
      * tables 
	  - son faciles, y buenas para menos de 20 valores
	  - son sobre-usadas
	* Grafes 3d
	  - mas modernos, impresionantes, pero
	  - en impresion no son tan informativos
	  - tiene un futuro con interactive media donde el usuario
	    puede cambiar el punto de vista, leer las valores
	    precisas, activar o no las surfacas.
	* Grafes 2d
	  - en general preferables, pero de manera inteligente!
	  - cosas a considerar:
	    - log scale en x y/o y
	    - rango de valores en x y/o y.
	    - regla de "banking to 45 deg":
	      - "The weighted average of the slants of the line
		segments in the figure should be about 45"
	      - se puede aproximar on un grafo en "paysage"
		siguiendo el ratio de oro.
	    - factor out la informacion ya conocida 
	* Maximiza el Data-Ink ratio.

	  ``Toward an Experimental method for algorithm simulation,'' INFORMS Journal on 
	  Computing, Vol 8, No 1  Winter 1995.

	  ``Analyzing Algorithms by simulation:  Variance Reduction Techniques and 
	  Simulation Speedups,'' ACM Computing Surveys, June 1992.

	  3.  For a good book on introductory statistics with computer science
	      examples, I recommend 
    - Cohen: Empirical Methods for Artificial Intelligence
    - Thomas Bartz-Beielstein et al, on Empirical Methods for the
      Analysis of OPtimization Algorithms 
    - Catherine McGeoch, A Guide to Experimental Algorithmics, (January 2011)


**** Sobre la "buena" manera de describir una investigacion en general:
     http://www.amazon.com/Making-Sense-Students-Engineering-Technical/dp/019542591X
     Making sense; a student's guide to research and writing;
     engineering and the technical sciences, 2d ed.  Northey, Margot
     and Judi Jewinski.  Oxford U. Press 2007 252 pages $32.50
     Paperback
**** Otras referencias:
     - Research Design: Qualitative, Quantitative, and Mixed Methods Approaches 
       John W. Creswell
       http://www.amazon.com/Research-Design-Qualitative-Quantitative-Approaches/
*** Complejidad Computacional Deterministica: cotas inferiores
    adversario, teoria de la informacion, reduccion
**** PREREQUISITOS
     - de las apuntes de CC3001:
       - Ordenacion / Cotas inferiores http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Ordenacion/#1
       - Ordenacion / Merge sort http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Ordenacion/#5
     - de las apuntes de CC4102:
       - min max
**** Minimo Maximo de un  arreglo
***** PREREQUISITOS
****** Cotas Inferiores en apuntes de CC3001			    :READING:
    - http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Ordenacion/#1
***** Minimo (resp. maximo) de un arreglo
    - Cota superior
    - Cota inferior
***** Calcular el Minimo y el Maximo de un Arreglo en una sola computacion
    - Cota superior?
      - cota superior para max + cota superior para min
    - cota inferior?
      - cota inferior para min + cota superior para max?
      - min(cota inferior para min , cota superior para max?)

***** Cota superior para el problema de minmax

      * Calcular el minimo con el algoritmo previo, y el maximo
       	con un algoritmo simetrico, da una complejidad de $2n-2$
       	comparaciones, que es demasiado.
      * El algoritmo siguente calcula el max y el min en
       	$\frac{3n}{2}-2$ comparaciones:
       1. Dividir $A$ en $\lfloor n/2 \rfloor$ pares (y
	  eventualemente un elemento mas, $x$).
       2. Comparar los dos elementos de cada par.
       3. Ponga los elementos superiores en el grupo $S$, y los
	  elementos inferiores en el grupo $I$.
       4. Calcula el minima $m$ del grupo $I$ con el algorimo de la
	  pregunta previa, que performa $\lfloor n/2 \rfloor-1$ comparaciones
       5. Calcula el maxima $M$ del grupo $I$ con un algoritmo
	  simetrico, con la misma complejidad.
       6. Si $n$ es par,
       	 + $m$ y $M$ son respectivamente el minimo y el maximo de $A$.
       7. Sino, si $x<m$, 
	  + $x$ y $M$ son respectivamente el minimo y el maximo de $A$.
       8. Sino, si $x>M$, 
	  + $m$ y $x$ son respectivamente el minimo y el maximo de $A$.
       9. Sino
	  + $m$ y $M$ son respectivamente el minimo y el maximo de $A$.
      * La complejidad total del algoritmo es
       -  $n/2+2(n/2-1)=3n/2-2\in 3n/2 + O(1)$ si $n$ es par
       -  $(n-1)/2 + 2(n-1)/2 +2 = 3n/2 + 1/2\in 3n/2 + O(1)$ si
	  $n$ es impar.
       -  en la clase $3n/2 + O(1)$ en ambos casos.

***** Cota inferior para el problema de minmax
      * Sean las variables siguentes:
       	- $O$ los $o$ elementos todavia no comparados;
       	- $G$ los $g$ elementos que ``ganaron'' todas sus comparaciones hasta ahora;
       	- $P$ los $p$ elementos que ``perdieron'' todas sus comparaciones hasta ahora;
       	- $E$ las $e$ valores eliminadas (que perdieron al menos una comparacion, y ganaron al menos una comparacion);
      -  $(o,g,p,e)$ describe el estado de cualquier algoritmo:
       	-  siempre $o+g+p+e=n$;
       	-  al inicio, $g=p=e=0$ y $o=n$;
       	-  al final, $o=0$, $g=p=1$, y $e=n-2$.
      -  Despues una comparacion $a?b$ en cualquier algoritmo del
       	 modelo de comparacion, $(o,g,p,e)$ cambia en funcion del
       	 resultado de la comparacion de la manera siguente:
       	 $$
       	 \begin{array}{c|c|c|c|c}
       	 & a\in O        & a\in G               & a\in P               & a\in E      \\ 
       	 \hline %--------------------------------------
       	 b\in O & o-2,g+1,p+1,e & o-1,p,e+1            & o-1,g,p,e+1          & o-1,g+1,p,e \\
       	 &               & o-1,g,p+1,e          & o-1,g+1,p,e          & o-1,g,p+1,e \\
       	 \hline %--------------------------------------
       	 b\in G &               & o,g-1,p,e+1          & o,g,p,e              & o,g,p,e     \\ 
       	 &               &                      & o,g-1,p-1,e+2        & o,g-1,p,e+1 \\ 
       	 \hline %--------------------------------------
       	 b\in P &               &                      & o,g,p-1,e+1          & o,g,p,e     \\ 
       	 &               &                      &                      & o,g,p-1,e+1 \\ 
       	 \hline %--------------------------------------
       	 b\in E &               &                      &                      & o,g,p,e
       	 \end{array}
       	 $$
      -  En algunas configuraciones, el cambio del vector estado depende del resultado de la comparacion: un adversario puede
       	 maximizar la complejidad del algoritmo eligando el resultado de cada comparacion. El arreglo siguente contiene en graso las
       	 opciones que maximizan la complejidad del algoritmo:
       	 $$
       	 \begin{array}{c|c|c|c|c}
       	 & a\in O        & a\in G               & a\in P               & a\in E           \\ 
       	 \hline %--------------------------------------
       	 b\in O & o-2,g+1,p+1,e & o-1,p,e+1            & o-1,g,p,e+1          & o-1,g+1,p,e      \\
       	 &               & \mathbf{o-1,g,p+1,e} & \mathbf{o-1,g+1,p,e} & o-1,g,p+1,e      \\
       	 \hline %--------------------------------------
       	 b\in G &               & o,g-1,p,e+1          & \mathbf{o,g,p,e}     & \mathbf{o,g,p,e} \\ 
       	 &               &                      & o,g-1,p-1,e+2        & o,g-1,p,e+1      \\ 
       	 \hline %--------------------------------------
       	 b\in P &               &                      & o,g,p-1,e+1          & \mathbf{o,g,p,e} \\ 
       	 &               &                      &                      & o,g,p-1,e+1      \\ 
       	 \hline %--------------------------------------
       	 b\in E &               &                      &                      & o,g,p,e
       	 \end{array}
       	 $$
      -  Con estas opciones, hay
       	-  $\lceil n/2\rceil$ transiciones de $O$ a $G\cup P$, y
       	-  $n-2$ transiciones de $G\cup P$ a $E$.
      -  Eso resulta en una complejidad en el peor caso de $\lceil
       	 3n/2 \rceil-2 \in 3n/2 +O(1)$ comparaciones.
**** Busqueda Ordenada (en el modelo de comparaciones)
       	1. Cota superior: $2\lg n$ vs $1+\lg n$
       	2. Cota inferior en el peor caso: Strategia de Adversario
	   cota inferior en el peor caso de  $1+\lg n$
       	3. Cota inferior en el caso promedio uniforme
	   - Teoria de la Informacion
	   - = Arbol de Decision
	   - cota inferior de $\lg(2n+1)$, i.e. de $1+\lceil\lg(n+1/2)\rceil$
       	4. La complejidad del problema
	   - en el peor caso es $\Theta(\lg n)$
	   - en el caso promedio es $\Theta(\lg n)$
       	5. Pregunta: en este problema las cotas inferiores en el peor
	   caso y en el mejor caso son del mismo orden. Siempre es verdad?
**** Busqueda desordenada
       1. Complejidad en el peor caso es $\Theta(n)$
       2. Complejidad en el caso promedio?
	  - cota superior
	    - Move To Front
	    - ?BONUS? Transpose
	  - cota inferior
	    - algoritmo offline, lemma del ave
	  - A VER EN CASA O TUTORIAL: Huffman?
**** Ordenamiento (en el modelo de comparaciones)
      - cota superior $O(n\lg n)$
      - cota inferior en el peor caso
	- cual tecnica?
	  - lema del ave?
	  - Strategia de Adversario?
	  - Arbol Binario de Decision
	- Resultado:
	  - $\Omega(n \lg n)$
      - cota inferior en el caso promedio
	- $\Omega(n \lg n)$
**** Lista de tecnicas para mostrar cotas inferiores
    1. lema del ave
    2. Strategia de Adversario
    3. Arbol Binario de Decision
    4. Lemma del Minimax (para complejidad en promedio y complejidad
       de algoritmos aleatroizados)
**** BONUS: complejidad en promedio y aleatorizada
     - La relacion entre 
       - complejidad en promedio de un algoritmo deterministico
       - complejidad en el peor caso de un algoritmo aleatorizado
         (promedio sobre su aleatoria)


**** PREGUNTAS [6/8]						 :PREGUNTAS:
***** DONE Cota superior de (la complejidad de) Max ord
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-15 Tue 12:20]
      :END:
      Dado un arreglo ordenado de $n$ enteros, en cuanto accessos al
      arreglo pueden  calcular su valor maximal?
      1. [ ] $0$
      2. [ ] $1$
      3. [ ] $n-1$
      4. [ ] $n$
      5. [ ] otra
***** NEXT Definicion de la mediana
      Dado un arreglo de $n$ enteros, cual es la definicion correcta
      de la mediana?
      1. [ ] El promedio de las valores minima y maxima del arreglo.
      2. [ ] La valor en el centro del arreglo.
      3. [ ] La valor en el centro del arreglo ordenado.
      4. [ ] La valor superior a $\lceil(n-1)/2\rceil$ valores y inferior a $\lfloor(n-1)/2\rfloor$ valores. 
      5. [ ] otra respuesta.
***** DONE Dificultad de problemas en arreglos
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-15 Tue 13:39]
      :END:
      Dado un arreglo de $n$ enteros, cual problema requiere mas
      accessos al arreglo? Mas computacion?
      1. [ ] Calcular la valor minima
      2. [ ] Calcular la valor maxima
      3. [ ] Calcular la valor mediana
      4. [ ] Calcular la valor promedia
      5. [ ] Son todos iguales
***** DONE [#A] Cota Inferior para Max
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-15 Tue 13:39]
      - State "NEXT"       from ""           [2011-03-15 Tue 11:08]
      :END:
      Dado un arreglo de $n$ enteros, cuanto comparaciones entre los
      elementos del arreglo se necessitan para calcular su valor
      maximal?
      1. [ ] $0$
      2. [ ] $1$
      3. [ ] $n-1$
      4. [ ] $n$
      5. [ ] otra respuesta
***** DONE Definicion del problema de MinMax
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-15 Tue 13:39]
      :END:
      Dado un arreglo $A$ de $n$ enteros, cual es la definicion del
      problema de "minmax"?
      1. [ ] calcular $\min_{i\in[1..n],j\in[i..n]} A[i]$
      2. [ ] calcular $\min_{i\in[1..n]} \max{j\in[i..n]} A[i]$
      3. [ ] calcular $(\min_{i\in[1..n]} A[i], \max_{i\in[1..n]} A[i])$
      4. [ ] calcular $(\min_{i\in[1..n]} A[i], \max_{j\in[1..n]} A[j])$
      5. [ ] otra respuesta
      
***** DONE Cotas de (la complejidad de) problemas combinados
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-15 Tue 13:39]
      :END:
      Dado dos problemas $A$ y $B$ (e.g. min y max), cada uno con un
      algoritmo que le resuelve optimalemente con complejidad $f_A(n)$
      y $f_B(n)$, cual es la complejidad del problema $AB$ (e.g. min
      max)?
      1. [ ] $\min\{f_A(n)$ , f_B(n)\}$
      2. [ ] $f_A(n)$ + f_B(n)$
      3. [ ] $(f_A(n)$ + f_B(n))/2$
      4. [ ] $\max\{f_A(n)$ , f_B(n)\}$
      5. [ ] otra respuesta
***** DONE Cota *superior* de (la complejidad de) Min Max
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-15 Tue 13:39]
      :END:
   :SOLUTION:     
   $3n/2-2$ si $n$ es par, $3n/2 + 1/2$ si $n$ es impar.
   :END:
   :PROOF: 
   cf apuntes
   :END:
      Dado un arreglo de $n$ enteros, en cuanto comparaciones
      (cantidad exacta, no asimptotica) entre los elementos del
      arreglo pueden calcular su valor maximal y minimal?
      1. [ ] $n-1$
      2. [ ] $3n/2-2$ si $n$ es par, $3n/2 + 1/2$ si $n$ es impar.
      3. [ ] $(n-1)+(n-2)$
      4. [ ] $2(n-1)$
      5. [ ] otra respuesta
***** NEXT Cota *inferior* de (la complejidad de) Min Max
   :SOLUTION:     
   $\lceil 3n/2 \rceil-2$
   :END:
   :PROOF: 
   cf apuntes
   :END:
      Dado un arreglo de $n$ enteros, cuanto comparaciones (cantidad
      exacta, no asimptotica) entre los elementos del arreglo se
      necessitan para calcular su valor maximal y minimal?
      1. [ ] $n-1$
      2. [ ] $\lceil 3n/2 \rceil-2$
      3. [ ] $(n-1)+(n-2)$
      4. [ ] $2(n-1)$
      5. [ ] otra respuesta
***** DONE Juego de las preguntas, $n=4$
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-17 Thu 13:32]
      - State "NEXT"       from "CANC"       [2011-03-16 Wed 17:55]
      - State "CANC"       from "NEXT"       [2011-03-16 Wed 17:49]
      :END:
      :SOLUTION:
      $2$
      :END:
      :PROOF:
      - $x<3$?
	- $x<2$
	  - 1
	  - 2
	- $x<4
	  - 3
	  - 4
      :END:
      Cuanta preguntas (e.g. "$x<4$?", "x=2"?)  se necesitan para
      adivinar un entero entre $1$ y $4 (i.e. $x\in[1..4]$)?
      1. [ ] 1
      2. [ ] 2
      3. [ ] 3
      4. [ ] 4
      5. [ ] otra
***** DONE Juego de las preguntas, $n=1024$
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-17 Thu 13:32]
      :END:
      :SOLUTION:
      $\log 1024=10$
      :END:
      :PROOF:
      - Cada pregunta permite de reducir el tamano del domanio por dos.
      - $1024=2^10$ es divido 10 veses por $10$ antes de ser reducido a $1$
      - como sabemos que $x$ es un entero (en el rango), es suficiente
      :END:
      Cuanta preguntas (e.g. "$x<10$?", "x=10"?)  se necesitan para
      adivinar un entero entre $1$ y $1024$?
      1. [ ] 8
      2. [ ] 9
      3. [ ] 10
      4. [ ] 11
      5. [ ] otra

***** DONE Codificacion de un simbolo
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-17 Thu 13:32]
      :END:
      Dado 1 simbolo elegido a dentro de $[1..\sigma]$
      1. [ ] no se puede codificar *nunca* en $o(\lg\sigma)$ bits
      2. [X] no se puede codificar *siempre* en $o(\lg\sigma)$ bits
      3. [X] no se sabe *como codificar siempre* en $o(\lg\sigma)$ bits
      4. [ ] no se sabe *si nunca se puede codificar* en $o(\lg\sigma)$ bits
      5. [ ] otra

***** DONE Definicion de un arbol de decision
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-17 Thu 13:32]
      - State "NEXT"       from ""           [2011-03-17 Thu 11:55]
      :END:
      :SOLUTION:
      :END:
      :PROOF:
      :END:
      Un arbol de decision es definido como un arbol
      1. [ ] modelisando algoritmos en el modelo de
             comparacion.
      2. [ ] binario donde cada hoja identifica una instancia.
      3. [ ] binario donde cada nodo prueba una caracteristica de la
             instancia.
      4. [X] un arbol de grado finito donde cada hoja indica una
             decision sobre la instancia.
      5. [ ] otra.


***** DONE Codificacion de $n$ simbolos
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-17 Thu 13:32]
      :END:
      Dado $n$ simbolos elegido a dentro de un alfabeto de tamaño  $\sigma$
      1. [ ] no se puede codificar *nunca* en $o(n\lg\sigma)$ bits
      2. [X] no se puede codificar *siempre* en $o(n\lg\sigma)$ bits
      3. [X] no se sabe *como codificar siempre* en $o(n\lg\sigma)$ bits
      4. [ ] no se sabe *si nunca se puede codificar* en $o(n\lg\sigma)$ bits
      5. [ ] otra
***** DONE Definicion de "InsertionRank"
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-17 Thu 13:32]
      :END:
      Dado un arreglo ordenado $A[1..n]$ de $n$ valores y una valor
      $x$, cual(es) de estas definiciones del /Posicion de Insercion/
      ("Insertion Rank") de $x$ en $A$ son incorectas?
      ($A[0]=-\infty$ y $A[n+1]=+\infty$)
      1. [ ] la posicion en cual $x$ deberia ser insertado por dejar
             $A$ ordenado
      2. [X] el entero $p\in[1..n+1]$ tal que $A[p-1]<x \leq A[p]$
      3. [ ] el entero $p\in[0..n]$ tal que $A[p]\leq x < A[p+1]$
      4. [ ] el entero $p\in[1..n]$ tal que $x = A[p]$
      5. [ ] ningunos o mas que dos 
***** DONE Dos tipos de busqueda ordenada
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-17 Thu 13:32]
      :END:
      Dado el codigo siguente, cual es la mejor manera de completarlo
      para minimizar la complejidad (non asymptotica) en el peor caso?
      El el caso promedio?

      insertionRank(x,A,l,r) { 
        if( $r-l<2$ ) return $l$ else { m=(l+r)/2; ... }
      }
    
      1. [ ] if( $x<A[m]$ )  return insertionRank(x,A,l,m)
             else if( $x>A[m]$ )  return insertionRank(x,A,m,r)
             else if( $x=A[m]$ )  return $m$
	     endif
      2. [ ] if( $x=A[m]$ )  return $m$
             else if( $x<A[m]$ )  return insertionRank(x,A,l,m)
             else if( $x>A[m]$ )  return insertionRank(x,A,m,r)
	     endif
      3. [ ] if( $x=A[m]$ )  return $m$
             else if( $x<A[m]$ )  return insertionRank(x,A,l,m)
             else return insertionRank(x,A,m,r)
	     endif
      4. [ ] if( $x<A[m]$ )  return insertionRank(x,A,l,m)
             else return insertionRank(x,A,m,r)
	     endif
      5. [ ] performan iguales todos en el peor caso.
	     
***** DONE Cota inferior por busqueda ordenada $n=1024$.
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-17 Thu 13:32]
      :END:
      :SOLUTION:
      $1+\log 1024=11$
      :END:
      :PROOF:
      - Cada comparacion permite de reducir el tamano del domanio por dos.
      - $1024=2^10$ es divido 10 veses por $10$ antes de ser reducido a $1$
      - como NO sabemos si $x$ partenece a $A$, necesitamos una comparacion mas.
      :END:
      Dado un arreglo ordenado $A$ de $1024$ enteros y un entero $x$,
      cuanto comparaciones con elementos del arreglo son necesarias
      para decidir si $x$ pertenece a $A$ (en el peor caso)?
      1. [ ] 9
      2. [ ] 10
      3. [ ] 11
      4. [ ] 1024
      5. [ ] otra

***** NEXT Cota inferior por busqueda ordenada general $n$.
      :SOLUTION:
      :END:
      :PROOF:
      :END:
      Dado un arreglo ordenado $A$ de $n$ enteros y un entero $x$,
      cuanto comparaciones con elementos del arreglo son necesarias
      para decidir si $x$ pertenece a $A$ (en el peor caso)?
      2. [ ] $\lceil\lg n\rceil$
      2. [ ] $1+\lceil\lg n\rceil$
      3. [ ] $n-1$
      4. [ ] $n$
      5. [ ] otra

***** TODO Definicion del modelo de comparacion
      :LOGBOOK:
      - State "TODO"       from "NEXT"       [2011-03-22 Tue 12:28]
      :END:
      :SOLUTION:
      :END:
      :PROOF:
      :END:
      Cuales de estos algoritmos simples son en el modelo de
      comparacion?
      1. [ ] c=0; for(int i=1; i<n; i++) { if(A[i]>A[i+1]) c++;}
      2. [ ] for(int i=1; i<n; i++) { if(A[i]>A[i+1]) print i;}
      ; 3. [ ] for(int i=1; i<n; i++) { if(A[i]>A[i+1]) print i;}
      ; 4. [ ] for(int i=1; i<n; i++) { if(A[i]>A[i+1]) print i;}
      5. [ ] ningunos

***** DONE [#A] Relacion entre codificacion y busqueda
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-26 Sat 17:51]
      - State "NEXT"       from ""           [2011-03-17 Thu 12:01]
      :END:
   :SOLUTION:     
   A cada algoritmo de busqueda corresponde una codificacion de enteros.
   :END:
   :PROOF: 
      1. [X] A cada algoritmo de busqueda corresponde una codificacion
             de enteros. -> Considera el algoritmo de interpolacion:
             no le corresponde una codificacion de entero, al menos no
             con la reduccion vista en curso.
      2. [ ] A cada codificacion de enteros corresponde un algoritmo
             de busqueda. -> El algoritmo de decodificacion es un
             algoritmo de busqueda, preguntando a cada bit alguna
             informacion.
      3. [ ] A algunos algoritmos de busqueda corresponde una
             codificacion de enteros -> busqueda binaria y codigo
             binario es un ejemplo.
      4. [ ] A algunas codificaciones de enteros corresponde un
             algoritmo de busqueda -> codigo binario y busqueda
             binarya.
   :END:
      Cual de estas aserciones es falsa en el modelo de comparacion?
      1. [ ] A cada algoritmo de busqueda corresponde una codificacion
             de enteros.
      2. [ ] A cada codificacion de enteros corresponde un algoritmo
             de busqueda.
      3. [ ] A algunos algoritmos de busqueda corresponde una
             codificacion de enteros
      4. [ ] A algunas codificaciones de enteros corresponde un
             algoritmo de busqueda.
      5. [ ] otra
***** DONE Busqueda Doblada
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-22 Tue 12:58]
      - State "NEXT"       from ""           [2011-03-22 Tue 11:47]
      :END:
   :SOLUTION:     
   ningunas
   :END:
   :PROOF: 
      1. [ ] $\lg(1+n)$ comparaciones -> Busqueda binaria
      2. [ ] $p+1$ comparaciones -> Busqueda secuencial
      3. [ ] $2\lg p$ comparaciones -> Busqueda dublada
      3. [ ] $2\lg(n-p)$ comparaciones -> Busqueda dublada, iniciando a la derecha
  :END:
      Cual de las asercions siguentes son falsas?
      Dado una valor $x$ y un arreglo ordenado $A$ de $n$ valores,
      existe un algoritmo calculando la posicion de inserción $p$ de
      $x$ en $A$ en 
      1. [ ] $\lg(1+n)$ comparaciones
      2. [ ] $p+1$ comparaciones
      3. [ ] $2\lg p$ comparaciones
      3. [ ] $2\lg(n-p)$ comparaciones
      5. [ ] ningunas o mas que dos.
***** DONE Compression de enteros
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-22 Tue 12:59]
      - State "NEXT"       from ""           [2011-03-22 Tue 11:48]
      :END:
      Dado un entero $x\in[1..n]$, existe un esquema de
      codificacion representando $x$ con 
      1. [ ] $\lg n$ bits,
      2. [ ] $2\lg p$ bits,
      3. [ ] $p$ bits,
      4. [ ] $2\lg(n-p)$ bits,
      6. [ ] ningunas o mas que dos.
***** DONE Cota inferior ordenamiento (en el modelo de comparacion)
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-17 Thu 11:53]
      :END:
      :SOLUTION:
      :END:
      :PROOF:
      :END:
      Decir que "Ordenar es en $\Omega(n\lg n)$ (en el modelo de
      comparacion) significa que
      1. [ ] no se puede ordenar en $o(n\lg n)$  comparaciones
      2. [ ] ninguno algoritmo conocido (del modelo de comparacion)
             ordena en $o(n\lg n)$ comparaciones
      3. [ ] no se puede ordenar en tiempo $o(n\lg n)$ 
      4. [ ] ninguno algoritmo conocido (del modelo de comparacion)
             ordena en tiempo $o(n\lg n)$
      5. [ ] otra respuesta

***** NEXT Complejidad en promedio de un algoritmo
      :SOLUTION:
      Ningunas. Falta una distribucion $(p_i)_{i\in[1..2^n]}$ sobre
      las instancias legales por $A$ de tamaño $n$
      :END:
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-22 Tue 11:55]
      - State "NEXT"       from ""           [2011-03-22 Tue 11:48]
      :END:
      Dado un entero fijado $n$, un algoritmo deterministico $A$. Cual
      de estas definiciones corresponde a la complejidad en promedio
      de $A$?
      1. [ ] $\sum_{x,|x|=n} C(A,x)/n$
      2. [ ] $\sum_{x,|x|=n} C(A,x)/2^n$
      3. [ ] $\sum_{x,|x|=n} C(A,x) / \#\{x, |x|=n\}$
      4. [ ] El promedio de su complejidad sobre cada instancia.
      5. [ ] ningunas o mas de dos.
***** NEXT Complejidad en promedio de un problema
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-22 Tue 11:54]
      - State "NEXT"       from ""           [2011-03-22 Tue 11:48]
      :END:
      Dado un problema $Pb$, un entero fijado $n$, un conjunto
      $X_n=(x_i)_{i\in[1..2^n]}$ de instancias legales por $Pb$ y una
      distribucion $(p_i)_{i\in[1..2^n]}$ sobre $X_n$. La complejidad
      en promedio de $Pb$ es
      1. [ ] $\max_A \sum_i p_i C(A,x_i)$
      2. [ ] $\min_A \sum_i p_i C(A,x_i)$
      3. [ ] $\sum_i p_i \max_A C(A,x_i)$
      4. [ ] $\sum_i p_i \min_A C(A,x_i)$
      5. [ ] ningunas
***** NEXT Complejidad aleatorizada
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-22 Tue 11:48]
      :END:
      1. [ ] 
***** NEXT Relacion entre Complejidad en Promedio y en el peor caso
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-17 Thu 10:30]
      :END:
      * Nota
	- $C(A,I)$ la complejidad de un algoritmo $A$ sobre la
          instancia $I$, y
	- $E_I(C(A,I))$ la complejidad en el peor caso sobre las
          instancias de tamano $n$, y
	- $E_I(C(A,I))$ la complejidad en promedio por la distribucion
          uniforme sobre las instancias de tamano $n$.
      * Cuales de estas relaciones son verdad?
	1. [ ] $E_I(C(A,I)) \leq \max_I C(A,I)$
	2. [ ] $E_I(C(A,I)) < \max_I C(A,I)$
	3. [ ] La complejidad en el peor caso (de un algoritmo) es siempre peor que la complejidad en promedio
	4. [ ] La complejidad en promedio (de un algoritmo) nunca es peor que la complejidad en el peor caso
	5. [ ] ningunas

***** DONE Tecnicas de cotas inferiores
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2011-03-22 Tue 13:19]
      :END:
      Cual(es) de las tecnicas siguentes permitten de mostrar cotas
      inferiores para la complejidad en promedio?
	1. [ ] lemma del ave
	2. [ ] Estrategia de Adversario
	3. [ ] Arbol Binario de Decision
	4. [ ] lemma del minimax
	5. [ ] ningunas o mas de dos.
*** TODO Complejidad Computacional en Promedio: cotas inferiores
    :LOGBOOK:
    - State "TODO"       from ""           [2011-03-31 Thu 11:37]
    :END:
*** Recurrencias y Introduccion a la programacion dinamica 
**** PREREQUISITOS
     - de las apuntes de CC3001:
       - Torre de Hanoi y recursion 
	 en  http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Programacion/#6 
       - Fibonnacci
     - de las apuntes de CC4102:
       - 1.5 Recurrencias y Introduccion a la programacion dinamica 
     - en la red (y en ingles)
       - http://en.wikipedia.org/wiki/Master_theorem
       - http://www.csanimated.com/animation.php?t=Master_theorem
**** Recurencias Lineales
     - $X_n = X_{n-1} + a_n$
     - Torre de Hanoi
     - Disk Piles
     - http://en.wikipedia.org/wiki/Master_theorem
**** Recurrencias mas complejas
    - Fibonacci: 
      - $F_n = F_{n-1}+F_{n-2}$; $F_0=F_1=1$;
    - Recurrencias Lineales de orden $r$:
      - $F_n = a_1 F_{n-1} + a_2 F_{n-2} + \ldots 

**** Formulas practicas
    - Subsecuencia de suma maximal

**** Programacion Dinamica
    - Subsecuencia commun mas larga
      + solucion injenua
      + Solucion en tiempo polynomial (pero espacio $O(n^2)$)
      + Solucion en espacio lineal (y tiempo $O(n^2)$)
      + (BONUS) Solucion de Hirshberg en tiempo $O(nm)$ y espacio $\min(n,m)$
*** RESUMEN de la Unidad 1
    1. Conceptos Basicos
       - $O(), o(), \Omega(), \omega(), \Theta(), \theta()$
       - Complejidad en el peor caso, en promedio
       - Modelos computacionales:
	 - modelo de comparaciones
	 - modelo de memoria externa
    2. Tecnicas de Cotas Inferiores
       - lema del ave (reduccion)
       - strategia de adversario
       - teoria de la informacion (arbol de decision binario)
       - Analisis fine
    3. Metodologia de experimentacion
       - Porque?
       - Como hacer la experimentacion
       - Como analizar y presentar los resultados
    4. Casos de Estudios
       - Torre de Hanoi
       - "Disk Pile problem"
       - Busqueda y Codificacion de Enteros (busqueda doblada)
       - Busqueda binaria en $\Theta(1+\lg n)$ (mejor que $2\lg n$)
       - Algoritmo en $2n/3 + O(1)$ comparaciones para min max

** Memoria Secundaria [Algoritmos y Estructuras de Datos para] (3 semanas = 6 charlas = 540mns)
*** DESCRIPCION de la Unidad
    1) Modelos de Memoria
    2) Diccionarios en Memoria Secundaria
    3) Colas de Prioridades en Memoria Secundaria
    4) Ordenamiento en Memoria Secundaria
    5) Cotas Inferiores en Memoria Secundaria
*** PREREQUISITOS DE UNIDAD 
    * Apuntes de CC3001:
      - Arboles 2-3
	- http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Diccionario/#4
      - Arboles B
	- http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Diccionario/#5
    * Mergesort y Ordenamiento Externo
      - http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Ordenacion/#5
    * Colas de Prioridades
      - http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/TDA/#4
*** Modelo de computacion en memoria secundaria. Accesos secuenciales y aleatorios
**** MATERIAL A LEER
     - Memory hierarchy
       - http://en.wikipedia.org/wiki/Memory_hierarchy
       - En castellano, mas corto,
         http://es.wikipedia.org/wiki/Jerarqu%C3%ADa_de_memoria
**** APUNTES
     Arquitectura de un computador: la memoria
       1) Muchos niveles de memoria
	  - Procesador
	  - registros
	  - Cache L1
	  - Cache L2
	  - Memory
	  - Cache 
	  - Disco Duro magnetico /  Memory cell  
	  - Akamai cache
	  - Discos Duros en la red
	  - CD y DVDs tambien son "memoria"
       2) Diferencias
	  - velocidad
	  - precio de construccion
	  - relacion fisica entre volumen y velocidad
	  - volatil o no
	  - accesso arbitrario en tiempo constante o no.
	  - latencia vs debito
       3) Modelos formales
	  - RAM
	  - Jerarquia con dos niveles, paginas de tamano B
	  - Jerarquia con k niveles, de paginas de tamanos $B_1,...,B_k$
	  - "Cache oblivious" http://en.wikipedia.org/wiki/Cache-oblivious_algorithm
	  - Sistemos Operativos
**** PREGUNTAS [6/7] 						 :PREGUNTAS:
***** Cuantos segundos vale 1ns?
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-29 Tue 10:49]
      :END:
      :SOLUTION:     
      | 1 nano  | $10^{-9}$ |
      :END:
      :PROOF: 
      | 1 nano  | $10^{-9}$ |
      | 1 micro | $10^{-6}$ |
      | 1 mili  | $10^{-3}$ |
      | 1 centi | $10^{-2}$ |
      |         |           |      
      :END:
      Cuántos segundos vale un nano segundo?
      1. [ ] $10^{-12}$ segundos
      2. [ ] $10^{-9}$ segundos
      3. [ ] $10^{-6}$ segundos
      4. [ ] $10^{-3}$ segundos
      5. [ ] otra respuesta
***** Camino de acceso a valores
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-29 Tue 10:51]
      :END:
      :SOLUTION:     
      :END:
      :PROOF: 
      - Registro mas =frecuente= si sistema bien hecho
      - =probable= no bien definido, pero en todo caso un aceso a
        cualquier nivel mas bajo que Registro resulta tambien en un
        acceso a todos los niveles mas cercano del CPU, asi que
        deberia ser =otra respuesta=.
      :END:
      Cuando un programa hace un acceso a dos elementos de un arreglo,
      cual es el camino de accesso a estas valores el mas \{frecuente
      / probable \}?
      1. [ ] Registros
      2. [ ] Caches (1,2 o 3)
      3. [ ] RAM (principal)
      4. [ ] Disco Duro
      5. [ ] otra respuesta
      6. [ ] Cache + RAM + Disco Duro
      7. [ ] Cache + Disco Duro
      8. [ ] RAM + Disco Duro
***** Cual es la significacion de GHz?			       :CANC:
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-29 Tue 11:01]
      :END:
      :SOLUTION:     
      $4.10^9$ ciclos per segundas, i.e. un poco menos que $4.10^9$
      instrucciones per segundas.
      :END:
      :PROOF: 
      The hertz is equivalent to cycles per second. (... ) commonly
      used multiples are kHz (kilohertz, $10^3$ Hz), MHz (megahertz,
      $10^6$ Hz), GHz (gigahertz, $10^9$ Hz) and THz (terahertz,
      $10^12$ Hz). (...) For home-based personal computers, the CPU
      has ranged from approximately 1 megahertz in the late 1970s
      (Atari, Commodore, Apple computers) to up to 6 GHz in the
      present (IBM POWER
      processors). http://en.wikipedia.org/wiki/Hertz
      :END:
      Que significa que un procesador funciona a 4 GHz?
      1. [ ] $4$ instrucciones per segunda
      2. [ ] $4*10^3$ instrucciones per segundo
      3. [ ] $4*10^6$ instrucciones per segundo
      4. [ ] $4*10^9$ instrucciones per segundo
      5. [ ] otra respuesta
***** Nivel a 25ns
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-29 Tue 10:54]
      :END:
      :SOLUTION:     
      RAM
      :END:
      :PROOF: 
      100 ciclos = 100 * .25ns = 25 ns  => RAM
|        | Registro | L1         | L2       | L3       | RAM        | Disco         | Red           | Static   |
|--------+----------+------------+----------+----------+------------+---------------+---------------+----------|
| tiempo | 1 ciclo  | O(1) ciclo | L1*2,*10 | >L2      | 100 ciclos | $10^6$ ciclos | O(1) Segundas | sin cota |
| tamaño | 100  B   | 10 KB      | >512 KB  | >2048 KB | O(1) GB    | 200 GB        | creciendo     | sin cota |
      :END:
      Cual de los niveles siguentes parece el mas cerca de un tiempo
      de acceso de 25 ns, por un computador funcionando a 4 GHz?
      1. [ ] Registro
      2. [ ] Cache (L1, L2 o L3)
      3. [ ] RAM
      4. [ ] Disco duro
      5. [ ] otra respuesta
***** Nivel a 1ns
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-29 Tue 10:54]
      :END:
      :SOLUTION:     
      Cache (*L1*, L2 o L3)
      :END:
      :PROOF: 
      O(1) ciclos ~ 1ns
|             | Registro | L1         | L2       | L3       | RAM        | Disco         | Red           | Static   |
|-------------+----------+------------+----------+----------+------------+---------------+---------------+----------|
| tiempo      | 1 ciclo  | O(1) ciclo | L1*2,*10 | >L2      | 100 ciclos | $10^6$ ciclos | O(1) segundas | sin cota |
| orden en ns | .25 ns   | 1ns        | 5ns      | 10ns     | 25ns       | 250.000 ns    | 10^9 ns       | sin cota |
| tamaño      | 100 b    | 10 Kb      | >512 Kb  | >2048 Kb | O(1) Gb    | 200 Gb        | creciendo     | sin cota |
      :END:
      Cual de los niveles siguentes parece el mas cerca de un tiempo
      de acceso de 1 ns, por un computador funcionando a 4 GHz?
      1. [ ] Registro
      2. [ ] Cache (L1, L2 o L3)
      3. [ ] RAM
      4. [ ] Disco duro
      5. [ ] otra respuesta
***** Cuanto se demora una instruccion?
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-29 Tue 10:59]
      :END:
      :SOLUTION:     
      1 ns (.25 para un ciclo, pero algunas isntrucciones usan O(1)
      ciclos)
      :END:
      :PROOF: 
      Intel Pentiums and higher, also all modern AMD processors (x86)
      support the "rdtsc" instruction (you can call this from inline
      assembly) which returns a 64-bit count of the number of CPU
      clock cycles since powerup. The clock referred to is the CPU
      code execution cycle, e.g. *a CPU running at 1 GHz runs through
      one billion cycles per second*. (...)
      If your clock is running at 4 GHz then rdtsc measures intervals
      down to 0.25 ns. http://www.velocityreviews.com/forums/t281979-re-how-many-cpu-cycles-does-an-instruction-take.html
      :END:
      Un CPU funciona a 4 GHz: cuanto se demora una instruccion?
      (elija el valor mas cercano).
      1. [ ] 1 nano segundo
      2. [ ] 1 micro segundo
      3. [ ] 1 mili segundo
      4. [ ] 1 centi segundo
      5. [ ] otra respuesta
***** TODO Costo (en plata) de la memoria
*** Diccionarios en Memoria Externa
**** MATERIAL A LEER
     - B-Arboles:
       - Arboles-B en apuntes de CC3001
	 - http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Diccionario/#5
       - Árbol-B
	 - http://es.wikipedia.org/wiki/%C3%81rbol-B
       - Árbol-B+ (corto)
	 - http://es.wikipedia.org/wiki/%C3%81rbol-B%2B
       - Árbol-B* (corto)
	 - http://es.wikipedia.org/wiki/%C3%81rbol-B*
	 - (en ingles http://en.wikipedia.org/wiki/B*-tree )
     - Descripcion de van Emde Boas arboles
       - http://en.wikipedia.org/wiki/Van\_Emde\_Boas\_tree
**** APUNTES
    1. Cota inferior en el modelo de comparación

       - Una cota inferior trivial de la cantidad de accesos (en el
         modelo de comparacion) es $\Omega(\log_B n)$:
	 - la búsqueda (en el modelo de comparación) tiene una cota
           inferior de $\Omega(\log n)$ sobre la cantidad de
           comparaciones para un "find"
	 - una página de $B$ elementos tiene una cota *superior* de
           $\lg B$ sobre la cantidad de información que puede dar
           sobre la posición relativa de un elemento $x$.
	 - eso resulta en una cota inferior de $\Omega(\log_B n)$
           sobre la cantidad de accesos a la memoria secundaria.

    2. Cota superior en el modelo de comparación 

       Nota que la cota inferior es válida a todos niveles de memoria,
       para cualquier valor de $B$.  La cota es asintóticamente
       estricta. Se puede lograr con un $B$-arbol, conociendo $B$, o
       con un $vEB$-arbol, sin conocer $B$.

       Nota que en la sección [[*Dominios%20discretos%20y%20finitos][Dominios discretos y finitos]], veremos
       una estructura de datos con mejor rendimiento, fuera del
       modelo de comparaciones.

       1. B-arbol
	  1) $(2,3)$ Arbol: un árbol de búsqueda donde
	     - cada nodo tiene 1 o 2 llaves, que corresponde a 2 o 3
	       hijos, con la excepción de la raíz;
	     - todas las hojas están al mismo nivel (árbol completamente balanceado)
	     - Propiedades: 
	       - Cuál es la altura de un $(2,3)$-árbol?,
	       - tiempo de búsqueda?,
	       - inserción en un $(2,3)$ arbol?,
	       - eliminación en un $(2,3)$ arbol?
	  2) $(d,2d)$-Arbol es un árbol donde
	     - Cada nodo tiene de $d$ a $2d$ hijos, con la excepción de la
	       raiz (es decir, $d-1$ a $2d-1$ llaves).
	     - todas las hojas están en el mismo nivel (árbol completamente
	       balanceado)
	     - Propiedades: 
	       - Cuál es altura de un $(d,2d)$ árbol?,
	       - tiempo de búsqueda?,
	       - inserción en un $(d,2d)$ árbol?,
	       - eliminación en un $(d,2d)$ árbol?
	  3) $B$-Arbol, y variantes
	     - $B$-Arbol
	       - http://www.youtube.com/watch?v=coRJrcIYbF4
	       - http://en.wikipedia.org/wiki/B-tree
	     - $B^*$-Arbol
	       - Todos los nodos, excepto la raiz, están llenos al menos 
		 hasta $2/3$ de su capacidad (en vez de $1/2$)
	       - http://en.wikipedia.org/wiki/B*-tree
	     - $B^+$-Arbol
	       - una lista enlazada conecta a todas las hojas del árbol: permite
		 describir intervalos de soluciones.
       2. Van Emde Boas arbol (vEB)
	  1) Historia:
	     - Originalemente (1977) un estructura de datos normal, que
	       suporta todas las operaciones en $O(\lg\lg n)$, inventada
	       por el equipo de Peter van Emde Boas.
	     - No son considerados útiles en la práctica para "pequeños"
	       árboles.
	     - Aplicación a algoritmos "Cache-Oblivious" y estructuras de datos
	       - Optimiza el cache sin conocer el tamaño $B$ de sus páginas
	       - => Optimiza todos los niveles sin conocer $B_1,...,B_k$
	     - Otras aplicaciones después en cálculo paralelo (?)
	  2) Definición
	     - Cada nodo contiene un arbol van Emde Boas sobre $\sqrt{n}$ elementos
	     - $\lg\lg n$ niveles de árboles
	     - Operaciones:
	       * FindNext
	       * Insert
	       * Delete
	  3) Análisis
	     - Búsqueda en "tiempo" $O(\lg n / \lg B)$ a cualquier nivel
	       $i$, donde el tiempo es la cantidad de accesos al cache
	       del nivel considerado
	     - Cuál es el tiempo de Inserción?
	     - de eliminación?
**** PREGUNTAS [3/3] 						 :PREGUNTAS:
***** Cola de Prioridad: operaciones
      :SOLUTION:     
      Todas se pueden implementar facilamente en un dicionario, pero
      findMind() y extractMin() no son tradicionalemente consideradas
      para un diccionario, mas para una cola de prioridad.
      :END:
      :PROOF: 
      :END:
      Cuáles (no) son operaciones de un diccionario?
      1. [ ] insert(key,item)
      2. [ ] search(key)
      3. [ ] delete(key)
      4. [ ] findNext(key)
      5. [ ] findPrevious(key)
      6. [ ] findMind()
      7. [ ] extractMin()
***** $(2,3)$ arboles
      :LOGBOOK:
      :END:
      :SOLUTION:     
      \{la altura, el tiempo de busqueda, el tiempo de insercion, el
      tiempo de delecion\} son en un orden entre $\log_3 n$ y $\log_2
      n$ (ignorando los problemas de constantes).
      :END:
      :PROOF: 
      un arbol binario tiene una altura de $\log_2 n$ y un arbol
      ternario tiene una altura de $\log_3 n$. Un arbol $(2,3)$ tiene
      una altura entre los dos.
      :END:
      Cuál es el orden de \{la altura, el tiempo de búsqueda, el
      tiempo de inserción, el tiempo de eliminación\} de un $(2,3)$-árbol
      con $n$ valores?
      1. [ ] menos que $\log_3 n +O(1)$
      2. [ ] $\log_3 n +O(1)$
      3. [ ] entre  $\log_3 n$ y  $\log_2 n$
      4. [ ] $\log_2 n +O(1)$
      5. [ ] otra respuesta
***** $(2,3)$ arboles con $n\in\{8,9,256\}$ elementos		       :CANC:
      :LOGBOOK:
      :END:
      :SOLUTION:     
      \{la altura, el tiempo de busqueda, el tiempo de insercion, el
      tiempo de delecion\} son en 
      :END:
      :PROOF: 
      :END:
      Cuál es \{la altura, el tiempo de búsqueda, el tiempo de
      inserción, el tiempo de eliminación\} de un $(2,3)$-árbol con
      $n\in\{8,9,256\}$ valores?
      1. [ ] menos que $\log_3 n +O(1)$
      2. [ ] $\log_3 n +O(1)$
      3. [ ] entre  $\log_3 n$ y  $\log_2 n$
      4. [ ] $\log_2 n +O(1)$
      5. [ ] otra respuesta
***** $B$ arboles vs $(2,3)$ arboles
      :LOGBOOK:
      :END:
      :SOLUTION:     
      Menos que $\log_3 n +O(1)$, por $B>3$, que es verdad en todas
      aplicaciones.
      :END:
      Cuál es \{la altura, el tiempo de búsqueda, el tiempo de
      inserción, el tiempo de eliminación\} de un $B$-árbol con
      $n=\{8,9,256\}$ valores?
      1. [ ] menos que $\log_3 n +O(1)$
      2. [ ] $\log_3 n +O(1)$
      3. [ ] entre  $\log_3 n$ y  $\log_2 n$
      4. [ ] $\log_2 n +O(1)$
      5. [ ] otra respuesta
***** Altura de $B$ arboles
      :LOGBOOK:
      - State "NEXT"       from "DONE"       [2011-03-30 Wed 22:09]
      :END:
      :SOLUTION:     
      Menos que $\log_3 n +O(1)$, por $B>3$, que es verdad en todas
      aplicaciones.
      :END:
      Cuál es \{la altura, el tiempo de búsqueda, el tiempo de
      inserción, el tiempo de eliminación\} de un $B$-Arbol sobre
      $n=\{8,9,256\}$ valores, si cada nodo contiene $B$ valores?
      1. [ ] $n/B$
      2. [ ] $\lg n / \lg B$
      3. [ ] $\log_B n$
      4. [ ] $\log_n B$
      5. [ ] otra respuesta
***** Relacion hijos/llaves en la raiz de un $B$ arboles
      :LOGBOOK:
      :END:
      Si un nodo de un $B$ arbol tiene $d$ llaves, cuántos hijos
      tiene?
      1. [ ] $d-1$
      2. [ ] $d$
      3. [ ] $d+1$
      4. [ ] $2d+1$
      5. [ ] otra respuesta
***** [#A] Cantidad de llaves en un nodo de $B$ arbol (Part 1)
      :LOGBOOK:
      :END:
      :SOLUTION:
      $d \in [1,B]$ si ignoramos el arbol vacillo (y contando las
      hojas vacillas como hijos pero no como nodos)
      :END:
      :PROOF:
      el nodo puede ser la raiz, que puede tener menos que $B/2$
      elementos.
      :END:
      :CONTEXT:
      Una pagina de la memoria secundaria puede tener $B$ valores
      juntas con $B+1$ punteros.  
      :END:
      Cuántos hijos ($d$) puede tener un $B$ arbol sobre $n>>B$
      elementos?
      1. [ ] $d \in [0,B/2]$
      2. [ ] $d \in [1,B/2]$
      3. [ ] $d \in [0,B]$
      4. [ ] $d \in [1,B]$
      5. [ ] $d \in [B/2,B]$
      6. [ ] otra respuesta
***** Cantidad de llaves en un nodo de $B$ arbol (Part 2)
      :LOGBOOK:
      :END:
      :SOLUTION:
      $d \in [1,B]$
      :END:
      :CONTEXT:
      Una pagina de la memoria secundaria puede tener $B$ valores
      juntas con $B+1$ punteros.  
      :END:
      Una página de la memoria secundaria puede tener $B$ valores
      juntos con $B+1$ punteros. La *raíz* de un $B$-Arbol sobre $n>>B$
      elementos tiene $d$ hijos. Cuál es el dominio de valores
      posibles por $d$?
      1. [ ] $d \in [0,B/2]$
      2. [ ] $d \in [1,B/2]$
      3. [ ] $d \in [0,B]$
      4. [ ] $d \in [1,B]$
      5. [ ] $d \in [B/2,B]$
      6. [ ] otra respuesta
***** Cantidad de llaves en un nodo de $B$ arbol (Part 3)
      :LOGBOOK:
      :END:
      :SOLUTION:
      $d \in [B/2,B]$
      :END:
      :PROOF:
      si el nodo no esta la raiz, el debe tener entre $B/2$ y $B$
      valores.
      :END:
      :CONTEXT:
      Una pagina de la memoria secundaria puede tener $B$ valores
      juntas con $B+1$ punteros.  
      :END:
      Un nodo (*distinto de la raiz*) en un $B$-Arbol sobre $n>>B$ elementos tiene $d$
      hijos. Cual es el dominio de valores posibles por $d$?
      1. [ ] $d \in [0,B/2]$
      2. [ ] $d \in [1,B/2]$
      3. [ ] $d \in [0,B]$
      4. [ ] $d \in [1,B]$
      5. [ ] $d \in [B/2,B]$
      6. [ ] otra respuesta
***** $B^*$ arboles						       
      :LOGBOOK:
      :END:
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      Un $B^*$ arbol llena sus nodos hasta $2/3$, en vez de $1/2$ por
      los $B$ arboles.
      :END:
      Cuál es el objetivo de un $B^*$-Arbol (en comparación con un $B$-Arbol)? 
      1. [ ] Reducir el tiempo de búsqueda?
      2. [ ] Reducir la cantidad de accesos al cache en búsqueda?
      3. [ ] Reducir la complejidad espacial?
      4. [ ] Reducir la frecuencia de "Split/Merge"?
      5. [ ] Practical [e.g. Optimizacion para data-set (de injeniero)]
      6. [ ] otra respuesta
***** $B^+$ arboles
      :SOLUTION:     
      Suportar otro tipos de consultas/busquedas?
      :END:
      :PROOF: 
      Suportar otro tipos de consultas/busquedas, tal que busqueda de
      valores en un interval: se busca para el valor minimal y
      maximal, y se returna el interval en la caldena.
      :END:
      :CONTEXT:
      En $B^+$ arboles, las hojas tienen punteros adicionales formando
      una caldena de todas las hojas. 
      :END:
      Cuál es el objetivo de un $B^+$-Arbol (en comparación con un $B$-Arbol)? 
      1. [ ] Optimizar la Busqueda Secuencial (adaptativa)?
      2. [ ] Suportar otro tipos de consultas/búsquedas?
      3. [ ] Suportar la exportación de los valores en tiempo razonable?
      4. [ ] Practical (e.g. facilitar el back-up de base de datos)?
      5. [ ] otra respuesta
***** [#A] vEB arboles vs AVL arboles, $(2,3)$ arboles y AVL Arboles   :CANC:
      :CONTEXT:
      Fija $k$, $m=2^k$, $M=2^m$.  Un vEB arbol de $n$ elementos sobre
      $[0..M-1]$ tiene una raiz con
      - $\sqrt{M}$ punteros a sus ninos $C[0..\sqrt{M}-1]$,
      - dos valores $min$ y $max$
      - un otro vEB $aux$ sobre $[0..\sqrt{M}-1]$
      :END: 
      Que *no* distingue los vEB arboles de las otras estructuras de
      arboles que conocen (e.g. B-arboles) para el ADT diccionario?
      1. [ ] usa el dominio de los valores *para buscar*
      2. [ ] el nodo contiene los elementos extremos (no medios como en un AVL)
      3. [ ] supporta /FindNext/ y /FindPrev/
      4. [ ] sirven para colas de prioridades también
      5. [ ] petmiten de optimizar mejor la memoria
      6. [ ] otra respuesta
***** Altura de un vEB arbol
      :SOLUTION:     
      $\lg m$, que es al maximo $\lg\lg n$.
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      Fija $k$, $m=2^k$, $M=2^m$.  Un vEB arbol de $n$ elementos sobre
      $[0..M-1]$ tiene una raiz con
      - $\sqrt{M}$ punteros a sus ninos $C[0..\sqrt{M}-1]$,
      - dos valores $min$ y $max$
      - un otro vEB $aux$ sobre $[0..\sqrt{M}-1]$
      :END: 
      En cual clase asintótica está \{ el tiempo de búsqueda, de
      inserción, de eliminación y la altura \} de un vEB con $n$ valores
      codificadas en $m$ bits?
      1. [ ] $O(\lg n)$
      2. [ ] $O(\lg m)$
      3. [ ] $O(\lg\lg n)$
      4. [ ] $O(\lg\lg m)$
      5. [ ] otra respuesta
***** vEB children
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-30 Wed 21:55]
      :END:
      :SOLUTION:     
      $x \over 2^{m/2}$
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      Fija $k$, $m=2^k$, $M=2^m$.  Un vEB arbol de $n$ elementos sobre
      $[0..M-1]$ tiene una raiz con
      - $\sqrt{M}$ punteros a sus ninos $C[0..\sqrt{M}-1]$,
      - dos valores $min$ y $max$
      - un otro vEB $aux$ sobre $[0..\sqrt{M}-1]$
      :END: 
      El valor $x\in]min,max[$ se encuentra en el hijo $C[i]$ donde $i=$
      1. [ ] $2^{m/2} (max - x) \over (max - min)$ 
      2. [ ] $2^{m-2} (max - x) \over (max - min)$ 
      3. [ ] $x \over 2^{m/2}$
      4. [ ] $x \over 2^{m-2}$
      5. [ ] otra respuesta
***** vEB aux
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-30 Wed 21:55]
      :END:
      :SOLUTION:     
      Find Next
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      Fija $k$, $m=2^k$, $M=2^m$.  Un vEB arbol de $n$ elementos sobre
      $[0..M-1]$ tiene una raiz con
      - $\sqrt{M}$ punteros a sus ninos $C[0..\sqrt{M}-1]$,
      - dos valores $min$ y $max$
      - un otro vEB $aux$ sobre $[0..\sqrt{M}-1]$
      :END: 
      El rol de $aux$ (o $summary$, como fue visto en la auxiliar) es de 
      memorizar cuales hijos están vacíos.  $j\in aux$ si y sólo si
      $T.C[j]$ es no vacío.  Cuál es el principal objetivo de esto?
      1. [ ] Optimizar Find
      2. [ ] Optimizar Insert
      3. [ ] Optimizar LookUp
      4. [ ] Optimizar FindNext
      5. [ ] otra respuesta
***** vEB Find Previous
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-30 Wed 21:55]
      :END:
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      Fija $k$, $m=2^k$, $M=2^m$.  Un vEB arbol de $n$ elementos sobre
      $[0..M-1]$ tiene una raiz con
      - $\sqrt{M}$ punteros a sus ninos $C[0..\sqrt{M}-1]$,
      - dos valores $min$ y $max$
      - un otro vEB $aux$ sobre $[0..\sqrt{M}-1]$
      :END: 
      La complejidade de Find Previous está en 
      1. [ ] $O(k)$
      2. [ ] $O(2^k=m)$
      3. [ ] $O(2^{2^{k-1}})=\sqrt{M}$
      4. [ ] $O(2^{2^k})=M$
      5. [ ] $O((\lg\lg M)^2))$
      6. [ ] otra respuesta
***** vEB Insercion
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-30 Wed 21:55]
      :END:
      :SOLUTION:     
      Genera un error
      :END:
      :PROOF: 
      Genera un error: entrada doblada, $x$ tiene que ser a dentro del
      arbol si el es lleno.
      :END:
      :CONTEXT:
      Fija $k$, $m=2^k$, $M=2^m$.  Un vEB arbol de $n$ elementos sobre
      $[0..M-1]$ tiene una raiz con
      - $\sqrt{M}$ punteros a sus ninos $C[0..\sqrt{M}-1]$,
      - dos valores $min$ y $max$
      - un otro vEB $aux$ sobre $[0..\sqrt{M}-1]$
      :END: 
      Si un hijo $C[i]$ está lleno antes de agregar un elemento $x$ adentro.
      1. [ ] Split $C[i]$ en dos
      2. [ ] Mudar algunos elementos de $C[i]$ a sus vecinos, y si no
             se puede a su padre, recursivamente
      3. [ ] Crea un nuevo sobre árbol con una hoja.
      4. [ ] Genera un error
      5. [ ] otra respuesta
*** Colas de prioridad en memoria secundaria. Cotas inferiores.
**** MATERIAL a LEER
     - Apuntes CC3001
       - http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/TDA/#4 (last accessed on [2011-04-13 Wed])
     - Colas de Prioridad
       - http://en.wikipedia.org/wiki/Priority\_queue (last accessed on [2011-04-13 Wed])
     - Heaps
       - http://en.wikipedia.org/wiki/Heap\_data\_structure (last accessed on [2011-04-13 Wed])
     - B-Heap
       - http://en.wikipedia.org/wiki/B-heap (last accessed on [2011-04-13 Wed])
     - van Emde Boas Queues
       - http://en.wikipedia.org/wiki/Van_Emde_Boas_priority_queue (last accessed on [2011-04-13 Wed])
       - http://www.itl.nist.gov/div897/sqg/dads/HTML/vanemdeboas.html (broken on [2011-04-13 Wed]?)
**** APUNTES
    1) Colas de Prioridades tradicional: 
       - que se necesita?
	 * Operadores 
 	   - /insert(key,item)/
	   - /findMind()/
	   - /extractMin()/
	 * Operadores opcionales
	   - /heapify/
	   - /increaseKey, decreaseKey/
	   - /find/
	   - /delete/
	   - /successor/predecessor/
	   - /merge/
	   - ...
       - diccionarios: demasiado espacio para que se pide
	 - menos operadores que diccionarios 
	   - => mas flexibilidad en la representación
	   - => mejor tiempo y/o espacio
       - *binary heap*: una estructura dentro de muchas otras:
	 - sequence-heaps
	 - binomial queues
	 - Fibonacci heaps
	 - leftist heaps
	 - min-max heaps
	 - pairing heaps
	 - skew heaps
	 - /van Emde Boas queues/

       - *van Emde Boas queues*
	 - Definición: 

	   "An efficient implementation of priority queues where
	   insert, delete, get minimum, get maximum, etc. take O(log
	   log N) time, where N is the total possible number of
	   keys. Depending on the circumstance, the implementation
	   is null (if the queue is empty), an integer (if the queue
	   has one integer), a bit vector of size N (if N is small),
	   or a special data structure: an array of priority queues,
	   called the bottom queues, and one more priority queue of
	   array indexes of the bottom queues."

	   - rendimiento en memoria secundaria de "binary heap": Muy malo?

    2) Colas de Prioridades en Memoria Secundaria: diseño  
       * El equivalente de B-Arbol
       * Muchas alternativas en la practica:
	 - Buffer trees
	 - M/B-ary heaps
	 - array heaps
	 - R-Heaps
	 - Array Heaps
	 - sequence heaps

    3) Colas de Prioridades en Memoria Secundaria: cota inferior?
       - Cota inferior para diccionarios es una cota inferior por
	 colas de prioridades o no?
	 - No. La reducción es en la otra dirección: una cota inferior
           por colas de prioridad impliqua una cota inferior por
           diccionarios.
       - Cual es la cota inferior más simple que se puede imaginar?
	 - $\Omega(n/B)$
       - La cota superior de $O(\log_B n)$ que da una estructura de
         diccionario, con un $B$-heap es optima o no?
	 - en el modelo de comparacion, se puede mostrar una cota
           inferior de $\Omega(\log_B n)$

**** PREGUNTAS [3/10] 						 :PREGUNTAS:
***** Cola de Prioridad: operaciones
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-30 Wed 22:49]
      :END:
      :SOLUTION:     
      1. [ ] insert(key,item)
      2. [X] search(key)
      3. [X] delete(key)
      4. [X] findNext(key)
      5. [X] findPrevious(key)
      6. [ ] findMind()
      7. [ ] extractMin()      
      :END:
      :PROOF: 
      search(key) delete(key) findNext(key) findPrevious(key) no son
      en el ADT de colas de prioridades.
      :END:
      Cuales (no) son operaciones de una (min) cola de prioridad?
      1. [ ] insert(key,item)
      2. [ ] search(key)
      3. [ ] delete(key)
      4. [ ] findNext(key)
      5. [ ] findPrevious(key)
      6. [ ] findMin()
      7. [ ] extractMin()
***** Colas de Prioridades contra Diccionarios
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-30 Wed 22:49]
      :END:
      :SOLUTION:     
      No son verdades las proposiciones siguientes:
      - $C$ toma menos espacio que $D$
      - $D$ es asintóticamente mas rápido que $C$
      :END:
      :PROOF: 
      Porque no se sabe nada de las complejidad de $C$ y $D$.
      :END:
      Dado estructuras de datos $C$ y $D$, respectivamente
      implementando los ADT "cola de prioridad" y "diccionario". 
      Cual(es) de estas proposiciones tiene(n) problemas?
      1. [ ] $C$ implementa el ADT "diccionario" también.
      2. [ ] $D$ implementa el ADT "cola de prioridad" también.
      3. [ ] $C$ toma menos espacio que $D$
      4. [ ] $D$ es asintóticamente mas rápido que $C$ (en los
         operadores que tienen en común)
      5. [ ] ninguna
***** Colas de Prioridades contra Diccionarios
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-30 Wed 22:49]
      :END:
      :SOLUTION:     
      1. [ ] $C$ implementa el ADT "diccionario" también, pero en malo *tiempo*.
      2. [X] $D$ implementa el ADT "cola de prioridad" también, pero en malo *tiempo*.
      3. [X] $C$ implementa el ADT "diccionario" también, pero en malo *espacio*.
      4. [ ] $D$ implementa el ADT "cola de prioridad" también, pero en malo *espacio*.
      5. [X] $C$ implementa el ADT "diccionario" también, pero en malo *tiempo y espacio*.
      6. [X] $D$ implementa el ADT "cola de prioridad" también, pero en malo *tiempo y espacio*.
      :END:
      :PROOF: 
      :END:
      Considera las estructuras de datos Heap $C$ y AVL-árbol $D$,
      respectivamente implementando los ADT "cola de prioridad" y
      "diccionario".  Cual(es) de estas proposiciones tiene(n)
      problemas?
      1. [ ] $C$ implementa el ADT "diccionario" también, pero en malo *tiempo*.
      2. [ ] $D$ implementa el ADT "cola de prioridad" también, pero en malo *tiempo*.
      3. [ ] $C$ implementa el ADT "diccionario" también, pero en malo *espacio*.
      4. [ ] $D$ implementa el ADT "cola de prioridad" también, pero en malo *espacio*.
      5. [ ] $C$ implementa el ADT "diccionario" también, pero en malo *tiempo y espacio*.
      6. [ ] $D$ implementa el ADT "cola de prioridad" también, pero en malo *tiempo y espacio*.
      7. [ ] otra respuesta
***** Cola de Prioridad: Heapify
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-30 Wed 22:49]
      :END:
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:
      El operador "Heapify" 
      1. [ ] es parte del ADT "colas de Prioridad"
      2. [ ] es parte de la estructura de datos "Heap"
      3. [ ] tiene complejidad $O(\lg n)$
      3. [ ] tiene complejidad $O(n)$
      3. [ ] tiene complejidad $O(n\lg n)$
      5. [ ] otra respuesta
***** Estructuras de datos "Cola de Prioridad"
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-30 Wed 22:50]
      :END:
       Cuales estructuras de datos "Cola de Prioridad" conocen?
         1. [ ] *binary heap*
	 2. [ ] sequence-heaps
	 3. [ ] /binomial queues/
	 4. [ ] /Fibonacci heaps/
	 5. [ ] leftist heaps
	 6. [ ] min-max heaps
	 7. [ ] pairing heaps
	 8. [ ] skew heaps
	 9. [ ] /van Emde Boas queues/
***** Heaps en Memoria Segundaría: Find Min
      :CONTEXT:
      Considera un modelo de memoria segundaría con paginas de tamaño
      para contener $B$ elementos, y un "heap" de $n$ elementos
      ($n>>B$).  
      :END:
      A cuantos accesos a la memoria secundaría corresponde un
      llamado a "FindMin" en un "min heap"?
      1. [X] $1$ acceso
      2. [ ] $\log_B n$ accesos
      3. [ ] $\log n / \log B$ accesos
      4. [ ] $n/B$ accesos
      5. [ ] $n$ accesos
***** Heaps en Memoria Segundaría: Delete Min
      :CONTEXT:
      Considera un modelo de memoria segundaría con paginas de tamaño
      para contener $B$ elementos, y un "binary min heap" de $n$
      elementos ($n>>B$).
      :END:
      :SOLUTION:
      $\lg n-\lg B$
      :END:
      A cuantos accesos a la memoria segundaría corresponde un
      llamado a "DeleteMin" en un "min heap"?
      1. [ ] $\log_B n$ accesos
      2. [ ] $(n-B)/B+1$ accesos
      3. [ ] $n/B$ accesos
      4. [ ] $n-B$ accesos
      5. [ ] $n$ accesos
      6. [ ] otra respuesta
***** vEB queues: Delete Min
      :CONTEXT:
      Considera un modelo de memoria segundaría con paginas de tamaño
      para contener $B$ elementos, y un "heap" de $n$ elementos
      ($n>>B$).  
      :END:
      :SOLUTION:     
      $\log_B n$ accesos, que $B$ sea conocido o no.
      :END:
      A cuantos accesos a la memoria segundaría corresponde un
      llamado a "DeleteMin" en un vEB Queue?
      1. [ ] $\log_B n$ accesos
      2. [ ] $(n-B)/B+1$ accesos
      3. [ ] $n/B$ accesos
      4. [ ] $n-B$ accesos
      5. [ ] otra respuesta

***** vEB queues: cantidad de hijos
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-30 Wed 23:10]
      :END:
      :CONTEXT:
      Considera un modelo de memoria segundaría con paginas de tamaño
      para contener $B$ elementos, y un "vEB queue" de $n$ elementos
      ($n>>B$).
      :END:
      :SOLUTION:     
      
      :END:
      :PROOF: 
      :END:

      Cuanto hijos tiene la raíz de un vEB?
      1. [ ] $2$
      2. [ ] $B$
      3. [ ] $B+1$
      4. [ ] $\sqrt{B}$
      4. [ ] $\sqrt{n}$
      5. [ ] otra respuesta

***** vEB queues: altura
      :CONTEXT:
      Considera un modelo de memoria segundaría con paginas de tamaño
      para contener $B$ elementos, y un "vEB queue" de $n$ elementos
      ($n>>B$).  
      :END:
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-30 Wed 23:04]
      :END:
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:
      Cual es la altura de un vEB queue?
      1. [ ] $\log_B\log_B n$
      1. [ ] $\log_2\log_2 n$
      1. [ ] $\log_B n$
      1. [ ] $\log_2 n$
      5. [ ] otra respuesta

***** vEB queues: tiempo de búsqueda
      :CONTEXT:
      Considera un modelo de memoria segundaría con paginas de tamaño
      para contener $B$ elementos, y un "vEB queue" de $n$ elementos
      ($n>>B$).  
      :END:
      :LOGBOOK:
      - State "NEXT"       from ""           [2011-03-30 Wed 23:04]
      :END:
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:

      Cual es el tiempo de búsqueda ("findKey(k)") un vEB queue?
      1. [ ] $\log_B\log_B n$
      1. [ ] $\log_2\log_2 n$
      1. [ ] $\log_B n$
      1. [ ] $\log_2 n$
      5. [ ] otra respuesta

***** vEB queues: tiempo de "deleteMin"
      :CONTEXT:
      Considera un modelo de memoria segundaría con paginas de tamaño
      para contener $B$ elementos, y un "vEB queue" de $n$ elementos
      ($n>>B$).  
      :END:
      :LOGBOOK:
      - State "TODO"       from "NEXT"       [2011-03-30 Wed 23:07]
      - State "NEXT"       from ""           [2011-03-30 Wed 23:04]
      :END:
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:

      Cual es el tiempo de "deleteMin" en un vEB queue?
      1. [ ] $\log_B\log_B n$
      1. [ ] $\log_2\log_2 n$
      1. [ ] $\log_B n$
      1. [ ] $\log_2 n$
      5. [ ] otra respuesta

***** vEB queues: espacio
      :CONTEXT:
      Considera un modelo de memoria segundaría con paginas de tamaño
      para contener $B$ elementos, y un "vEB queue" de $n$ elementos
      ($n>>B$).  
      :END:
      :LOGBOOK:
      - State "TODO"       from ""           [2011-03-30 Wed 23:11]
      :END:
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:

      Cuanto bytes toma un "vEB queue"?
      
      1. [ ] $n$ 
      2. [ ] $k=\lg m$
      3. [ ] $m$
      4. [ ] $M=2^m$
      5. [ ] otra respuesta

***** Cotas Inferiores en Memoria Secundaria
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      :END:
      
      Cual(es) de estas afirmaciones esta(n) correctas (en el modelo
      de comparacion)?

      1. [ ] $\Omega(\log_B N)$ por colas de prioridades implicaria $\Omega(\log_B N)$ por diccionarios
      2. [ ] $\Omega(\log_B N)$ por diccionarios implicaria $\Omega(\log_B N)$ por colas de prioridades
      3. [ ] $\Omega(\log_B N)$ por colas de prioridades implicaria $\Omega(N\log_B N)$ por ordenamiento 
      4. [ ] $\Omega(N\log_B N)$ por ordenamiento implicaria $\Omega(\log_B N)$ por colas de prioridades
      5. [ ] ninguna
**** REFERENCIAS ADICIONALES
     + http://www.dcc.uchile.cl/~gnavarro/algoritmos/tesisRapa.pdf
       - paginas 9 hasta 16: para cotas inferiores y resultados experimentales.
     + Otras referencias en http://www.leekillough.com/heaps/
     + "An experimental Study of Priority Queues in External Memories" by Brengel, Crauser, Ferragina and Meyer
       - http://portal.acm.org/citation.cfm?id=351827.384259

*** Ordenamiento en memoria secundaria: Mergesort. *Cota inferior*.
**** MATERIAL A LEER
       - Algoritmos de Ordenamiento en Apuntes de CC3001
	 - http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Ordenacion/
	 - [ ] Quicksort
	 - [ ] Heapsort
	 - [ ] Bucketsort
	 - [ ] Mergesort
	 - [ ] Ordenamiento Externo
       - Ordenamiento en Memoria externa en Wikipedia:
	 - http://en.wikipedia.org/wiki/External_sorting
       - Cota Inferior Ordenamiento en Memoria externa
	 - http://www.daimi.au.dk/~large/ioS06/Alower.pdf 
**** APUNTES
    1. Un modelo mas fino quel anterior

       1) Cuantos paginas quedan en memoria local?
	  - no tan importante para busqueda
	  - muy importante para applicaciones de computacion con mucho
	    datos.

       2) Nuevas notaciones

	  - $B$ = Tamano pagina
	  - $N$ = cantidad de elementos en total
	  - $n$ = cantidad de paginas con elementos = $N/B$
	  - $M$ = cantidad de memoria local
	  - $m$ = cantidad de paginas locales = $M/B$
	  - mnemotechnique: 
	    - $N,M,B$ en cantidad de "palabras maquinas" (=bytes?)
	    - $n,m$ en cantidad de paginas
	    - $n<<N$, $m<<M$, por eso son "pequeñas" letras

       3) En estas notaciones, usando resultos previos:

	  * =Insertion Sort= (en un B-Arbol) 
	    - usa dictionarios en memoria externa
	    - $N \lg N / \lg B = N \log_B N$

	  * =Heap Sort= 
	    - usa colas de prioridades en memoria externa
	    - $N \lg N / \lg B = N \log_B N$

	  * Eso es optimo o no?

    2. Cotas Inferiores en Memoria Secundaria 
       * para buscar en un diccionario?
	 - en modelo RAM? (de comparaciones)
	   - $\lg N$
	 - en modele Memoria Externa? (de comparaciones)
	   - al maximo $\lg N / \lg B = \log_B N$    

       * para fusionar dos arreglos ordenados?
	 - en modelo RAM?
	   - $N$
	 - en modelo Memoria Externa con paginas de tamano B?
	   - al maximo $N/B = n$

       * para fusionar $k$ arreglos ordenados?
	 - en modelo RAM? 
	   - $N$
	 - en modelo de Memoria Externa con $M$ paginas de tamano $B$?
	   - al maximo $N/B = n$ *si $M > kB$*
	   - que hacemos si $M < kB$?
	     - el caso extremo cuando $k=N$ se llama ordenar
	     - veamos como ordenar, generalisar a la union de $k$
               arreglos ordenados es un ejericio despues.

       * para Ordenar
	 - (corrige y adapte la prueba de http://www.daimi.au.dk/~large/ioS06/Alower.pdf )
	 - en modelo RAM de comparaciones 
	   - $N \lg N$
	 - en modelo Memoria Externa con $n/B$ paginas de tamano B
	   * $\Omega( N/B \frac{ \lg(N/B) }{ \lg(M/B) }  )$
	   * que se puede notar mas simplamente $\Omega( n\lg_m n )$
	 - Prueba:

	   * en vez de considerar el problema de ordenamiento,
	     supponga que el arreglo sea una permutacion y
	     considera el problema (equivalente en ese caso) de
	     identificar cual permutacion sea.

	   * inicialemente, pueden ser $N!$ permutaciones.
	     - supponga que cada bloque de $B$ elementos sea ya
	       ordenado (impliqua un costo de al maximo $n=N/B$
	       accessos a la memoria externa).
	     - queda $N! / ( (B!)^n )$ permutaciones posibles.

	   * para cada accesso a una pagina de memoria externa,
	     cuantos permutaciones potenciales podemos eliminar?
	     - con $M$ entradas en memoria primaria
	     - $B$ nuevas entradas se pueden quedar de 
	       ${M \choose B}= \frac{M!}{B!(M-B)!}$ maneras distintas
	     - calcular la union de los $M+B$ elementos reduce la
	       cuantidad de permtuaciones por un factor de
	       $1 / {M \choose B}$
	     - después de $t$ accessos (distintos) a la memoria
	       externa, se reduci la cuantidad de permutaciones a
	       $N! / (  (B!)^n  {M \choose B}^t  )$

	   * cuanto accessos a la memoria sean necesarios para que
	     queda al maximo una permutacion?
	     - $N! / (  (B!)^n  {M \choose B}^t )$ debe ser al maximo uno.
	     - usamos las formulas siguientes:
	       - $\log(x!) \approx x\log x$
	       - $\log {M \choose B} \approx B \lg \frac{M}{B}$
	     - Inicialemente, tenemos la inequalidad siguiente:
	       $$N! \leq (B!)^n {M \choose B}^t$$
	     - aplicando el log de ambos lado (y las formulas previas) queda con 
	       $$ N \lg N \leq n B \lg B + t B \lg \frac{M}{B}$$
	     - Una secuencia de reduccion simples da:
	       $$t \geq \frac { N\lg N - nB \lg B }{ B \lg(M/B) }$$
	     - que se puede reescribir como $\frac { N \lg(N/B) }{ B \lg(M/B) }$        
	     - Pero $n=N/B$ y $m=M/B$, asi se puede reescribir $\frac { n \lg n }{ \lg m }$
	     - en final, deducimos $t \geq n \log_m n$.                                

       * BONUS: Para ordenar strings, un caso particular (donde la
	 comparacion de dos elementos tiene un costo variable):
	 - http://www.brics.dk/~large/Papers/stringsstoc97.pdf
	 - $\Omega( N_1/B \log_{M/B}(N_1/B) + K_2 \lg_{M/B} K_2 + N/B )$
	 - donde
	   - $N_1$ es la suma de los tamanos de las caldenas mas cortas que $B$
	   - $K_2$ es la cuantidad de caldenas mas largas que $B$

    3. Ordenar en Memoria Externa N elementos (en $n=N/B$ paginas)

       * http://en.wikipedia.org/wiki/External_sorting

       * Usando dictionarios o colas de prioridades en memoria externa
	 - $N \lg N / \lg B = N \log_B N$
	 - No es "ajustado" con la cota inferior
	 - impliqua 
	   - o que hay un mejor algoritmo
	   - o que hay una mejor cota inferior

       * Queda un algoritmo de ordenamiento: MergeSort
	 - usa la fusion de $m-1$ arreglos ordenados en memoria
	   externa:
	   1) carga en memoria principal $m-1$ paginas, cada una la
	      primera de su arreglo.
	   2) calcula la union de estas paginas en la pagina $m$ de
	      memoria principal, 
	      - botando la pagina cuando llena
	      - cargando una nueva pagina (del mismo arreglo) cuando
		vacilla
	   3) La complejidad es $n$ accessos. 
	 - Algoritmo:
	   1) ordena cada de las $n$ paginas \rightarrow $n$ accessos
	   2) Cada nodo calcula la union de $m$ arreglos y escribe su
	      resultado, pagina por pagina, en la memoria
	      externa. 
	 - Analisis:
	   - Cada nivel de recurencia costa $n$ accessos
	   - Cada nivel reduce por $m-1$ la cantidad de arreglos
	   - la complejidad total es de orden $n\log_m n$ accessos. (ajustado)

    4. *BONUS* cota inferior para una cola de prioridad?
       - una cola de prioridad se puede usar para ordenar (con $N$ accessos)
       - hay una cota inferior para ordernar de $n \log_m n$
       - entonces????

**** PREGUNTAS [6/9]						 :PREGUNTAS:
***** Effecto de $M$ sobre /Find/

      La complejidad de /Find/ en un $B$-arbol o vEB arbol es de
      $\lg_B N$ acesos a la memoria segundaria, con $M=1$ paginas en
      memoria principal. Con $M$ mas largo, este complejidad 

      1. [ ] se queda igual
      2. [ ] baja a $\lg_B N / M$
      3. [ ] baja a $\lg_{B/M} N $
      4. [ ] baja a $\lg_B (N/M)$
      5. [ ] Otra respuesta

***** Effecto de $M$ sobre /FindMin/

      La complejidad de /Find/ en un $B$-arbol o vEB arbol es de
      $\lg_B N$ acesos a la memoria segundaria, con $M=1$ paginas en
      memoria principal. Con $M$ mas largo, este complejidad 

      1. [ ] se queda igual
      2. [ ] baja a $\lg_B N / M$
      3. [ ] baja a $\lg_{B/M} N $
      4. [ ] baja a $\lg_B (N/M)$
      5. [ ] Otra respuesta

***** Complejidad de Insertion Sort en Memoria Segundaria
      :SOLUTION:     
      - =Insertion Sort= (en un B-Arbol) performa $N \lg N / \lg B = N \log_B N$  accesos 
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      Considera que 
      - $B$ = Tamano pagina
      - $N$ = cantidad de elementos en total
      - $n$ = cantidad de paginas con elementos = $N/B$
      - $M$ = cantidad de memoria local
      - $m$ = cantidad de paginas locales = $M/B$
      :END:

      El algoritmo de =Insertion Sort=, con un $B$-arbol, permite de
      ordenar $N$ elementos en

      1. [ ] al menos $N \lg N / \lg B = N \log_B N$  accesos 
      2. [ ] exactamente $N \lg N / \lg B = N \log_B N$  accesos 
      3. [ ] al maximo $N \lg N / \lg B = N \log_B N$  accesos 
      4. [ ] menos que $N \lg N / \lg B = N \log_B N$  accesos 
      5. [ ] otra respuesta

***** Complejidad de Heap Sort en Memoria Segundaria
      :SOLUTION:     
      - =Heap Sort= (con una vEB cola) performa $N \lg N / \lg B = N \log_B N$ accesos
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      Considera que 
      - $B$ = Tamano pagina
      - $N$ = cantidad de elementos en total
      - $n$ = cantidad de paginas con elementos = $N/B$
      - $M$ = cantidad de memoria local
      - $m$ = cantidad de paginas locales = $M/B$
      :END:

      El algoritmo de =Heap Sort=, con un vEB-cola de prioridad,
      permite de ordenar $N$ elementos en

      1. [ ] al menos $N \lg N / \lg B = N \log_B N$  accesos 
      2. [ ] exactamente $N \lg N / \lg B = N \log_B N$  accesos 
      3. [ ] al maximo $N \lg N / \lg B = N \log_B N$  accesos 
      4. [ ] menos que $N \lg N / \lg B = N \log_B N$  accesos 
      5. [ ] otra respuesta

***** Cota inferior de ordenamiento en memoria segundaria
      :SOLUTION:     
      $\Omega( N/B \frac{ \lg(N/B) }{ \lg(M/B) }  )$
      se nota igualemente $\Omega( n\lg_m n )$      
      :END:
      :PROOF: 
      La cota inferior de $\Omega(n\lg_m n)$ se prueba en las apuntes,
      y en pegacitos en las otras preguntas de conceptos. 

      Las cotas inferiores de $\Omega(N \lg N / \lg B)$ y $\Omega(N
      \log_B N)$ son equivalentes, y faltas, porque el algoritmo de
      mergesort con funnels permite de ordenar $N$ elementos en 
      $O(n\lg_m n)$ acesos, que contradice la cota inferior.
      :END:
      :CONTEXT:
      Considera que 
      - $B$ = Tamano pagina
      - $N$ = cantidad de elementos en total
      - $n$ = cantidad de paginas con elementos = $N/B$
      - $M$ = cantidad de memoria local
      - $m$ = cantidad de paginas locales = $M/B$
      :END:

      Cual de estas cotas inferiores para el problema de ordenar en
      memoria segundaria parece la mas razonable?

      1. [ ] $\Omega( N/B \frac{ \lg(N/B) }{ \lg(M/B) }  )$
      2. [ ] $\Omega( n\lg_m n )$      
      3. [ ] $\Omega(N \lg N / \lg B)$
      4. [ ] $\Omega(N \log_B N)$
      5. [ ] otra respuesta

***** Ordenar (a dentro de las) paginas
      :SOLUTION:     
      $n = N/B$
      :END:
      :PROOF: 
      No contamos las comparaciones, solamente los acesos a las $n$
      paginas en memoria segundaria.  Ordenar cada pagina se hace sin
      problema en memoria principal (e.g. si $M=2$ con mergesort, si
      $M=1$ con heapsort).
      :END:
      :CONTEXT:
      Considera que 
      - $B$ = Tamano pagina
      - $N$ = cantidad de elementos en total
      - $n$ = cantidad de paginas con elementos = $N/B$
      - $M$ = cantidad de memoria local
      - $m$ = cantidad de paginas locales = $M/B$
      - un arreglo $A$ contiene unas de las $N!$ permutaciones possibles sobre $N$ elementos.
      :END:

      Cual es el costo asintótico (en cantidad de accesos a la memoria
      secundaria) de ordenar cada bloque (pagina) de $B$ elementos?

      1. [ ] $n = N/B$
      2. [ ] $N = n\times B$
      3. [ ] $n \times B\lg B$
      4. [ ] $N \times B\lg B$
      5. [ ] otra respuesta

***** La permutacion escrita
      :SOLUTION:     
      $(N-1)!= N!/N$
      :END:

      Alguien elijo una permutacion muy grande sobre $[1..N]$, una de
      las $N!$ posibles.  El entrega la primera cifra. Cuantas
      permutaciones posibles quedan?


      1. [ ] $N!/(N-1)!$
      2. [ ] $N!/(N-1)$
      3. [ ] $N!/N$
      4. [ ] $N!$
      5. [ ] otra respuesta
	     
***** Contando permutaciones (Part 1)
      :SOLUTION:     
      $N! / B!$ 
      :END:
      :PROOF: 
      Si hay $X$ permutaciones posibles, cada vez que un bloque es
      ordenado, eso elimina $X/B!$ permutaciones. Iterar $n$ veces
      (hay $n$ bloques) lleva el resultado.
      :END:
      :CONTEXT:
      Considera que 
      - $B$ = Tamano pagina
      - $N$ = cantidad de elementos en total
      - $n$ = cantidad de paginas con elementos = $N/B$
      - $M$ = cantidad de memoria local
      - $m$ = cantidad de paginas locales = $M/B$
      - un arreglo $A$ contiene unas de las $N!$ permutaciones possibles sobre $N$ elementos;
      :END:

      Cuantas posibilidades de permutaciones quedan en $A$ despues de
      ordenar la primera pagina?

      1. [ ] $n! / B!$
      2. [ ] $N! / B!$ 
      3. [ ] $N! / B\lg B$
      4. [ ] $N!$
      5. [ ] $(N-B)!
      6. [ ] $N!-B!$
      7. [ ] otra respuesta

***** Contando permutaciones (Part 2)
      :SOLUTION:     
      $N! / (B!)^n$ 
      :END:
      :PROOF: 
      Si hay $X$ permutaciones posibles, cada vez que un bloque es
      ordenado, eso elimina $X/B!$ permutaciones. Iterar $n$ veces
      (hay $n$ bloques) lleva el resultado.
      :END:
      :CONTEXT:
      Considera que 
      - $B$ = Tamano pagina
      - $N$ = cantidad de elementos en total
      - $n$ = cantidad de paginas con elementos = $N/B$
      - $M$ = cantidad de memoria local
      - $m$ = cantidad de paginas locales = $M/B$
      - un arreglo $A$ contiene unas de las $N!$ permutaciones possibles sobre $N$ elementos;
      - alguien ya ordeno cada bloque de $B$ elementos.
      :END:

      Cuantas posibilidades de permutaciones quedan en $A$ despues de
      ordenar a dentro de las paginas?

      1. [ ] $N! / (B!)^n$ 
      2. [ ] $N! / n!$
      3. [ ] $N! / B!$
      4. [ ] $N!$
      5. [ ] otra respuesta

***** Insertando $B$ elementos en un arreglo ordenado de $M$ elementos (Part 1)
      :SOLUTION:     
      $B$ nuevas entradas se pueden quedar de 
      ${M \choose B} = \frac{M!}{B!(M-B)!} = M\times(M-1)\times\ldots\times(M-B+1)$
      maneras distintas      
      :END:

      De cuantas maneras se pueden mezclar $B$ valores en un arreglo
      de $M$ valores?

      1. [ ] ${M \choose B}$
      2. [ ] $\frac{M!}{B!(M-B)!}$
      3. [ ] $M\times(M-1)\times\ldots\times(M-B+1)$
      4. [ ] $M\times(M-1)\times\ldots\times(M-B)$
      5. [ ] otra respuesta

***** Insertando $B$ elementos en un arreglo ordenado de $M$ elementos (Part 2)
      :SOLUTION:     
      $X / {M \choose B} = X / \frac{M!}{B!(M-B)!} = X / M\times(M-1)\times\ldots\times(M-B+1)$
      :END:

      Si tenemos $X$ permutaciones posibles, que descubrimos las
      posiciones relatives de $B$ nuevas valores en relacion con $M$
      valores en memoria primaria, cuantas permutaciones quedan?
      
      1. [ ] $X / {M \choose B}$
      2. [ ] $X / \frac{M!}{B!(M-B)!}$
      3. [ ] $X / M\times(M-1)\times\ldots\times(M-B+1)$
      4. [ ] $X / M\times(M-1)\times\ldots\times(M-B)$
      5. [ ] otra respuesta

***** Insertando $t$ veces $B$ elementos a dentro de $M$ elementos
      :CONTEXT:
      Considera que 
      - $B$ = Tamano pagina
      - $N$ = cantidad de elementos en total
      - $n$ = cantidad de paginas con elementos = $N/B$
      - $M$ = cantidad de memoria local
      - $m$ = cantidad de paginas locales = $M/B$
      - un arreglo $A$ contiene unas de las $N!$ permutaciones possibles sobre $N$ elementos;
      - alguien ya ordeno cada bloque de $B$ elementos,
      - deseamos "descubrir" cual es la permutacion.
      :END:
      :SOLUTION:     
      $N! /  (B!)^n {M \choose B}^t$
      :END:
      :PROOF: 
      No se olvida que iniciamos el proceso con $N! / ( (B!)^n$
      permutaciones al inicio, y cada etapa reduce la cantidad de
      permutacion de un factor de $1/ {M \choose B}$.
      :END:

      Después de $t$ accessos (distintos) a la memoria externa, la
      cuantidad de permutaciones se reduci a

      1. [ ] $N! /  (B!)^t$
      2. [ ] $N! /  (B!)^n {M \choose B}^t$
      3. [ ] $N! /  {M \choose B}^t $
      4. [ ] $N! / (N-B\times t)!$
      5. [ ] otra respuesta

***** Cuantos acesos para reducir a una sola permutacion?
      :SOLUTION:     
        |-----------+--------+---------------------------------------------+-----------------------------------------------|
        | $N!$      | $\leq$ | $(B!)^n {M \choose B}^t$                    |                                               |
        |-----------+--------+---------------------------------------------+-----------------------------------------------|
        | $N \lg N$ | $\leq$ | $n B \lg B + t B \lg \frac{M}{B}$           | $\lg$ cresciente$                             |
        |           |        |                                             | $\lg(x!) \approx x\lg x$                      |
        |           |        |                                             | $\lg {M \choose B} \approx B \lg \frac{M}{B}$ |
        |-----------+--------+---------------------------------------------+-----------------------------------------------|
        | $t$       | $\geq$ | $\frac { N\lg N - nB \lg B }{ B \lg(M/B) }$ | reduccion lineal                              |
        |           | $\geq$ | $\frac { N \lg(N/B) }{ B \lg(M/B) }$        | $\lg(x/y) = \lg x - \lg y$                    |
        |           | $\geq$ | $\frac { n \lg n }{ \lg m }$                | $n=N/B$ y $m=M/B$                             |
        |           | $\geq$ | $n \log_m n$                                | definicion de $\lg_b$                         |
        |-----------+--------+---------------------------------------------+-----------------------------------------------|
      :END:
      Que agumento se usa para cada etapa del razonamiento siguente?

	|-----------+--------+---------------------------------------------|
	| $N!$      | $\leq$ | $(B!)^n {M \choose B}^t$                    |
	|-----------+--------+---------------------------------------------|
	| $N \lg N$ | $\leq$ | $n B \lg B + t B \lg \frac{M}{B}$           |
	|-----------+--------+---------------------------------------------|
	| $t$       | $\geq$ | $\frac { N\lg N - nB \lg B }{ B \lg(M/B) }$ |
	|           | $\geq$ | $\frac { N \lg(N/B) }{ B \lg(M/B) }$        |
	|           | $\geq$ | $\frac { n \lg n }{ \lg m }$          |
	|           | $\geq$ | $n \log_m n$                                |
	|-----------+--------+---------------------------------------------|

      1. [ ] $n=N/B$ y $m=M/B$
      2. [ ] $\lg x$ es cresciente
      4. [ ] $\lg(x/y) = \lg x - \lg y$
      5. [ ] $\lg(x!) \approx x\lg x$
      6. [ ] $\lg {M \choose B} \approx B \lg \frac{M}{B}$
      7. [ ] otra tecnica

***** Cota inferior ordenamiento en memoria segundaria
      :SOLUTION:     
      No se puede ordenar en menos que $n \log_m n$ acesos a la memoria segundaria.
      :END:

      Que significa que $t \geq n \log_m n$ ?

      1. [ ] No se puede ordenar en menos que $n \log_m n$ comparaciones.
      2. [ ] No se puede ordenar en menos que $n \log_m n$ acesos a la memoria segundaria.
      3. [ ] Se puede ordenar en menos que $n \log_m n$ comparaciones.
      4. [ ] Se puede ordenar en menos que $n \log_m n$ acesos a la memoria segundaria.
      5. [ ] otra respuesta

***** Cota superior ordenamiento en memoria segundaria
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      :END:

      Existe un algoritmo que ordena $N$ elementos (repartidos en $n$
      paginas de al maximo $B$ elementos cada una) en $O(n \log_m n)$
      acesos a la memoria segundaria?

      1. [ ] No
      2. [ ] Si, es una varianta de Merge Sort
      3. [ ] Si, es una varianta de Insertion Sort
      4. [ ] Si, es una varianta de Heap Sort
      5. [ ] Otra Respuesta

***** Cantidad de memoria Local
      :SOLUTION:     
      $M$ affecta  /MergeSort/ mucho mas que los otros:
      :END:
      :PROOF: 
      - en Mergesort una valor mas grande de $M$ permite de fusionar
        mas arreglos en parallelos (a cada etapa de la computacion);
      - en Diccionarios y Colas de prioridades, permite solamente de
        memorisar mas de los primeros niveles de la estructura de
        datos (con control del algoritmo de cache), o de cachear mas
        caminos de la estructura de datos (sin control del algoritmo
        de cache).
      :END:
      :CONTEXT:
      :END:

      Dado un tamaño de pagina fijo $B$, en cual problema la cantidad
      $M$ de memoria local (y la cantidad $m$ de paginas que se pueden
      guardar en memoria local) affecta mas la complejidad asintótica?

      1. [ ] /Find/ en ADT Diccionario
      2. [ ] /FindNext/ en ADT Diccionario (e.g. $B$-arbol o van Emde Boas)
      3. [ ] /FindMin/ en ADT Cola de prioridad
      4. [ ] /MergeSort/ 
      5. [ ] todas iguales: mas memoria siempre ayuda.

*** RESUMEN de la Unidad 2
**** Objetivos
      - Comprender el modelo de costo de memoria secundaria
      - Conocer algoritmos y estructuras de datos basicos que son eficientes en memoria secundaria, 
      - y el analisis de su desempeno.
**** Temas:
     1. [ ] Memoria Secundaria
     2. [ ] vEB diseno original ($\lg\lg m$ busqueda)
     3. [ ] vEB diseno "cache-oblivious" ($\log_B n$ busqueda sin conocer $B$)
     4. [ ] Diccionarios en Memoria Secundaria
     5. [ ] Colas de Prioridades en Memoria Secundaria
     6. [ ] Ordenamiento en Memoria Secundaria
     7. [ ] Cotas Inferiores en Memoria Secundaria
**** Summary of vEB trees variants

     |                   | B-Arbol             | recursive vEB     | value based vEB       | (otras) |
     |-------------------+---------------------+-------------------+-----------------------+---------|
     | Diccionario       | $(2,3)$ generalized | un vEB a dentro   | un arreglo indexado   | (...)   |
     |                   |                     | de un vEB         | por k/2 bits          |         |
     |-------------------+---------------------+-------------------+-----------------------+---------|
     | Cola de prioridad | Heap Generalized    | idem              | idem                  | (...)   |
     |-------------------+---------------------+-------------------+-----------------------+---------|
     | Propriedades      | simple cuando       | cache-oblivious   | tiempo $\lg\lg m$     | (...)   |
     |                   | $B$ conocido        | ($B$ desconocido) | (cuando $n\approx m$) |         |
     |-------------------+---------------------+-------------------+-----------------------+---------|

**** Draft of Test (see LaTeX file for final version)

1. Memoria Secundaria

    Considera un nivel de memoria tal que 
	+ $B$ = Tamano pagina
	+ $N$ = cantidad de elementos en total
	+ $n$ = cantidad de paginas con elementos = $N/B$
	+ $M$ = cantidad de memoria local
	+ $m$ = cantidad de paginas locales = $M/B$	 

    Maneja bien su tiempo para responder a las preguntas siguentes:

    1) Mejor y Peor caso

       Para cada de las estructuras de datos siguentes, cuál es el
       rendimiento (asintótico), en términos de accesos a la memoria
       secundaria en el peor caso y en el mejor caso, por un llamado a
       "Insert"? (recuerde que la estructura de datos contiene $N$
       elementos)

           |                                               | Peor Caso | Mejor Caso |
           |-----------------------------------------------+-----------+------------|
           | "min binary heap"                             |           |            |
           | avl arbol                                     |           |            |
           | (2,3)-arbol                                   |           |            |
           | $B$-arbol para diccionario                    |           |            |
           | $2B$-arbol para diccionario                   |           |            |
           | $B/2$-arbol para diccionario                  |           |            |
           | vEB-arbol original para colas de prioridades  |           |            |
           | vEB-arbol recursivo para colas de prioridades |           |            |
           | vEB-arbol original para diccionario           |           |            |
           | vEB-arbol recursivo  para diccionario         |           |            |
  
 	  
    2) Ordenamiento

       Cuál(es) algoritmos, en su variante adaptada a la memoria
       secundaria, permite(n) de ordenar $N$ elementos en el peor caso

       - En $O(N\lg N)$ accesos a la memoria secundaria?

       - En $O(N\log_B N)$ accesos a la memoria secundaria?

       - En $O(n\log_m n)$ accesos a la memoria secundaria?


2. Torneo

    El torneo internacional de Karate se tiene en una isla con
    capacidad por $M=20$ participantes. Una sola nave puede traer los
    $N=200$ participantes, que pudede transportar $B=5$ participantes
    al mismo tiempo. Se supone que los niveles de los participantes
    corresponden a un orden total, de manera que se pueden ordenar
    completamente.

    1. Cuántos viajes de la nave se necesitan en total para
       identificar el ganador del torneo, en el *peor* caso?

    2. Cuántos viajes de la nave se necesitan en total para
       identificar el ganador del torneo, en el *mejor* caso?

    3. Cuántos viajes de la nave se necesitan en total para
       identificar el orden total del torneo, en el peor caso?

    4. Cuántos viajes de la nave se necesitan en total para
       identificar el orden total sobre los $M=20$ mejores
       participantes del torneo, en el peor caso?

    5. Cuántos viajes de la nave se necesitan en total para
       identificar el orden total sobre los $2M=40$ mejores
       participantes del torneo, en el peor caso?

3. Inventa una pregunta.

   Imagina una pregunta sobre la memoria secundaria y sus $5$ (o mas)
   respuestas para ayudar un futuro alumno a entender mejor el
   tema. Indica la(s) respuesta(s) correcta(s) y el porqué en pocas
   palabras.

**** PREGUNTAS [0/10]						 :PREGUNTAS:
***** NEXT Peor Caso de "Insert" en Memoria Secundaria
      :SOLUTION:     
       - $\log_B\log_B N$
       - $\log_2\log_2 N$
       - $\log_B N$
	 + $B$-arbol para diccionario
	 + $2B$-arbol para diccionario
	 + $B/2$-arbol para diccionario
	 + vEB-arbol recursivo para colas de prioridades
	 + vEB-arbol recursivo  para diccionario
       - $\log_2 N$
	 + "min binary heap"
	 + avl arbol
	 + (2,3)-arbol
       - Otra Respuesta
	 + vEB-arbol original para colas de prioridades
	 + vEB-arbol original para diccionario
      :END:
      :PROOF:
      - For general large values of $B$, es imposible de lograr a
        $\log_B\log_B N$ en el modelo de comparaciones.
      - vEB's complexity cannot be guaranteed to be in $\log_2\log_2
        N$, when the domain of the values is not specified.
      - Differencia de un factor de 2 en el tamano de B no impacta la
        compleidad asintotica.
      :END:
      :CONTEXT:
       Considera un nivel de memoria tal que 
       - $B$ = Tamano pagina
       - $N$ = cantidad de elementos en total
       - $n$ = cantidad de paginas con elementos = $N/B$
       - $M$ = cantidad de memoria local
       - $m$ = cantidad de paginas locales = $M/B$
      :END:

      Para cada de las estructuras de datos siguentes,
       1) "min binary heap"
       2) avl arbol
       3) (2,3)-arbol
       4) $B$-arbol para diccionario
       5) $2B$-arbol para diccionario
       6) $B/2$-arbol para diccionario
       7) vEB-arbol original para colas de prioridades
       8) vEB-arbol recursivo para colas de prioridades
       9) vEB-arbol original para diccionario
       10) vEB-arbol recursivo  para diccionario

      Cual es el rendimiento (asintótico), en terminos de accesos a la
      memoria secundaria en el peor caso, por un llamado a "Insert"?
      (recuerde que la estructura de datos contiene $N$ elementos)

      1. [ ] $\log_B\log_B N$
      2. [ ] $\log_2\log_2 N$
      3. [ ] $\log_B N$
      4. [ ] $\log_2 N$
      5. [ ] otra respuesta

***** NEXT Mejor Caso de "Insert" en Memoria Secundaria (Part 1)
      :SOLUTION:     
       - $\log_B\log_B N$
       - $\log_2\log_2 N$
       - $\log_B N$
	 + "min binary heap"
	 + $B$-arbol para diccionario
	 + $2B$-arbol para diccionario
	 + $B/2$-arbol para diccionario
       - $\log_2 N$
       - (2,3)-arbol 
	 + avl arbol
       - Otra Respuesta
	 + Constante
	   + vEB-arbol recursivo para colas de prioridades
	   + vEB-arbol recursivo para diccionario
	 + sin cota
	   + vEB-arbol original para colas de prioridades
	   + vEB-arbol original para diccionario
      :END:
      :PROOF:
      - $B$-arboles y $(2,3)$ arboles, aunque pueden agregar un
        elemento en un nodo, igual deben ir hasta el 

      :END:
       - $\log_B\log_B N$
       - $\log_2\log_2 N$
       - $\log_B N$
       - $\log_2 N$
       - otra respuesta (constante)
	 - "min binary heap"
	 - avl arbol
	 - (2,3)-arbol
	 - $B$-arbol para diccionario
	 - $2B$-arbol para diccionario
	 - $B/2$-arbol para diccionario
	 - vEB-arbol original para colas de prioridades
	 - vEB-arbol recursivo para colas de prioridades
	 - vEB-arbol original para diccionario
      :END:
      :CONTEXT:
       Considera un nivel de memoria tal que 
       - $B$ = Tamano pagina
       - $N$ = cantidad de elementos en total
       - $n$ = cantidad de paginas con elementos = $N/B$
       - $M$ = cantidad de memoria local
       - $m$ = cantidad de paginas locales = $M/B$
      :END:

      Para cada de las estructuras de datos siguentes,
       1) "min binary heap"
       2) avl arbol
       3) (2,3)-arbol
       4) $B$-arbol para diccionario
       5) $2B$-arbol para diccionario
       6) $B/2$-arbol para diccionario
       7) vEB-arbol original para colas de prioridades
       8) vEB-arbol recursivo para colas de prioridades
       9) vEB-arbol original para diccionario
       10) vEB-arbol recursivo  para diccionario

      Cual es el rendimiento, en terminos de accesos a la memoria
      secundaria en el *mejor* caso, por un llamado a "Insert" ?

      1. [ ] $\log_B\log_B N$
      2. [ ] $\log_2\log_2 N$
      3. [ ] $\log_B N$
      4. [ ] $\log_2 N$
      5. [ ] otra respuesta

***** NEXT Mejor Caso de "Insert" en Memoria Secundaria (Part 2)
      :SOLUTION:     
       - $\log_B\log_B N$
       - $\log_2\log_2 N$
       - $\log_B N$
	 - $B$-arbol para diccionario
	 - $2B$-arbol para diccionario
	 - $B/2$-arbol para diccionario
	 - vEB-arbol recursivo para colas de prioridades
	 - vEB-arbol recursivo para diccionarios
       - $\log_2 N$
	 - "min binary heap"
	 - avl arbol
	 - (2,3)-arbol
       - otra respuesta
	 - vEB-arbol original para colas de prioridades
	 - vEB-arbol original para diccionario
      :END:
      :CONTEXT:
       Considera un nivel de memoria tal que 
       - $B$ = Tamano pagina
       - $N$ = cantidad de elementos en total
       - $n$ = cantidad de paginas con elementos = $N/B$
       - $M$ = cantidad de memoria local
       - $m$ = cantidad de paginas locales = $M/B$
      :END:

      Para cada de las estructuras de datos siguentes,
       1) "min binary heap"
       2) avl arbol
       3) (2,3)-arbol
       4) $B$-arbol para diccionario
       5) $2B$-arbol para diccionario
       6) $B/2$-arbol para diccionario
       7) vEB-arbol original para colas de prioridades
       8) vEB-arbol recursivo para colas de prioridades
       9) vEB-arbol original para diccionario
       10) vEB-arbol recursivo  para diccionario

      Cual es el rendimiento, en terminos de accesos a la memoria
      secundaria en el *mejor* caso, por un llamado a "Insert" *con un
      nuevo elemento*?

      1. [ ] $\log_B\log_B N$
      2. [ ] $\log_2\log_2 N$
      3. [ ] $\log_B N$
      4. [ ] $\log_2 N$
      5. [ ] otra respuesta

***** NEXT Ordenamiento en Memoria Secundaria: Cota superior (Part 0)
      :SOLUTION:     
      Insertion Sort, Merge Sort, Heap Sort, pero no Bubble Sort
      :END:
      :CONTEXT:
      Considera un nivel de memoria tal que 
      - $B$ = Tamano pagina
      - $N$ = cantidad de elementos en total
      - $n$ = cantidad de paginas con elementos = $N/B$
      - $M$ = cantidad de memoria local
      - $m$ = cantidad de paginas locales = $M/B$
      :END:

      Cual(es) de los algoritmos siguentes, en su variante adaptada a
      la memoria secundaria, permite(n) de ordenar $N$ elementos en
      $O(N\lg N)$ accesos a la memoria secundaria en el peor caso?

      1. [ ] Insertion Sort
      2. [ ] Merge Sort
      3. [ ] Heap Sort
      4. [ ] Bubble Sort
      5. [ ] otra respuesta

***** NEXT Ordenamiento en Memoria Secundaria: Cota superior (Part 1)
      :SOLUTION:     
      Insertion Sort, Merge Sort, Heap Sort, pero no Bubble Sort
      :END:
      :CONTEXT:
      Considera un nivel de memoria tal que 
      - $B$ = Tamano pagina
      - $N$ = cantidad de elementos en total
      - $n$ = cantidad de paginas con elementos = $N/B$
      - $M$ = cantidad de memoria local
      - $m$ = cantidad de paginas locales = $M/B$
      :END:

      Cual(es) de los algoritmos siguentes, en su varianta adaptada a la
      memoria secundaria, permite(n) de ordenar $N$ elementos en
      $O(N\log_B N)$ accesos a la memoria secundaria en el peor caso?

      1. [ ] Insertion Sort
      2. [ ] Merge Sort
      3. [ ] Heap Sort
      4. [ ] Bubble Sort
      5. [ ] otra respuesta

***** NEXT Ordenamiento en Memoria Secundaria: Cota superior (Part 2)
      :SOLUTION:     
      Merge Sort (=funnel sort)
      :END:
      :CONTEXT:
      Considera un nivel de memoria tal que 
      - $B$ = Tamano pagina
      - $N$ = cantidad de elementos en total
      - $n$ = cantidad de paginas con elementos = $N/B$
      - $M$ = cantidad de memoria local
      - $m$ = cantidad de paginas locales = $M/B$
      :END:

      Cual(es) de los algoritmos siguentes, en su varianta adaptada a la
      memoria secundaria, permite(n) de ordenar $N$ elementos en
      $O(n\log_m n)$ accesos a la memoria secundaria en el peor caso?

      1. [ ] Insertion Sort
      2. [ ] Merge Sort
      3. [ ] Heap Sort
      4. [ ] Bubble Sort
      5. [ ] otra respuesta

***** NEXT Torneo Vencedor: Cota inferior en el peor caso
      :SOLUTION:     
      $N/B$
      :END:
      :PROOF: 
      Es un problema de Max.
      :END:
      :CONTEXT:
      El torneo internacional de Karate se tiene en una isla con
      capacidad por $M=20$ participantes. Una sola nave puede traer
      los $N=200$ participantes, que pudede transportar $B=5$
      participantes al mismo tiempo. Se supone que los niveles de los
      participantes corresponden a un orden total, de manera a ce que
      se pueden ordenar completamente.
      - $M=20$
      - $N=200$
      - $B=5$
      :END:
      
      Cuantos viajes de la nave se necessitan en total para
      identificar el ganador del torneo, en el peor caso?
      1. [ ] $\log_B N$
      2. [ ] $N/B$
      3. [ ] $N\log_B N$
      4. [ ] $N/B + N\log_B N$
      5. [ ] otra respuesta

***** NEXT Torneo Vencedor: Cota superior en mejor caso
      :SOLUTION:     
      $N/B$, igual, hay que leerlas todas para asegurarse de ser
      correcto.
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      El torneo internacional de Karate se tiene en una isla con
      capacidad por $M=20$ participantes. Una sola nave puede traer
      los $N=200$ participantes, que pudede transportar $B=5$
      participantes al mismo tiempo. Se supone que los niveles de los
      participantes corresponden a un orden total, de manera a ce que
      se pueden ordenar completamente.
      - $M=20$
      - $N=200$
      - $B=5$
      :END:
      
      Cuantos viajes de la nave se necessitan en total para
      identificar el gañador del torneo, en el *mejor* caso?
      1. [ ] $\log_B N$
      2. [ ] $N/B$
      3. [ ] $N\log_B N$
      4. [ ] $N/B + N\log_B N$
      5. [ ] otra respuesta

***** NEXT Torneo Orden: Cota inferior (Part 1)
      :SOLUTION:     
      - otra respuesta: $n\log_m n$ (es un problema de ordenamiento
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      El torneo internacional de Karate se tiene en una isla con
      capacidad por $M=20$ participantes. Una sola nave puede traer
      los $N=200$ participantes, que pudede transportar $B=5$
      participantes al mismo tiempo. Se supone que los niveles de los
      participantes corresponden a un orden total, de manera a ce que
      se pueden ordenar completamente.
      - $M=20$
      - $N=200$
      - $B=5$
      :END:
      
      Cuantos viajes de la nave se necessitan en total para
      identificar el orden total del torneo, en el peor caso?

      1. [ ] $\log_B N$
      2. [ ] $N/B$
      3. [ ] $N\log_B N$
      4. [ ] $N/B + N\log_B N$
      5. [ ] otra respuesta

***** NEXT Torneo Orden: Cota inferior (Part 2)
      :SOLUTION:     
      $N/B$
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      El torneo internacional de Karate se tiene en una isla con
      capacidad por $M=20$ participantes. Una sola nave puede traer
      los $N=200$ participantes, que pudede transportar $B=5$
      participantes al mismo tiempo. Se supone que los niveles de los
      participantes corresponden a un orden total, de manera a ce que
      se pueden ordenar completamente.
      - $M=20$
      - $N=200$
      - $B=5$
      :END:
      
      Cuantos viajes de la nave se necessitan en total para
      identificar el orden total sobre los $M=20$ mejores participantes
      del torneo, en el peor caso?

      1. [ ] $\log_B N$
      2. [ ] $N/B$
      3. [ ] $N\log_B N$
      4. [ ] $N/B + N\log_B N$
      5. [ ] otra respuesta

***** NEXT Torneo Orden: Cota inferior (Part 3)
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      El torneo internacional de Karate se tiene en una isla con
      capacidad por $M=20$ participantes. Una sola nave puede traer
      los $N=200$ participantes, que pudede transportar $B=5$
      participantes al mismo tiempo. Se supone que los niveles de los
      participantes corresponden a un orden total, de manera a ce que
      se pueden ordenar completamente.
      - $M=20$
      - $N=200$
      - $B=5$
      :END:
      
      Cuantos viajes de la nave se necessitan en total para
      identificar el orden total sobre los $2M=40$ mejores participantes
      del torneo, en el peor caso?

      1. [ ] $\log_B N$
      2. [ ] $N/B$
      3. [ ] $N\log_B N$
      4. [ ] $N/B + N\log_B N$
      5. [ ] otra respuesta



** Tecnicas avanzadas de diseno y analisis de algoritmos (4 semanas = 8 charlas = 720mns)
*** Material relevante de los años previos:
    :PROPERTIES:
    :Effort:   10
    :END:
    - Colas de Prioridades http://www.leekillough.com/heaps/
    - Arboles 2-3 (para "Finger Search Trees"
      - http://www.dcc.uchile.cl/~bebustos/apuntes/cc3001/Diccionario/#4
    - http://www.wimp.com/justcoincidence/ Birthday Paradox, in English.
    - Interpolation Search
      - CC3001?
    - Counting Sort
      - CLRS 
      - CC3001 
*** Introduccion
    :PROPERTIES:
    :Effort:   20
    :END:
     1) Dominios discretos y finitos 
	1. Busqueda en Dominios discretos y finitos
	   - Interpolacion/extrapolation
	   - Tries o arboles digitales
	   - Arboles y Arreglos de Sufijos
	   - Hash y Hash en memoria Secundaria
	2. Algoritmos de Ordenamientos con universo finito 
	   - Counting Sort
	   - Bucket Sort
	   - Radix Sort
     2) Tecnicas de Analisis 
	1. Analisis amortizada
	   - tecnicas
	   - colas de prioridades
	   - splay arboles
	2. Analisis parametrizada
	   - busqueda doblada y finger search trees
	   - ordenamiento adaptivo
	   - operaciones de conjuntos adaptivos
	3. Analisis de Algoritmos en linea
	   - "ski renting" problema
	   - analisis competitiva ("Competitive Analysis")

*** Dominios discretos y finitos
**** Introduccion

     - afuera del modelo de comparacion. 
**** Algoritmos de Ordenamiento ( Counting Sort, Bucket sort, radix sort, string sort)
     :PROPERTIES:
     :Effort:   70:00
     :END:
***** Counting Sort   $O(\sigma + n)$
      :PROPERTIES:
      :Effort:   10
      :END:

      1. for $j=1$ to $\sigma$ do $C[j] \leftarrow 0$
      2. for $i=1$ to $n$ do $C[A[i]]++$
      3. $p\leftarrow 1$ 
      4. for $j=1$ to $\sigma$ do
	 - for $i=1$ to $C[j]$ do
	   - $A[p++] \leftarrow j$

	     Este algoritmo es bueno para ordenar multi conjuntos (donde
	     cada elementos puede ser presente muchas veces), pero pobre
	     para diccionarios, para cual es mejor usar la extension
	     logica, Bucket Sort.

***** Bucket Sort   $O(\sigma+n)$
      :PROPERTIES:
      :Effort:   20
      :END:

     1. for $j=1$ to $\sigma$ do $C[j] \leftarrow 0$
     2. for $i=1$ to $n$ do $C[A[i]]++$
     3. $P[1] \leftarrow 1$
     4. for $j\leftarrow 2$ to $\sigma$ do 
	- $P[j] \leftarrow P[j-1] + C[j-1]$
     5. for $i\leftarrow 1$ to $n$
	- $B[P[A[i]]++] \leftarrow A[i]$

	  Este algoritmo es particularmente practica para ordenar
	  llaves asociadas con objetos, donde dos llaves pueden ser
	  asociadas con algunas valores distintas. Nota que el
	  ordenamiento es *estable*.

***** Radix Sort $O(n \lg_n \sigma) = O(c n)$
      :PROPERTIES:
      :Effort:   30
      :END:

     - Considera un arreglo A de tamaño n sobre alfabeto $\sigma$
     - si $\sigma=n$, se recuerdan que bucket sort puede
       ordenar A en $O(n)$
     - si $\sigma=n^2$, bucket sort puede ordenar A en $O(n)$:
       - 1 ves con los $\lg n$ bits de la derecha
       - 1 ves con los $\lg n$ bits de la izquierda
	 (utilizando la estabilidad de bucket sort)
     - si $\sigma=n^c$, bucket sort puede ordenar A 
       - en tiempo $O(cn)$
       - con espacio $O(n)$

	 El espacio se puede reducir a $2n+\sqrt{n}$ con $\lg n/ 2$
	 bits a cada iteracion de Bucketsort, cambiando la
	 complejidad solamente por un factor de $2$.

     - En final, si $A$ es de tamaño $n$ sobre un alfabeto de
       tamaño $\sigma$, radix sort puede ordenar $A$ en tiempo
       $O( n \lceil \frac{\lg \sigma}{\lg n}\rceil )$

***** MAYB Provechando de las repeticiones en el modelo de Comparaciones :MAYB:
      :PROPERTIES:
      :Effort:   10
      :END:
      - Se puede o no?

  - Ordenar en $n H_i$ comparaciones

***** MAYB String Sort						       :MAYB:

      + Problema: Ordenar $k$ strings sobre alfabeto [\sigma], de
	largo total $n=\sum_i n_i$.

 + Si $\sigma \leq k$, y cada string es de mismo tamaño.

   - Si utilizamos bucket-sort de la derecha a la izquierda,
     podemos ordenar en tiempo $O(n)$, porque $O(n \lceil \lg
     \sigma/\lg l \rceil )$ y $\sigma<n$.

 + Si $\sigma \in O(1)$

   - Radix Sort sobre $c$ simboles, donde $c$ es el tamaño
     minima de una string, y iterar recursivamente sobre el
     restos de la strings mas grande con mismo prefijo.

   - En el peor caso, la complejidad corresponde a la suma de
     las superficias de los bloques, aka $O(n)$.

**** Busqueda en domanios discretos
     :PROPERTIES:
     :Effort:   100:00
     :END:
***** Busqueda por Interpolacion/Extrapolacion
      :PROPERTIES:
      :Effort:   10
      :END:

	 1. Introduccion:	     
	    - Haria busqueda binaria en un guia telefonico para
	      el nombre "Barbay"? En un diccionario para la ciudad
	      "Zanzibar"?
	    - ojala que no: se puede provechar de la informacion
	      que da la primera letra de cada palabra

	 2. Algoritmo

	    - Interaccion

	 3. Analisis
	    * La analisis *en promedio* es complicada: conversamos
	      solamente la intuicion matematica (para mas ver la
	      publicacion cientifica de SODA04, Demaine Jones y
	      Patrascu):
	      - si las llaves son *distribuidas uniformamente*, la
	       	distancia en promedio de la posicion calculada por
	       	interpolacion *lineal* hasta la posicion real es de
	       	$\sqrt{r-l}$.
	      - entonces, se puede reducir el tamaño del subarreglo
	       	de $n$ a $\sqrt{n}$ cada (dos) comparaciones
	      - la busqueda por interpolacion 
	       	- en promedio, 
	       	- si las llaves son *distribuidas uniformamente*,
	       	- toma $O(\lg \lg n)$ comparaciones

	    * La analisis *en el peor caso*

	      - Interaccion.

	 4. Variantas

	    1. Interpolacion non-lineal

	       - en un anuario telefonico o en un diccionario, las
		 frecuencias de las letras *no* son uniformes

	    2. Busqueda por Interpolacion Mixta con Binaria

	       - Se puede buscar en tiempo 
		 - $O(\lg n)$ en el peor caso Y
		 - $O(\lg \lg n)$ en el caso promedio?

	       - Solucion facil

	       - Solucion mas compleja

	    3. Busqueda por Extrapolacion
	       
	       - Tarea 3

	    4. Busqueda por Extrapolacion Mixta con Doblada

	       - Tarea 3

	 5. Discussion:

	    - Porque todavia estudiar la complejidad en el modelo de comparaciones? 

	      - Cuando el peor caso es importante

	      - Cuando la distribucion no es uniforme o no es conocida

	      - cuando el costo de la evaluacion es mas costo que una
		simple comparacion (en particular para la interpolacion
		non lineal)



***** Tries o Arboles Digitales
      :PROPERTIES:
      :Effort:   45
      :END:
      Ordenamos usualmente como pre-computacion para buscar despues. En el
      caso donde n es demasiado grande, ordenar puede ser demasiado
      carro. Consideramos alternativas para buscar.

      1. Ejemplo de trie

      - Insertar los nodos siguiente en un trie
	- hola
	- holistico
	- holograme
	- hologramas
	- ola
	- ole


   2. Busqueda

      - con arreglos de tamaño $\sigma$ en cada nodo: 
	- $O(l)$ tiempo, pero $O(L\sigma)$ espacio
      - con arreglos de tamaño variables en cada nodo:
	- $O(l \lg sigma)$ tiempo (busqueda binaria),  $O(L)$ espacio
	  (optima).
      - con hashing
	- $O(l)$ tiempo en promedio, $O(L)$ espacio.

   3. Insercion

      - Insertar "hora" en el arbol precedente

      - Insertar "holistico" en el arbol precedente

      - Borrar "hola" y "holistica"
	- (TAREA)
	- Tiene de "limpiar", pero no costo mas que un factor
	  constante de la busqueda.



   4. BONUS: PAT Trie

      - Comprime las ramas de nodos de grado uno en una sola arista.
      - La caldena ("string") etiquetando la arista se guarda en el
	nodo hijo de la arista.
      - Superio tan en tiempo que en espacio en practica.

***** Arboles y Arreglos de Sufijos
      :PROPERTIES:
      :Effort:   45
      :END:

  1) Arbol de Sufijos 

     - Espacio $O(n)$

     - Construccion $O(n)$

     - Busqueda $O(m)$

     - expresion regular $O(n^\lambda)$ donde $0\leq \lambda \leq 1$

  2) Arreglo de Sufijos

     - Lista de sufijos ordenados
     - busqueda de patrones = dos busquedas binarias, donde cada
       comparacion costa $\leq m$, resultando en una complejidad de
       $O(m \lg n)$

  3) BONUS: Rank en Bitmaps

     - $\mathtt{rank}(B,i)$ = cantidad de unos en $B[1,i]$
     - consideramos 
       - $B$ estatico
       - se puede almacenar $\lg n$ bits.

     - Solucion de Munro, Raman y Raman:

       - $b= 1/2 \lg n$   y    $s = \lg^2  n$

       - Dividimos el index de $B$ en 

	 - $s$ Superbloques de tamaño $n/s \lg n$ bits

	 - $b$ Mini bloques de tamaño $n/b \lg s$ bits
	   $${n \over 1/2 \lg n} \lg(\lg^2 n) = \frac{4n \lg\lg n}{\lg n} \in o(n)$$

	 - un diccionario con todos los bit vectores de tamaño
	   $$\sqrt{n} \lg n /2 \lg\lg n  \in o(n)$$
**** Hashing
     :PROPERTIES:
     :Effort:   200:00
     :END:
***** Introduccion
      :PROPERTIES:
      :Effort:   10
      :END:
      * Motivaciones	 
      - Mejor tiempo *en promedio*
      - Uso de todos la herramientas que tenemos
	- dominio de las valores
	- distribuciones de probabilidades de las valores
    * Terminologia 
      - Tabla de Hash
	- Arreglo de tamaño $N$ 
	- que contiene $n$ elementos a dentro de un universo $[1..U]$
      - Funccion de Hash $h(K)$
	- $h:[1..U] \rightarrow [0..N-1]$
	- se calcula rapidamente
	- distribue uniformemente (mas o menos) las llaves en la tabla
	  en el caso ideal, $P[h(K)=i] = 1/N, \, \forall K,i$
      - Collision 
	- cuando $h(K_{1})= h(K_{2})$
	- la probabilidad es alta: "Paradoxe del cumpleaños"
	  - ( http://www.wimp.com/justcoincidence/ )
	- Cual es la probabilidad que en una pieca de $n$ personas,
	  dos tiene la misma fecha de cumpleaños (a dentro de $365$
	  dias)?

	  - probabilidad que cada cumpleaños es unico:
	    $$ 364! \over {(365 - n)! \times 365^{n-1} }$$

	  - Probabilidad que hay al menos un cumpleaños compartido:
	    $$ 1 - 364! \over {(365 - n)! \times 365^{n-1} }$$

	    |   n |    Proba |
	    |-----+----------|
	    |  10 |      .12 |
	    |  23 |       .5 |
	    |  50 |      .97 |
	    | 100 | .9999996 |

***** Hashing Abierto
      :PROPERTIES:
      :Effort:   45
      :END:

      1) Idea principal:
	 - resolver las colisions con caldenas

      2) Ejemplo:  (muy irealistico)
	 - $h(K) = K \mathtt{ mod } 10$
	 - Secuencia de insercion $52,18,70,22,44,38,62$
	   - Insertando al final (si hay que probar por repeticiones)
	     | 0 |       70 |
	     | 1 |          |
	     | 2 | 52,22,62 |
	     | 3 |          |
	     | 4 |       44 |
	     | 5 |          |
	     | 6 |          |
	     | 7 |          |
	     | 8 |    18,38 |
	     | 9 |          |
	   - Insertando al final (si no hay que probar por repeticiones)
	     | 0 |       70 |
	     | 1 |          |
	     | 2 | 62,22,52 |
	     | 3 |          |
	     | 4 |       44 |
	     | 5 |          |
	     | 6 |          |
	     | 7 |          |
	     | 8 |    38,18 |
	     | 9 |          |
	     
      3) Analisis:

	 - factor de carga es $\lambda= {n \over N}$
	 - Rendimiento en el peor caso: $O(n)$

***** Hashing Cerrado
      :PROPERTIES:
      :Effort:   45
      :END:
      
      1) Idea principal:
	 - resolver las colisiones con busqueda, i.e. 
	   - $( h(K)+f(i) ) \mathtt{ mod } N$
	 - differentes tipos de busqueda:
	   - lineal  $f(i) = i$
	     - primary "clustering" (formacion de secuencias largas)		
	   - cuadratica $f(i) = i^2$
	     - secundario "clustering" (si $h(K_1)=h(K_2)$, la
	       secuencias son las mismas.
	   - doble hashing $f(i) = i . h'(K)$
	     - $h'(K)$ debe ser prima con N

      2) Ideal Hashing

	 - Imagina una funcion de hash que genera una secuencia que
	   parece aleatoria.

	 - cada posicion tiene la misma probabilidad de ser la
	   proxima
	    - Probabilidad $\lambda$ de elegir una posicion ocupada
	    - Probabilidad $1-\lambda$ de elegir une posicion libre
	    - la secuencia de prueba puede tocar la misma posicion
	      mas que una vez.
	    - llaves identicas todavia siguen la misma secuencia.
	 - Cual es el costo promedio $u_j$ de una busqueda negativa
	   con $j$ llaves en la tabla?

	   - $\lambda= \frac{j}{N}$

	   - $u_j = 1 (1-\lambda) + 2\lambda(1-\lambda)+r\lambda^2(1-\lambda)+\ldots$

	   - $= 1 + \lambda + \lambda^2 + \ldots$

	   - $= \frac{1}{1-\lambda}$

	   - $= \frac{1}{1-j/N}$

	   - $= \frac{N}{N-j} \in[1..N]$

	 - Cual es el costo promedio de una busqueda positiva $s_i$
	   para el $i$-th elemento insertado?
	   
	   - $s_i = \frac{1}{1- i/N} = \frac{N}{N-i}$

	   - $s_n = 1/n \sum \frac{N}{N-i}$

	   - $= \frac{N}{n} \sum_{i=0}^{n-1} \frac{1}{N-i}$

	   - $= \frac{1}{\alpha} \sum_{i=0}^{n-1} \frac{1}{N-i}$
	     con $\alpha = \frac{n}{N}$

	   - $< \frac{1}{\alpha} \int_{N-n}^{N} 1/x dx$

	   - $= 1/\alpha \ln\frac{N}{N-n}$

	   - $= 1/\alpha \ln\frac{1}{\alpha}$

	 - Para $\alpha = 1/2$, el costo promedio es $1.387$

	 - Para $\alpha = 0.9$, el costo promedio es $2.559$

***** Universal Hashing
      :PROPERTIES:
      :Effort:   10
      :END:

      - $h(K) = ( (aK+b) \mathtt{ mod } p) \mathtt{ mod } N$
      - $a \in [1..p-1]$ elegido al azar
      - $b \in [0..p-1]$ elegido al azar
      - $p$ es primo y mas grande que $N$ 
      - $N$ no es necesaramente primo

***** Hashing en memoria externa
      :PROPERTIES:
      :Effort:   90
      :END:

      1) Que pasa si la tabla de hashing no queda en memoria?

     - IDEA: Simula un B-arbol de altura dos
       - organiza el dato con valores de hash
       - guarda un index en el nodo raiz
       - usa solamente *una parte* de la valor de hash para elegir
	 el sobre-arbol
       - extiende el index cuando mas dato es agregado

  2) Descripcion
     
     - $B$ - cantidad de elementos en una pagina
     - $h$ - funcion de hash $\rightarrow [0..2^k-1]$
     - $D$ - *profundidad general*, con $D\leq k$
       - la raiz tiene $2^D$ punteros a las paginas horas
       - la raiz es indexada con los $D$ primeros bits de cada
	 valor de hash.
     - $d_l$ - *profundidad local* de cada hora $l$
       - Las valores de hash en $l$ tienen en comun los primeros
	 $d_l$ bits.
       - Hay $2^{D-d_l}$ punteros a la hora $l$
       - Siempre, $d_l\leq D$

  3) Ejemplo

     - $B=4, k=6, D=2$

       | $d_l=2$ | 000100 |
       |         | 001000 |
       |         | 001011 |
       |         | 001100 |

       | $d_l=2$ | 010101 |
       |         | 011100 |
       |         |        |
       |         |        |

       | $d_l=1$ | 100100 |
       |         | 101101 |
       |         | 110001 |
       |         | 111100 |

  4) Algoritmos

     - Buscar
     - Insertar
     - Remover

  5) Analisis

     - Buscar, insertar remover
       - 1 acceso a la memoria secundariaa si el index se queda
     - cantidad Promedio de paginas para tener $n$ llaves
       - $\frac{n}{B\lg 2}\approx 1.44 \frac{n}{B}$
       - paginas son llenas a $69\%$ mas o menos.
**** Conclusion
*** Tecnicas de Analisis
**** Introduccion
     :PROPERTIES:
     :Effort:   10
     :END:
**** Analisi amortizada 
***** MATERIAL A LEER
      - "Amortized Analysis Explained" by Rebecca Fiebrink
	- http://www.cs.princeton.edu/~fiebrink/423/AmortizedAnalysisExplained_Fiebrink.pdf
      - Amortized Analysis
	- CLRS, Chapter 17: Amortized Analizis p.405-430
      - Árbol biselado (Splay Trees)
	- http://es.wikipedia.org/wiki/%C3%81rbol_biselado
***** Principio de Analisi amortizada
      :PROPERTIES:
      :Effort:   30
      :END:   

        * Costo Amortizado:
	  - Se tiene una secuencia de n operaciones con costos
	    $c_1,c_2,...,c_n$. 
	  - Se quiere determinar $C=\sum_{i\in[1..n]} c_i$.
	  - Se puede tomar el peor caso de $ci\leq t$ para tener una
	    cota superior de $C\leq t n$.
	  - Un mejor analisis puede analizar el costo amortizado,
	    con varias tecnicas:
	    - analisis agragada
	    - contabilidad de costos
	    - funcion potencial.

       	* Applicaciones: 
	  - Move To Front
	  - "Self-adjusting and balanced binary trees" (Splay Trees"
	  - union-find data-structures 
	  - max flow
	  - Fibonacci heaps
	  - dynamic array (vector en Java)


       	* Tres tecnicas basicas:

	  1) "Aggregate analysis"
	     - bound las proporciones de operaciones de cada tipo
	     - (e.g. mas inserciones que deleciones)
	  2) "Accounting Method"
	     - asigna un costo (positivo o negativo) a cada operacion
	     - ejemplos:
	       - stack
	       - java vector
	     - costo amortizado es la suma de los costos.
	  3) "Potential Method" (CLRS p.405)
	     - asigna una funcion de "energia potencial" (como en fisica)
	       - (accounting method = height, potential method = potential energy)
	     - costo amortizado de una operacion es su costo mas el
               cambio de funcion de potencial que resulta de la
               operacion.
	     - es sufficiente de asegurarse que la funcion de
               potencial es siempre mas grande que su valor inicial
               para mostrar que el costo total amortizado es una cota
               superior sobre el costo total de las operaciones.
	     - ejemplo:
	       - analisis del algoritmo para min y max
	       - analisis de MTF

       	* Ejemplo: Incremento binario
	   - Incrementar $n$ veces un numero binario de $k$ bits,
	   - e.g. desde cero hasta $2^k-1$, con $n=2^k$.
	   - costo $\leq k n$   (brute force)
	   - costo $\leq n + n/2 + ... \leq 2n$ (costo amortizado)
	   - La tecnica usada aqui es la contabilidad de costos:
	     - un flip de 0 a 1 cuesta 2
	     - un flip de 1 a 0 cuesta 0
	     - cada incremento cuesta 2.
	   - Analisis
	     - $\phi$ = cantidad de unos en el numero
	     - $\phi_0 = 0$
	     - $c_i = l+1$ cuando hay $l$ unos
	     - $\Delta \phi_i = -l+1$
	     - $\sum \overline{c_i} = \sum c_i + \phi_n -\phi_0 
	       \geq 2$

       	* Ejemplo: arreglo dinamico, e.g. java Vector
	  - considera el tipo "Vector" en Java.
	  - de tamaño fijo $n$
	  - cuando accede a $n+1$, crea un otro arreglo de tamaño $2n$, y
	    copia todo.
	  - cual es el costo amortizado si agregando elementos uno a
	    uno?
	* Ejemplo: Move-to-Front
	  - analisis amortizada se puede usar para mostrar que MTF
            siempre performa a dentro de un factor de 4 de cualquier
            algoritmo (incluido un algoritmo optimal que conoce la
            secuencia de busquedas desde el inicio).
	  - Foncion de potencial es $2\times Inv(MTF)
	* Ejemplo: Splay Arboles
	  - el costo amortizado de cada insercion es $O(\lg n)$.
	* Ejemplo: Fibonacci Heap


***** Analisi amortizada de Colas de Prioridades
      :PROPERTIES:
      :Effort:   30
      :END:

    * Problema: Dado un conjunto (dinamica) de $n$ tareas con
      valores, elegir y remudar la tarea de valor maxima.

      * operaciones basicas:
       	- M.build({e1,e2,...,2n})
       	- M.insert(e)
       	- M.min
       	- M.deleteMin

      * operaciones adicionales ("Addressable priority queues")
       	- M.insert(e), volviendo un puntero h ("handle") al elemento insertado
       	- M.remove(h), remudando el elemento especificado para h
       	- M.decreaseKey(h,k), reduciendo la llave del elemento especificado para h
       	- M.merge(Q), agregando el heap Q al heap M.

      * Soluciones (conocidas o no):

       	|             | Linked List | Binary Tree    | (Min-)Heap      | Fibonacci Heap | Brodal Queue [1] |
       	|-------------+-------------+----------------+-----------------+----------------+------------------|
       	| insert      | $O(1)$      | $O(\lg n)$     | $O(\lg n)$      | $O(1)$         | $O(1)$           |
       	| accessmin   | $O(n)$      | $O(1)$         | $O(1)$          | $O(1)$         | $O(1)$           |
       	| deletemin   | $O(n)$      | $O(\lg n)$     | $O(\lg n)$      | $O(\lg n)^*$   | $O(\lg n)$       |
       	| decreasekey | $O(1)$      | $O(\lg n)$     | $O(\lg n)$      | $O(1)^*$       | $O(1)$           |
       	| delete      | $O(n)$      | $O(n)$         | $O(\lg n)$      | $O(\lg n)^*$   | $O(\lg n)$       |
       	| merge       | $O(1)$      | $O(m\lg(n+m))$ | $O(m \lg(n+m))$ | $O(1)$         | $O(1)$           |


     1) Colas de prioridades binarias ("Binary Heaps")

       	* La solucion tradicional 
	  - un arreglo de $n$ elementos
	  - hijos del nodo $i$ en posiciones $2i$ y $2i+1$
	  - la valor de cada nodo es mas pequena que las valores de su hijos.

       	* Complejidad: Espacio n+O(1), y 
	  | Operacion               | Tiempo   |
	  |-------------------------+----------|
	  | M.build({e1,e2,...,2n}) | $O(n)$     |
	  | M.insert(e)             | $O(\lg n)$ |
	  | M.min                   | $O(1)$     |
	  | M.deleteMin             | $O(\lg n)$ |

       	* Detalles de implementacion (mejorando las constantes)

	  - Cuidado de no implementar M.build({e1,e2,...,2n}) con n
	    inserciones (sift up $\rightarrow O(n\lg n)$), pero con $n/2$
	    sift-down ($\rightarrow O(n)$).

	  - En M.deleteMin(), algunas variantes de implementacion
	    (despues de cambiar el min con $A[n]$):
	    1. dos comparaciones en cada nivel hasta encontrar la
	       posicion final de $A[n]$ 
	       - $2\lg n$ comparaciones en el peor caso
	       - $\lg n$ copias
	    2. una comparacion en cada nivel para definir un camino
	       de tamaño lg n, y una busqueda binaria para encontrar
	       la posicion final
	       - $\lg n + O(\lg \lg n)$ comparaciones en el peor caso
	       - $\lg n$ copias en el peor caso
	    3. una comparacion en cada nivel para definir un camino
	       de tamaño lg n, y una busqueda *secuencial* up para encontrar
	       la posicion final
	       - $2\lg n$  comparaciones en el peor caso
	       - $\lg n$ copias en el peor caso
	       - pero en practica y promedio mucho mejor.


       	* Flexibilidad de estructuras
	  + Porque la complejidad es mejor con un heap que con un
	    arreglo ordenado?
	    - Para cada conjunto, hay solamente un arreglo ordenado
	      que puede representarlo, pero muchos heaps posibles:
	      eso da mas flexibilidad para la mantencion dinamica de
	      la estructura de datos.

	  + Colas de prioridades con punteros ("Addressable priority queues")
	    - Algun que mas flexible que los arreglos ordenados, las
	      colas de prioridades binarias todavia son de estructura
	      muy estricta, por ejemplo para la union de
	      filas. Estructuras mas flexibles consideran un "bosque"
	      de arboles. Ademas, estas estructuras de arboles son
	      implementadas con puntadores (en ves de implementarlos
	      en arreglos).
	    - Hay diferentes variantes. Todas tienen en comun los
	      puntos siguientes:
	      - un puntero *minPtr* indica el nodo de valor minima
	       	en el bosque, raiz de alguno arbol.
	      - *insert* agregas un nuevo arbol al bosque en tiempo $O(1)$
	      - *deleteMin* remudas el nodo indicado par minPtr,
	       	dividiendo su arbol en dos nuevos arboles. Buscamos
	       	para el nuevo min y fusionamos algunos arboles (los
	       	detalles diferencian las variantes)
	      - *decreaseKey(h,k)* es implementado cortando el arbol
	       	al nodo indicado por h, y rebalanceando el arbol
	       	cortado.
	      - *delete()* es reducido a *decreaseKey(h,0)* y
	       	*deleteMin*

     2) "Pairing Heaps"

       	- Malo rendimiento en el peor caso, pero bastante buena en
	  practica.

       	- rebalancea los arboles solamente en deleteMin, y solamente
	  con pares de raises (i.e. la cantidad de arboles es
	  reducida por dos a cada deleteMin).

	  | Operacion | Amortizado                 |
	  |------------------+----------------------------|
	  | Insert(C,x)      | $O(1)$                     |
	  | Merge            | $O(1)$                     |
	  | ExtractMin       | $O(\lg n)$                 |
	  | decreaseKey(h,k) | $\Omega(n \lg n \lg\lg n)$ |
	  |------------------+----------------------------|


     3) Colas de prioridades binomiales ("Binomial Heaps")

       	http://en.wikipedia.org/wiki/Binomial_heap
       	o p136 de Melhorn y Sanders

       	* Definicion
	  - Un *arbol binomial* (de orden $k$) tiene exactamente $k$
	    hijos de orden distintos $k-1,k-2,..., 0$. [Un arbol
	    binomial de orden 0 tiene 0 hijos.]
	  - Un *bosque binomial* es un conjunto de arboles binomiales
	    de orden *distintas* (i.e. hay cero o uno arboles de cada
	    orden).

       	* Propiedades
	  - Para cada arbol $T$
	    - $h(T) \leq lg |T|$
	    - $|T| \geq 2^{h(T)}$
	  - Para el bosque
	    - $\forall n$ hay solamente uno bosque binomial con n nodos.
	    - al maxima tiene $\lfloor \lg (n+1) \rfloor$ arboles.
	    - la descomposicion del bosque en arboles de orden $k$
	      corresponde a la descomposicion de $n$ en base de dos.

       	* Definicion
	  - una *cola binomial* es un bosque binomial donde cada nodo
	    almacena una clave, y siempre la clave de un padre es
	    inferior o igual a la clave de un hijo.

       	* Operaciones
	  | Operacion | Peor Caso  |
	  |------------------+------------|
	  | Merge            | $O(\lg n)$ |
	  | FindMin          | $O(\lg n)$ |
	  | ExtractMin       | $O(\lg n)$ |
	  | Insert(C,x)      | $O(\lg n)$ |
	  | Heapify          | $O(n)$     |
	  |------------------+------------|
	  | remove(h)        | $O(\lg n)$ |
	  | decreaseKey(h,k) | $O(\lg n)$ |
	  | merge(Q)         | $O(\lg n)$ |
	  |------------------+------------|

       	* Union

	  - Union de dos arboles binomiales de mismo orden:
	    - agrega $T_2$ a $T_1$ si $T_1$ tiene la raiz mas pequena.
	  - Union de dos bosques binomiales:
	    - si hay uno arbol de orden $k$, es lo de la union
	    - si hay dos arboles de orden $k$, calcula la union en un arbol de orden $k+1$
	    - la propagacion es similar a la suma de enteros en binario.

	  - Complejidad 
	    - $O(\lg n)$ en el peor caso 

       	* Insert
	  - agrega un arbol de orden 0 y hace la union si necesitado
	  - Complejidad $O(\lg n)$ en el peor caso
	  - Puede ser $O(1)$ sin corregir el bosque, que tiene de ser
	    corregido mas tarde, que puede ser en tiempo $O(n)$ peor
	    caso, pero sera $O(\lg n)$ en tiempo amortizado.

       	* Minima
	  - lei la lista de al maxima $\lfloor \lg (n+1) \rfloor$ raices
	  - Complejidad $O(\lg n)$
	  - Puede ser $O(1)$ si precalculando un puntero al minima, que
	    tiene de ser corregido (en tiempo $O(\lg n))$ a cada modificacion.

       	* DeleteMin
	  - encontra el min
	  - remuda el min de su arbol (la raiz)
	  - reordena su hijos para su orden, en un bosque binomial
	  - hace la union con el bosque binomial original, menos el arbol del min
	  - complejidad $O(\lg n)$

       	* DecreaseKey
	  - sigue el camino abajo hasta que la condicion del heap es
	    corregida.
	  - cada arbol tiene altura lg n, entonces la complejidad es
	    $O(\lg n)$ en el peor caso.

       	* Delete 
	  - reducido a DecreaseKey+DeleteMin



     4) Colas de prioridades de Fibonacci ("Fibonacci Heaps")

       	[[http://en.wikipedia.org/wiki/Fibonacci_heap]] o pagina 135 de
       	Melhorn y Sanders.


       	* Diferencia con la cola binomial:

	  - relax la estructura de los arboles (heap-forma), pero de
	    forma controlada.
	  - el tamaño de un sub-arbol cual raiz tiene $k$ hijos es al
	    maxima $F_k+2$, donde $F_k$ es el $k$-esimo numero de
	    Fibonacci.

       	* Operaciones
	  | Operacion | Peor Caso  | Amortizado |
	  |------------------+------------+------------|
	  | Merge            | $O(\lg n)$ | $O(1)$       |
	  | FindMin          | $O(\lg n)$ | $O(1)$       |
	  | ExtractMin       | $O(\lg n)$ | .          |
	  | Insert(C,x)      | $O(\lg n)$ | $O(1)$       |
	  | Heapify          | $O(n)$       | .          |
	  |------------------+------------+------------|
	  | remove(h)        | $O(\lg n)$ | .          |
	  | decreaseKey(h,k) | $O(\lg n)$ | $O(1)$       |
	  | merge(Q)         | $O(\lg n)$ | $O(1)$       |
	  |------------------+------------+------------|

     5) Overview
       	(copidao de http://en.wikipedia.org/wiki/Fibonacci_heap)

\begin{center}
      |             | Linked List | Binary Tree    | (Min-)Heap     | Fibonacci Heap | Brodal Queue     |
      |-------------+-------------+----------------+----------------+----------------+------------------|
      | insert      | $O(1)$      | $O(\lg n)$     | $O(\lg n)$     | $O(1)$         | $O(1)$           |
      | accessmin   | $O(n)$      | $O(1)$         | $O(1)$         | $O(1)$         | $O(1)$           |
      | deletemin   | $O(n)$      | $O(\lg n)$     | $O(\lg n)$     | $O(\lg n)^*$   | $O(\lg n)$       |
      | decreasekey | $O(1)$      | $O(\lg n)$     | $O(\lg n)$     | $O(1)^*$       | $O(1)$           |
      | delete      | $O(n)$      | $O(n)$         | $O(\lg n)$     | $O(\lg n)^*$   | $O(\lg n)$       |
      | merge       | $O(1)$      | $O(m\lg(n+m))$ | $O(m\lg(n+m))$ | $O(1)$         | $O(1)$           |

\end{center}

       6) BONUS Heapsort

	  - in place
	  - $O(n \lg n)$ con cualquiera de estas variantes.
***** Árbol biselado ("Splay Tree")
      :PROPERTIES:
      :Effort:   30
      :END:
****** APUNTES
**** Analisi adaptativa
***** MATERIAL A LEER
	- Árbol 2-3 http://es.wikipedia.org/wiki/%C3%81rbol_2-3
	- Finger tree http://en.wikipedia.org/wiki/Finger_tree
***** Analisi en el peor caso: a dentro de que?
      :PROPERTIES:
      :Effort:   10
      :END:
      - peor caso a dentro de las instancias de tamaño fijo
	- el tamaño puede ser multidimensional (e.g. grafos)
      - peor caso a dentro de las instancias de tamaño de resultado fijado
	- cantidad infinita
      - peor caso a dentro de las instancias de *dificultad* fijada
	- cantidad infinita
      - peor caso a dentro de las instancias de *tamaño fijo y dificultad* fijada

***** Busqueda Doblada:  $1+2\lceil\lg p\rceil$ comparaciones
      :PROPERTIES:
      :Effort:   20
      :END:

      El algoritmo de busqueda doblada 

      - encuentra en $1+\lceil\lg p\rceil$ comparaciones un intervalo
	de tamaño $p/2$ que contiene $x$, 
      - encuentra $p$ en $1+\lceil\lg p\rceil$ comparaciones
	adicionales (usando la segunda variante de la busqueda
	binaria),
      - por un total de   $1+2\lceil\lg p\rceil$ comparaciones
***** Finger Search Tree: la busqueda doblada de los arboles de busqueda
      :PROPERTIES:
      :Effort:   20
      :END:
****** APUNTES
	- Estructura de datos
	- algorimo de busqueda
	- analisis: busqueda en $O(\lg p)$ 
***** Algoritmo de Ordenamiento adaptivos basicos
      :PROPERTIES:
      :Effort:   20
      :END:
****** Merge Sort Adaptivo: Runs
****** Local Insertion Sort: Inv
****** Another Insertion Sort: REM
***** Computacion de la Union
      :PROPERTIES:
      :Effort:   10
      :END:
***** Computacion de la Interseccion
      :PROPERTIES:
      :Effort:   10
      :END:
**** Algoritmos en linea
***** List Accessing
      :PROPERTIES:
      :Effort:   45
      :END:

      REFERENCIA: Capitulo 1 en "Online Computation and Competitive
      Analysis", de Allan Borodin y Ran El-Yaniv

      1) "List Accessing"

	 - Considera la secuencia de busqueda de tamaño $n$, en un
	   diccionario de tamaño $\sigma$:
	   "1,1,1,1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,..."

	 - Cual es el rendimiento de una estructura de diccionario
	   *estatica* (tal que AVL) en este secuencia?
	   - $n\lg\sigma$
	     
	 - Se puede mejorar?
	   - si, utilizando la *localidad* de las consultas, en
	     *estructuras de datos dinamicas*.

      2) Soluciones

	 1. MTF ("Move To Front"): 
	    - pone las llaves en un arreglo desordenado
	    - buscas secuencialmente en el arreglo
	    - muda la llave encontrada en frente
	 2. TRANS ("Transpose"):
	    - pone las llaves en un arreglo desordenado
	    - buscas secuencialmente en el arreglo
	    - muda la llave encontrada de una posicion mas cerca del  frente
	 3. FC ("Frequency Count"):
	    - mantiene un contador para la frecuencia de cada elemento
	    - mantiene la lista ordenada para frecuencia decreciente.

	 4. Splay Trees (y otras estructuras con propiedades de localidad)
	    (http://www.dcc.uchile.cl/~cc30a/apuntes/Diccionario/#8b)


      3) Estos son "Algoritmos en Linea"
	 - algoritmo de optimizacion
	 - que conoce solamente una parte de la entrada al tiempo t.
	 - se compara a la competitividad con el algoritmo offline que
	   conoce toda la instancia.

	 - Como se puede medir su complejidad?
	   - cada algoritmo ejecuta $O(n)$ comparaciones para cada
	     busqueda en el peor caso!!!!
	   - tiene de considerar instancies "faciles" y "dificiles"
	   - una medida de dificultad
	   - e.g. el rendimiento del *mejor algoritmo "offline"*


      4) Competitive Analysis: instancias "dificiles" o "faciles"

	 - Las estructuras de datos dinamicas pueden aprovechar de
	   secuencias "faciles" de consultas: eso se llama "online".

	 - pero para muchos problemas online, todas las heuristicas se
	   comportan de la misma manera en el peor caso.

	 - Por eso se identifica una medida de dificultad de las
	   instancias, y se comparan los rendimientos de los
	   algoritmos sobre instancias que tienen una valor fijada de
	   este medida de dificultad.

	 - Tradicionalmente, esta medida de dificultad es el
	   rendimiento del mejor algoritmo "offline": eso se llama
	   *competitive analysis*, resultando en el *competitive
	   ratio*, el ratio entre la complejidad del algoritmo ONLINE
	   y la complejidad del mejor algoritmo OFFLINE.

	   - por ejemplo, veamos que MTF tiene un competitive ratio de
	     2

	 - Pero todavia hay algoritmos con performancia practicas muy
	   distintas que tienen el mismo competitive ratio. Por eso se
	   introduce otras medidas de dificultadas mas sofisticadas, y
	   mas especialidades en cada problema.

      5) Competitividad

	 * Optimizacion/aproximacion

	   - A es *$k(n)$ competitiva*  para un problema de *minimizacion* si
	     $$\exists b \forall n,x,\, |x|=n,\, C_A(x) - k(n) C_{OPT}(x) \leq b$$

	   - A es *k(n) competitiva*  para un problema de *maximizacion* si
	     $$\exists b, \forall n,x,\, |x|=n, \,
	     C_{OPT}(x) - k(n) C_{A}(x) \leq b$$

	 * Competitiva Ratio

	   - an algoritmo en linea es *c-competitiva* si
	     $$ \exists \alpha, \forall I 
	     ALG(I) \leq c OPT(I) + \alpha$$

	   - an algoritmo en linea es *estrictamente c-competitiva* si
	     $$ \forall I 
	     ALG(I) \leq c OPT(I) $$

      6) Sleator-Tarjan sobre MTF

	   - costo de una busqueda negativa (la llave NO esta en el
	     diccionario)
	     - $\sigma$
	   - costo de una busqueda positiva (la llave esta en el
	     diccionario)
	     - la posicion de la llave, no mas que $\sigma$
	     - en promedio para una distribucion de probabilidad fijada:
	       - $MTF \leq 2 OPT$, 
	     - Prueba: (from my notes in my CS240 slides)

	       How does MTF compare to the optimal ordering?	       
	       - Assume that:
		 - the keys $k_1,\ldots,k_n$ have probabilities 
		   $p_1 \ge p_2 \ge \ldots \ge p_n \ge 0$
		 - the list is used sufficiently to reach a steady state.
	       - Then: 
		 $$C_{MTF} < 2\cdot C_{OPT}$$
	       - Proof:
		 - $C_{OPT}=\sum_{j=1}^{n} jp_j$
		 - $C_{MTF}=\sum_{j=1}^{n} p_j(\mbox{ cost of finding } k_j)$
		 - $C_{MTF}=\sum_{j=1}^{n} p_j(1+\mbox{number of keys before } k_j)$
		   
		 - To compute the average number of keys before $k_j$:

		   $$\Pr[\mbox{ $k_i$ before $k_j$}] = \frac{p_i}{p_i+p_j}$$

		   $$E(\mbox{ number of keys before $k_j$}) = \sum_{i\neq j} \frac{p_i}{p_i+p_j}$$

		 - $k_i$ is before $k_j$ if and only if
		   $k_i$ was accessed more recently than $k_j$. 

		 - Consider the last time either $k_i$ or $k_j$ was looked up. What is the probability that it was $k_i$?
		   $$P(k_i \textrm{ before } k_j) = P(k_i \textrm{ chosen }|\ k_i \textrm{ or }k_j\textrm{ chosen })$$
		   $$P(k_i \textrm{ before } k_j) = \frac{P(k_i \textrm{ chosen })}{P(k_i \textrm{ or } k_j \textrm{ chosen })}$$
		   $$P(k_i \textrm{ before } k_j) = \frac{p_i}{p_i+p_j}$$

		 - Therefore,
		   - Joining both previous formulas:
		     $$C_{MTF} = \sum_{j=1}^{n} p_j (1+\sum_{i\not = j} \frac{p_i}{p_i+p_j})$$
		   - reordering the terms:
		     $$C_{MTF} =  1 + 2 \sum_{j=1}^n \sum_{i<j} \frac{p_i p_j }{p_i+p_j}$$
		   - Because $\frac{p_i}{p_i+p_j}\leq1$:
		     $$C_{MTF} \leq   1 + 2 \sum_{j=1}^n p_j (\sum_{i<j} 1)$$
		     $$C_{MTF} =  1 +  2 \sum_{j=1}^n p_j (j-1)$$
		     $$C_{MTF} = 1 + 2C_{OPT} + 2 \sum_{j=1}^n(-p_j)$$
		 - Because $\sum_{j=1}^n(p_j)=1$:
		     $$C_{MTF} = 2C_{OPT}-1$$



      7) [BONUS] Applicaciones a la compression de textos
	 

	 * Bentley, Sleator, Tarjan and Wei proponieron de
	   comprimir un texto utilizando una lista dinamica, donde el
	   codigo para un simbolo es la posicion del simbolo en la
	   lista.

	   - Experimentalmente, se compara a Huffman:
	     - a veces mucho mejor
	     - nunca mucho peor.

	 * Burrows and Wheeler proponieron una transformacion
	   (biyectiva) del texto, y de comprimir el resultado de esta
	   transformacion con MTF

	   - Experimentalmente, 6% mejor que GZip, que es enorme!

***** Paginamiento Deterministico
      :PROPERTIES:
      :Effort:   45
      :END:

      REFERENCIA: Capitulo 2 en "Online Computation and Competitive
      Analysis", de Allan Borodin y Ran El-Yaniv
      
      1) Paginamiento

	 - Definicion:
	   - elegir cual paginas guardar en memoria, dado
	     - una secuencia online de $n$ consultas para paginas, y
	     - un cache de $k$ paginas.
	 - Politicas:	
	   - LRU (Least Recently Used)
	   - CLOCK (1bit LRU)
	   - FIFO (First In First Out)
	   - LFU (Least Frequently Used)
	   - LIFO = MRU (Most Recently Used)
	   - FWF (Flush When Full)
	   - LFD (Offline, Longuest Forward Distance)

	 - Ustedes tienen una idea de cuales son las peores/mejores?

      2) Relacion con "List Accessing"

	 1. Cada "List accessing" algoritmo corresponde a un algoritmo
	    de paginamiento:
	    - cada miss, borra el ultimo elemento de la lista y
	      "inserta" el nuevo elemento.
	 2. No hay una reduccion tan clara en la otra direccion.


      3) Offline analysis

	 - LFD performa O(n/k) misses
	 - Cualquier algoritmo Offline performa Omega(n/k) en el peor
	   caso.

      4) Online analisis: resultados basicos

	 1. $\forall A$ online, hay una entrada con $n$ fallas.

	    - Estrategia de adversario.

	 2. No algoritmo online puede ser mejor que $k$ competitivo.

	    - Obvio, comparando con LFD.

	 3. MRU=LIFO NO es competitivo
	    - Considera S=
	      p_1,p_2,\ldots,p_k,p_{k+1},p_k,p_{k+1},p_k,p_{k+1},p_k,...
	    - despues las k primeras consultas, MRU va a tener un miss
	      cada consulta, cuando LFD nunca mas.

	 4. LFU no es competitivo 
	    - Considera $l>0$ y $S=
	      p_1^l,p_2^l,\ldots,p_{k-1}^l,(p_k,p_{k+1})^{l-1}$
	    - Despues de las $(k-1)l$ primeras consultas, LFU va a tener
	      un miss cada consulta, cuando LFD solamente dos.

	 5. Que tal de FWF?

	    - MRU=LIFO es un poco estupido, su mala rendimiento no es
	      una sorpresa.

	    - FWF es un algoritmo muy ingenuo tambien, pero vamos a
	      ver que no tiene un rendimiento tan mal "en teoria".

      5) BONUS: Competive Analysis: Algoritmos a Marcas

	 1. $k$-fases particiones

	    Para cada secuencia $S$, partitionala en secuencias
	    $S_1,\ldots,S_\delta$ tal que 
	    - $S_0 = \emptyset$
	    - $S_i$ es la secuencia Maxima despues de $S_{i-1}$ que
	      contiene al maximum $k$ consultas distintas.

	    - Llamamos "fase $i$" el tiempo que el algoritmo considera
	      elementos de la subsecuencia $S_i$.
	    - Nota que eso es independiente del algoritmo considerado.

	 2. Algoritmo con marcas 

	    - agrega a cada pagina de memoria lenta un bit de marca.

	    - al inicio de cada fase, remuda las marcas de cada
	      pagina en memoria.

	    - a dentro de una fase, marca una pagina la primera vez
	      que es consultada.

	    - un algoritmo a marca ("marking algorithm") es un
	      algoritmo que nunca remuda una pagina marcada de su
	      cache.

	 3. Un algoritmo con marcas es $k$-competitiva

	    - En cada fase, 
	      - un algoritmo ONLINE con marcas performa al maximum $k$ miss.
	      - Un algoritmo OFFLINE (e.g. LFD) performa al minimum
	       	$1$ miss.
	    - QED

	 4. LRU, CLOCK y FWF son  algoritmos con marcas

	 5. LRU, CLOCK y FWF tienen un ratio competitivo OPTIMO


      6) Mas resultados:

	 1. La analisis se puede generalizar al caso donde el
	    algoritmo offline tiene h paginas, y el algoritmo online
	    tiene $k\geq h$ paginas.

	    - Cada algoritmo *con marcas* es
	      $\frac{k}{k-h+1}$-competitiva.

	 2. Definicion de algoritmos (conservadores) da resultado
	    similar para FIFO (que no es con marcas pero es
	    conservador).

	 4. En practica, sabemos que LRU es mucho mejor que FWF (for
	    instancia). Habia mucha investigacion para intentar de
	    mejorar la analisis por 20 años, ahora parece que hay una
	    analisis que explica la mejor rendimiento de LRU sobre
	    FWF, y de variantes de LRU que pueden saber $x$ pasos en
	    el futuro (Reza Dorrigiv y Alex Lopez-Ortiz).



***** BONUS: "Ski Renting"
      http://en.wikipedia.org/wiki/Ski_rental_problem      
**** MAYB Complejidad Parametrizada
     :PROPERTIES:
     :Effort:  
     :END:
**** Conclusion
     :PROPERTIES:
     :Effort:   20
     :END:
**** PREGUNTAS [0/0]						 :PREGUNTAS:
***** Analisis Amortizada: arreglo dinamico (Part 1)
      :SOLUTION:     
      $O(n)$
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      :END:
      
      Queremos implementar una pila ("stack") en un arreglo. Iniciamos
      con un arreglo de tamaño $s=1$, y cuando se llena, creamos un
      arreglo mas grande, copiamos todo en en nuevo arreglo y sigamos.

      Cual es el costo amortizado de una insercion si el nuevo arreglo
      es de tamaño $n+1$?

      1. [ ] $O(1)$
      2. [ ] $O(\lg n)$
      3. [ ] $O(n)$
      4. [ ] $O(n^2)$
      5. [ ] otra respuesta

***** Analisis Amortizada: arreglo dinamico (Part 2)
      :SOLUTION:     
      $3\in O(1)$
      :END:
      :PROOF: 
      Para cada secuencia de $n$ operaciones, el costo total de copias
      es $1+2+4+8+\ldots+2^i$, donde $2^i< n \leq 2^{i+1}$.  

      La suma es $2^{i+1}-1$, en el pero caso (si $n=2^i+1$) igual a
      $2n-1$ copias. 

      Adicionando eso al costo de las $n$ inserciones da un costo
      total de $3n-1$, amortizado a un costo amortizado por operacion
      de $3\in O(1)$.
      :END:
      :CONTEXT:
      Queremos implementar una pila ("stack") en un arreglo. Iniciamos
      con un arreglo de tamaño $s=1$, y cuando se llena, creamos un
      arreglo mas grande, copiamos todo en en nuevo arreglo y sigamos.
      :END:
      
      Cual es el costo amortizado de una insercion si el nuevo arreglo
      es de tamaño $2n$?

      1. [ ] $O(1)$
      2. [ ] $O(\lg n)$
      3. [ ] $O(n)$
      4. [ ] $O(n^2)$
      5. [ ] otra respuesta

***** Analisis Amortizada: arreglo dinamico (Part 3)
      :SOLUTION:     
      menos que $2$: exactamente, de $1+1/3\in O(1)$
      :END:
      :PROOF: 
      Para cada secuencia de $n$ operaciones, el costo total de copias
      es $1+4+16+\ldots+4^i$, donde $4^i< n \leq 4^{i+1}$.  

      La suma es $(4^{i+1}-1)/3$, en el pero caso (si $n=4^i+1$,
      i.e. $4n-5=4^{i+1}-1$) igual a $(4n-5)/3$ copias.

      Adicionando eso al costo de las $n$ inserciones da un costo
      total de $(7n-5)/3$, amortizado a un costo amortizado por
      operacion de $7/3 - 5/3n\in O(1)$ asintoticamente.
      :END:
      :CONTEXT:
      Queremos implementar una pila ("stack") en un arreglo. Iniciamos
      con un arreglo de tamaño $s=1$, y cuando se llena, creamos un
      arreglo mas grande, copiamos todo en en nuevo arreglo y sigamos.
      :END:

      Cual es el costo amortizado de una insercion si el nuevo arreglo
      es de tamaño $4n$? 

      1. [ ] menos que $2$
      2. [ ] $2$
      3. [ ] entre $2$ y $3$
      4. [ ] $3$
      5. [ ] mas que $3$

***** Analisis Amortizada: arreglo dinamico (Part 4)
      :SOLUTION:     
      :END:
      :PROOF: 
      :END:
      :CONTEXT:
      Queremos implementar una pila ("stack") en un arreglo. Iniciamos
      con un arreglo de tamaño $s=1$, y cuando se llena, creamos un
      arreglo mas grande, copiamos todo en en nuevo arreglo y sigamos.
      :END:
      
      Cual es el costo amortizado de una insercion si el nuevo arreglo
      es de tamaño $n^2$ (y el primero arreglo de tamaño 2)?

      1. [ ] $O(1)$
      2. [ ] $O(\lg n)$
      3. [ ] $O(n)$
      4. [ ] $O(n^2)$
      5. [ ] otra respuesta

*** RESUMEN Unidad 3
    :PROPERTIES:
    :Effort:   20
    :END:
  * Resultados de Aprendisajes de la Unidad
	- Comprender las tecnicas de algoritmos de 
	  - costo amortizado, 
	  - uso de finitud, y 
	  - algorimos competitivos
	- Ser capaz de disenar y analzar algoritmos y estructuras de
	  datos basados en estos principios.
	- conocer algunos casos de estudio relevantes
  * Principales casos de estudio: 
	 - estructuras para union-find, 
	 - colas binomiales
	 - splay trees,
	 - busqueda por interpolacion
	 - radix sort
	 - arboles de van Emde Boas
	 - arboles de sufijos
	 - tecnica de los cuatro rusos,
	 - paginamiento
	 - busqueda no acotada (unbounded search, doubling search)


# 

#
** Algoritmos no convencionales (5 semanas = 10 charlas = 900mns) 
   :LOGBOOK:
   - State "PAUS"       from "TODO"       [2010-10-07 Thu 13:44]
   - State "TODO"       from ""           [2010-10-07 Thu 13:43]
   :END:
*** Descripcion de la Unidad
    :LOGBOOK:
    - State "DONE"       from ""           [2010-10-14 Thu 22:34]
    :END:
**** Resultados de Aprendisajes de la Unidad
     :LOGBOOK:
     - State "DONE"       from ""           [2010-10-07 Thu 13:43]
     :END:
     - Comprender el concepto de algoritmos 
       - aleatorizados, 
       - probabilisticos, 
       - aproximados
       - paralelos
     - y cuando son relevantes
     - ser capaz de disenar y analizar algoritmos de estos tipos
     - Conocer algunos casos de estudio relevantes
**** Principales casos de estudio:
     :LOGBOOK:
     - State "DONE"       from ""           [2010-10-07 Thu 13:43]
     :END:
     - primalidad
     - Karp Rabin para busqueda en strings
     - numero mayoritario
     - arboles binarios de busqueda aleatorizados 
     - quicksort
     - hashing universal y perfecto
     - aproximaciones para recubrimiento de vertices
     - vendedor viajero
     - mochila
     - ordenamiento paralelo
     - paralel prefix

*** *Aleatorizacion* (1 semana = 2 charlas)
      :LOGBOOK:
      - State "DONE"       from "ACTF"       [2010-10-18 Mon 19:45]
      - State "ACTF"       from "PAUS"       [2010-10-18 Mon 15:36]
      - State "PAUS"       from "DONE"       [2010-10-14 Thu 23:28]
      * State "DONE"       from "PAUS"       [2010-10-14 Thu 22:34]
      * State "PAUS"       from ""           [2010-10-14 Thu 15:05]
      :END:

  * REFERENCIA: 
    - Capitulo 1 en "Randomized Algorithms", de Rajeev Motwani and Prabhakar Raghavan.

**** Definiciones

       - *Algoritmos deterministico*
	 - algoritmo que usa solamente instrucciones
	   *deterministicas*. 
	 - algoritmo de cual la ejecucion (y, entonces, el
	   rendimiento) depende solamente del input.

       - *Algoritmos aleatorizados*
	 - algoritmo que usa una instruccion *aleatorizada*
	   (potentialemente muchas veces)
	 - una distribucion de probabilidades sobre una familia de
	   algoritmos deterministicos.

       - *Analisis probabilistica*
	 - analysis del rendimiento de un algoritmo, en promedio
	   sobre
	   - el aleatorio de la entrada, o 
	   - el aleatorio del algoritmo, o 
	   - los dos.
	 - veamos que son nociones equivalentes.

       - Formalizacion
	     |------------------------+---------------|
	     | Algoritmos clasicos    | Non clasicos  |
	     |------------------------+---------------|
	     | Siempre hacen lo mismo | aleatorizados |
	     | Nunca se equivocan     | Monte Carlo   |
	     | Siempre terminan       | Las Vegas     |
	     |------------------------+---------------|

       - Clasificacion de los algoritmos *de decision* aleatorizados

	 1. Probabilistico: Monte Carlo
	    - $P(error) < \epsilon$
	    - Ejemplo:
	      - deteccion de cliquas
	    - Se puede considerar tambien variantes mas fines:
	      - two-sided error ( => clase de complejidad $BPP$)
	       	- $P(accept | negative) < \epsilon$
	       	- $P(refuse | negative) > 1-\epsilon$
	       	- $P(accept | positive) > 1-\epsilon$
	       	- $P(refuse | positive) < epsilon$
	      - One-sided error
	       	- $P(accept | negative) < epsilon$
	       	- $P(refuse | negative) > 1 -\epsilon $
	       	- $P(accept | positive) = 1$
	       	- $P(refuse | positive) = 0$

	 2. Probabilistico: Las Vegas
	    - Tiempo es una variable aleatoria
	    - Ejemplos: 
	      - determinar primalidad [Miller Robin]
	      - busqueda en arreglo desordenado 
	      - interseccion de arreglos ordenados
	      - etc...

       - Relacion
	 - Si se puede verificar el resultado en tiempo razonable,
	   Monte Carlos iterado hasta correcto => Las Vegas

**** El poder de un algoritmo aleatorizado: Ejemplos
     Nota: Trae los juguetes/cajas de colores, con un tesoro a esconder a dentro.
     Ejemplos de algoritmos o estructuras de datos aleatorizados

    1. *hidden coin*
       - Decidir si un elemento pertenece en un lista
	 desordenada de tamano k, o si hay una moneda a dentro
	 de una de las $k$ caja.
       - cual son las complejidades determinisitica y
	 aleatorizada del problema de encontrar *una* moneda,
	 con $c$ la cantidad de monedas,
	 - si $c=1$?
	 - si $c=n-1$?
	 - si $c=n/2$?
    2. Respuestas:
       - Si una sola instancia de la valor buscada
	 - $k$ en el peor caso deterministico
	 - $k/2$ en (promedio y en) el peor caso aleatorio
	   - con una direccion al azar
	   - con $\lg(k!)$ bits aleatorios
       - Si $r$ instancias de la valor buscada
	 - $k-r$ en el peor caso deterministico
	 - $O(k/r)$ en (promedio y en) el peor caso aleatorio
    3. Decidir si un elemento pertenece en una lista ordenadas
       de tamano $n$
       * $\Theta(\lg n)$ comparaciones en ambos casos,
	 deterministico y probabilistico.
    4. *problema de union* 
       * Decidir si un elemento pertenece en una de las $k$ listas
	 ordenadas de tamano n
       * Si una sola lista contiene la valor buscada
	 - $k$ busquedas en el peor caso deterministico, que da
	   $k\lg(n)$ comparaciones
	 - $k/2$ busquedas en (promedio y en) el peor caso
	   aleatorio, que da $k\lg(n)/2$ comparaciones
       * Si $r<k$ listas contienen la valor buscada
	 - $k-r$ busquedas en el peor caso deterministico, que dan
	   $(k-r)\lg(n)$ comparaciones
	 - $k/r$ busquedas en (promedio y en) el peor caso
	   aleatorio, que dan $(k/r)\lg(n)$ comparaciones
       * Si $r=k$ listas contienen la valor buscada
	 - $k$ busquedas en el peor caso deterministico, en
	   promedio y en el peor caso aleatorio, que dan $k\lg n$
	   comparaciones

    5. *problema de interseccion*
       - dado $k$ arreglos ordenados de tamano $n$ cada uno, y un
	 elemento $x$.
       - cual son las complejidades determinisitica y aleatorizada
	 del problema de encontrar *un* arreglo que no contiene
	 $x$ (i.e. mostrar que la interseccion de $\{x\}$ con $\cap A$
	 es vacilla)? 
	 - si $c=1$ arreglo contiene $x$?
	 - si $c=n-1$ arreglos contienen $x$?
	 - si $c=n/2$ arreglos contienen $x$?

	 | $c$   | deterministica | aleatorizada |                   |
	 |-------+----------------+--------------+-------------------|
	 | $1$   | $2\lg n$       | $< 2\lg n$   |                   |
	 | $k-1$ | $k\lg n$       | $< (k-1)\lg n$ |                   |
	 | $k/2$ | $(k/2+1)\lg n$ | $< 2\lg n$   | HUGE IMPROVEMENT! |
	 |-------+----------------+--------------+-------------------|
	 | $c$   | $(c+1)\lg n$   | $<$          |                   |

    6. Eso se puede aplicar a la interseccion de posting lists
       (Google Queries).

**** Aleatorizacion de la entrada

     * Independencia de la distribucion  de la entrada

       - Si el input sigue una distribucion non-conocida, el input
	 perturbado tiene una distribucion conocida (para una
	 perturbacion bien elegida)

       - Ejemplo:
	 - flip $b$ de una bit con probabilidad $p$ que puede ser
	   distinta de $1/2$.
	 - suma-lo modulo 1 con un otro bit aleatorizado, con
	   probabilidad $1/2$ de ser uno.
	 - la suma es igual a uno con probabilidad $1/2$.

     * Estructuras de datos aleatorizadas

	- *funciones de hash*: estructura de datos aleatorizada, donde
          $(a,b)$ son elegidos al azar.

	- *skiplists*: estructura de datos aleatorizada, que simula en
          promedio un arbol binario

***** SkipLists
      :LOGBOOK:
      - State "DONE"       from ""           [2010-10-07 Thu 10:54]
      :END:

     1) Estructuras de datos para diccionarios

      - [X] Arreglo ordenado
      - [ ] "Move To Front" list (did they see it already?)
      - [X] Arboles binarios
      - [X] Arboles binarios aleatorizados
      - [X] Arboles 2-3   ( they saw it already?)
      - [X] Red-Black Trees  ( they saw it already?)
      - [X] AVL
      - [X] Skip List 
      - [ ] Splay trees

     2) Skip Lists

	1. Motivacion
	   - un arbol binario con entradas aleatorizadas tienen una
	     altura $O(\lg n)$, pero eso supone un orden de input
	     aleatorizados.
	   - El objetivo de las "skip lists" es de poner el aleatorio
	     a dentro de la estructura.
	   - tambien, es el equivalente de una busqueda binaria en
	     listas via un resumen de resumen de resumen...

	2. Definicion 
	   
	   - una skip-list de altura $h$ para un diccionario $D$ de
	     $n$ elementos es una familia de lists $S_0,\ldots,S_h$ y
	     un puntero al primero elemento de $S_h$, tal que

	     - $S_0$ contiene $D$;
	     - cada $S_i$ contiene un subconjunto aleatorio de $S_{i-1}$
	       (en promedio la mitad) 
	     - se puede navegar de la izquierda a la derecha, y de la
	       cima hasta abajo.

	   - se puede ver como $n$ torres de altura aleatorizadas,
	     conectadas horizontalmente con punteros de la
	     izquierda a la derecha.

	   - la informacion del diccionario estan solamente en $S_0$ (no
	     se duplica)

	3. Ejemplo

	 | 4 | X       | -  | -  | -  | -  | -  | -  | -> | X      |
	 | 3 | X       | -  | -> | X  | -  | -  | -  | -> | X      |
	 | 2 | X       | -  | -> | X  | X  | -> | X  | -> | X      |
	 | 1 | X       | X  | -> | X  | X  | -> | X  | -> | X      |
	 | 0 | X       | X  | X  | X  | X  | X  | X  | X  | X      |
	 |---+---------+----+----+----+----+----+----+----+--------|
	 |   | $-\infty$ | 10 | 20 | 30 | 40 | 50 | 60 | 70 | $\infty$ |

	4. Operaciones

	   * Search(x): 
	     - Start a the first element of $S_h$
	     - while not in $S_0$
	       - go down one level
	       - go right till finding a key larger than x

	   * Insert(x)
	     - Search(x)
	     - create a tower of random height ($p=1/2$ to increase
               height, typically)
	     - insert it in all the lists it cuts.

	   * Delete(x)
	     - Search(x)
	     - remove tower from all lists.

	5. Ejemplos 

	   * Insert(55) con secuencia aleatora (1,0)

         | 4 | X         | -  | -  | -  | -  | -    | -    | -  | -> | X        |
         | 3 | X         | -  | -> | X  | -  | -    | -    | -  | -> | X        |
         | 2 | X         | -  | -> | X  | X  | -    | ->   | X  | -> | X        |
         | 1 | X         | X  | -> | X  | X  | *->* | *X*  | X  | -> | X        |
         | 0 | X         | X  | X  | X  | X  | X    | *X*  | X  | X  | X        |
         |---+-----------+----+----+----+----+------+------+----+----+----------|
         |   | $-\infty$ | 10 | 20 | 30 | 40 | 50   | *55* | 60 | 70 | $\infty$ |

	   * Insert(25) con (1,1,1,1,0)

         | 5 | *X*       | -  | -    | -    | -  | -  | -  | -  | -  | *->* | *X*      |
         | 4 | X         | -  | *->* | *X*  | -  | -  | -  | -  | -  | ->   | X        |
         | 3 | X         | -  | *->* | *X*  | X  | -  | -  | -  | -  | ->   | X        |
         | 2 | X         | -  | *->* | *X*  | X  | X  | -  | -> | X  | ->   | X        |
         | 1 | X         | X  | *->* | *X*  | X  | X  | -> | X  | X  | ->   | X        |
         | 0 | X         | X  | X    | *X*  | X  | X  | X  | X  | X  | X    | X        |
         |---+-----------+----+------+------+----+----+----+----+----+------+----------|
         |   | $-\infty$ | 10 | 20   | *25* | 30 | 40 | 50 | 55 | 60 | 70   | $\infty$ |

	   * Delete(60) 


         | 5 | X         | -  | -  | -  | -  | -  | -  | -  | ->   | X        |
         | 4 | X         | -  | -> | X  | -  | -  | -  | -  | ->   | X        |
         | 3 | X         | -  | -> | X  | X  | -  | -  | -  | ->   | X        |
         | 2 | X         | -  | -> | X  | X  | X  | -  | -  | *->* | X        |
         | 1 | X         | X  | -> | X  | X  | X  | -> | X  | ->   | X        |
         | 0 | X         | X  | X  | X  | X  | X  | X  | X  | X    | X        |
         |---+-----------+----+----+----+----+----+----+----+------+----------|
         |   | $-\infty$ | 10 | 20 | 25 | 30 | 40 | 50 | 55 | 70   | $\infty$ |

	6. Analisis

	   * Espacio: Cuanto nodos en promedio?
	     - cuanto nodos en lista S_i?
	       - $n/2^i$ en promedio
	     - Summa sobre todos los niveles
	       - $n \sum 1/2^i < 2n$

	   * Tiempo: 
	     - altura promedio es $O(\lg n)$
	     - tiempo promedio es $O(\lg n)$


***** Paginamiento al Azar					   :OPTIONAL:
      :LOGBOOK:
      - State "DONE"       from ""           [2010-10-07 Thu 10:53]
      :END:
  - REFERENCIA: 
    - Capitulo 3 en "Online Computation and Competitive Analysis", de
      Allan Borodin y Ran El-Yaniv
    - Capitulo 13 en "Randomized Algorithms", de Rajeev Motwani and
      Prabhakar Raghavan, p. 368


****** Tipos de Adversarios (cf p372 [Motwani Raghavan])

     * Veamos en el caso deterministico un tipo de adversario
       offline, como medida de dificultad para las instancias online.
       Para el problema de paginamiento con k paginas, el ratio
       optima entre un algoritmo online y offline es de $k$
       (e.g. entre LRU y LFD).
     
     * DEFINICION: 

       En el caso aleatorizado, se puede considerar mas tipos de
       adversarios, cada uno definiendo una medida de dificultad y
       un modelo de complejidad.

       1. Adversario "Oblivious" ("Oblivious Adversary")

	  El adversario conoce $A$ pero no $R$: el elija su instancia
	  completamente al inicial, antes de la ejecucion online del
	  algoritmo.

       2. Adversario Offline adaptativo

	  Para este definicion, es mas facil de pensar a un
	  adversario como un agente distinto del algoritmo offline
	  con quien se compara el algoritmo online.

	  El adversario conoce $A$ en total, pero $R$ online, y le
	  utiliza para generar una instancia peor $I$. Este instancia
	  $I$ es utilizada de nuevo para ejecutar el algoritmo
	  offline (quien conoce el futuro) y producir las
	  complejidades (a cada instante online) con cual comparar la
	  complejidad del algoritmo online.

       3. Adversario Online adaptativo

	  En este definicion, el algoritmo conoce $A$ en total,
	  construir la instancia $I$ online como en el caso
	  precedente, pero tiene de tiene de resolverla online tambien
	  (de una manera, no se ve en el futuro).

****** Comparacion de los Tipos de adversarios.

      - Por las definiciones, es claro que 
	- el adversario offline adaptativo 
	  - es mas poderoso que
	- el adversario online adaptativo
	  - es mas poderoso que
	- el adversario oblivious

****** Competitiva Ratios

     * Para un algoritmo online $A$, para cada tipo de adversario se
       define un ratio de competitividad:
	 - $C_A^{obl}$: competitivo ratio con adversario oblivious
	 - $C_A^{aon}$: competitivo ratio con adversario adaptativo online
	 - $C_A^{aof}$: competitivo ratio con adversario adaptativo offline

       Es obvio que, para $A$ fijada, considerando un adversario mas
       poderoso va aumentar el ratio competitivo: 
       $$C_A^{obl} \leq C_A^{aon} \leq C_A^{aof}.$$

     * Para un problema el ratio de competitividad de un problema es
       el ratio de competitividad minima sobre todos los algoritmos
       correctos para este problema:

       $$C^{obl} \leq C^{aon} \leq C^{aof} \leq C^{det}$$

       donde $C^{det}$ es el competitivo ratio de un algoritmo online
       deterministico.



***** Arboles Binarios de Busqueda aleatorizados
      :LOGBOOK:
      - State "DONE"       from ""           [2010-10-07 Thu 10:53]
      :END:
   - Conrado Martinez. Randomized binary search trees. Algorithms
     Seminar. Universitat Politecnica de Catalunya, Spain, 1996.

   - Conrado Martinez and Salvador Roura. Randomized binary search
     trees. J. ACM, 45(2):288--323, 1998.

   - http://en.wikipedia.org/wiki/Treap, seccion
     "Randomized_binary_search_tree".

**** Complejidad Probabilistica: cotas inferiores
     :LOGBOOK:
     - State "DONE"       from ""           [2010-10-07 Thu 10:53]
     :END:
 * REFERENCIA: 
   - Capitulo 1 en "Randomized Algorithms", de Rajeev Motwani and Prabhakar Raghavan.

  1) Problema

      - Strategia de adversario no funciona

      - En algunos casos, teoria de codigos es suficiente
        (e.g. busqueda en arreglo ordenado). Eso es una cota inferior
        sobre el tamano del certificado.

      - En otros caso, teoria de codigos no es suficiente.  En
        particular, cuando el precio para verificar un certificado es
        mas pequeno que de encontrarlo. En estos casos, utilizamos
        otras tecnicas:
	  - teoria de juegos (que vamos a ver) y equilibro de Nash
	  - cotas sobre la comunicacion en un sistema de "Interactive
            Proof"

  2) Algunas Notaciones Algebraicas

     Sea:

     - $A$ una familia de $n_a$ algoritmos deterministicos
     - $a$ un vector $(0,...,0,1,0,...,0)$ de dimension $n_a$
     - $\alpha$ una distribucion de probabilidad de dimension $n_a$

     - $B$ una familia de $n_n$ instancias
     - $b$ un vector $(0,...,0,1,0,...,0)$ de dimension $n_b$
     - $\beta$ una distribucion de probabilidad de dimension $n_b$

     - $M$ una matriz de dimension $n_a\times n_b$ tal que M_{a,b}
       es el costo del algoritmo $a$ sobre la instancia $b$.
       Por definicion,
       - $a^t M b = M_{a,b}$
       - $\alpha^t M b$ es la complejidad en promedio (sobre el
	 aleatorio del algoritmo $\alpha$) de $\alpha$ sobre $b$
       - $a^t M \beta$ es la complejidad en promedio (sobre la
	 distribucion de instancias $\beta$) de a sobre \beta
       - \alpha^t M \beta es la complejidad en promedio del
	 algoritmo aleatorizados $\alpha$ sobre la distribucion de
	 instancia $\beta$.


  3) von Neuman's theorem: infsup = supinf = minmax = maxmin

     1. OPCIONAL Existencia de $\tilde{\alpha}$ et $\tilde{\beta}$ 

	Dado  $\phi$ y $\psi$ definidas sobre $\mathcal{R}^m$ y $\mathcal{R}^n$ por
	$$\phi(\alpha) = \sup_\beta \alpha ^T M \beta 
	\,\mbox{  y  }\,
	\psi(\beta)  = \inf_\alpha \alpha ^T M \beta$$
	Entonces:

	- $\phi(\alpha) = \max_\beta \alpha ^T M \beta$ 
	- $\psi(\beta)  = \min_\alpha \alpha ^T M \beta$
	- hay estrategias mixtas  
	  - $\tilde{\alpha}$ por $A$
	  - $\tilde{\beta}$ por $B$ 
	- tal que 
	  - $\phi$ es a su minima  en $\tilde{\alpha}$ y
	  - $\psi$ es a su maxima en $\tilde{\beta}$.

     2. Resultado de von Neuman: 

	Dado un juego $\Gamma$ definido por la matrica  $M$~:
	$$
	\min_\alpha \max_\beta  \alpha ^T M \beta 
	=
	\max_\beta  \min_\alpha \alpha ^T M \beta 
	$$

     3. Interpretacion:

	+ Este resultado significa que si consideramos ambos
	  distribuciones sobre algoritmos y instancias, no
	  importa el orden del max o min:
	     - podemos elegir el mejor algoritmo (i.e. minimizar
	       sobre los algoritmos aleatorizados) y despues
	       elegir la peor distribucion de instancias para el
	       (i.e. maximizar sobre las distribuciones de
	       instancias), o al reves
	     - podemos elegir la peor distribucion de instancias
	       (i.e. maximizar sobre las distribuciones de
	       instancias), y considerar el mejor algoritmo
	       (i.e. minimizar sobre los algoritmos
	       aleatorizados) para este distribucion.
	+ ATENCION!!!!  Veamos que 
	  - El promedio (sobre las instancias) de las
	    complejidades (de los algoritmos) en el peor caso
	  - no es igual 
	  - al peor caso (sobre las instancias) de la complejidad
	    en promedio (sobre el aleatorio del algoritmo)
	  - donde el segundo termo es realmente la complejidad de
	    un algoritmo aleatorizados.

	+ Todavia falta la relacion con la complejidad en el
	  peor caso $b$ de un algoritmo aleatorizados $\alpha$:

	  $$\max_b  \min_\alpha \alpha ^T M b$$


  4) Lema de Loomis 

     * Dado una estrategia aleatoria $\alpha$, emite una instancia
       $b$ tal que $\alpha$ es tan mal en $b$ que en el
       peor $\beta$.

       $$
       \forall \alpha \exists b,
       \max_\beta \alpha^T M \beta = \alpha^T M b
       $$

     * Dado una distribucion de instancias $\beta$, existe un
       algoritmo deterministico $a$ tal que $a$ es tan
       bien que el mejor algoritmo aleatorizados $\alpha$ sobre
       la distribucion de instancias $\beta$:

       $$
       \forall \beta \exists a,
       \min_\alpha \alpha^T M \beta = a^T M \beta
       $$

     * Interpretacion:

       + En frente a una distribucion de instancias especifica,
	 siempre existe un algoritmo deterministico optima en
	 comparacion con los algoritmos aleatorizados (que
	 incluen los deterministicos).

       + En frente a un algoritmo aleatorizados, siempre existe
	 una instan ca tan mal que la pero distribucion de
	 instancias.

  5) Principe de Yao

     * Del leima de Loomis podemos concluir que 

       $$
       \max_\beta \alpha^T M \beta = \max_b \alpha^T M b
       $$

       $$
       \min_\alpha \alpha^T M \beta = \min_a a^T M \beta
       $$

     * Del resultado de von Neuman sabemos que maxmin=minmax
       (sobre \alpha y \beta):

	$$
	\min_\alpha \max_\beta  \alpha ^T M \beta 
	=
	\max_\beta  \min_\alpha \alpha ^T M \beta 
	$$

     * Entonces    

       $$
       \min_alpha \max_b \alpha^T M b
       = (Loomis)
       \min_\alpha \max_\beta  \alpha ^T M \beta 
       = (von Neuman)
       \max_\beta  \min_\alpha \alpha ^T M \beta 
       = (Loomis)
       \max_\beta \min_a a^T M \beta
       $$

     * Interpretacion

      | $\min_\alpha$                    | $\max_b$        | $\alpha^T M b$ |
      |                                  |                 | La complejidad |
      | del mejor algoritmo aleatorizado |                 |                |
      |                                  | en el peor caso |                |

       es igual a 

      | $\max_\beta$                             | $\min_a$                           | $\alpha^T M b$ |
      |                                          |                                    | La complejidad |
      |                                          | del mejor algoritmo deterministico |                |
      | sobre la peor distribucion de instancias |                                    |                |


       "El peor caso del mejor algoritmo aleatorizado
       corresponde a
       la peor distribucion para el mejor algoritmo deterministico."


 * Ejemplos de cotas inferiores:

   1. Decidir si un elemento pertenece en una lista ordenadas de tamano $n$

      - Cual es el peor caso $b$ de un algoritmo aleatorizado $\alpha$?
      - Buscamos una distribucion $\beta_0$ que es mala para todos
	los algoritmos deterministicos $a$ (del modelo de comparaciones)
      - Consideramos la distribucion uniforma.
      - Cada algoritmo deterministico se puede representar como un
	arbol de decision (binario) con $2n+1$ hojas.

      - Ya utilizamos para la cota inferior deterministica que la
	altura de un tal arbol es al menos $\lg(2n+1)\in\Omega(\lg
	n)$. Esta propiedad se muestra por recurrencia.
      - De manera similar, se puede mostrar por recurrencia que la
	altura en promedio de un tal arbol binario es al menos
	$\lg(2n+1)\in\Omega(\lg n)$.

      - Entonces, la complejidad promedio de cada algoritmo
	deterministico $a$ sobre $\beta_0$ es al menos
	$\lg(2n+1)\in\Omega(\lg n)$.

      - Entonces, utilizando el principie de Yao, la complejidad en
	el peor caso de un algoritmo aleatorizado en el modelo de
	comparaciones es al menos $\lg(2n+1)\in\Omega(\lg n)$.

      - El corolario interesante, es que el algoritmo
	deterministico de busqueda binaria es *oprima* a dentro de
	la clase mas general de algoritmos aleatorizados.

   2. Decidir si un elemento pertenece en un lista desordenada de tamano k
      * Si una sola instancia de la valor buscada ($r=1$)
	* Cotas superiores
	  - $k$ en el peor caso deterministico
	  - $(k+1)/2$ en el peor caso aleatorio
	    - con una direccion al azar
	    - con $\lg(k!)$ bits aleatorios
	* Cota inferior
	  - Buscamos una distribucion $\beta_0$ que es mala para
	    todos los algoritmos deterministicos $a$ (en el modelo de
	    comparaciones).
	  - Consideramos la distribucion uniforma (cada algoritmo
	    reordena la instancia a su gusto, de toda manera,
	    entonces solamente la distribucion uniforma tiene
	    sentido): cada posicion es elegida con probabilidad $1/k$
	  - Se puede considerar solamente los algoritmos que no
	    consideran mas que una ves cada posicion, y que
	    consideran todas las posiciones en el peor caso:
	    entonces cada algoritmo puede ser representado por una
	    permutacion sobre $k$.

	  - Dado un algoritmo deterministico $a$, para cada
	    $i\in[1,k]$, hay una instancia sobre cual el performe
	    $i$ comparaciones. Entonces, su complejidad en promedio
	    en este instancia es $\sum_i i/k$, que es $k(k+1)/2k =
	    (k+1)/2$. Como eso es verdad para todos los algoritmos
	    deterministicos, es verdad para el mejor de ellos
	    tambien.

	  - Entonces, utilizando el principio de Yao, la complejidad
	    en el peor caso de un algoritmo aleatorizado en el
	    modelo de comparaciones es al menos $(k+1)/2$.

      * Si $r$ instancias de la valor buscada
	* Cotas superiores
	  - $k-r$ en el peor caso deterministico
	  - $O(k/r)$ en (promedio y en) el peor caso aleatorio
	* Cota inferior
	  - Buscamos una distribucion $\beta_0$ que es mala para
	    todos los algoritmos deterministicos $a$ (en el modelo de
	    comparaciones).
	  - Consideramos la distribucion uniforma (cada algoritmo
	    reordena la instancia a su gusto, de toda manera,
	    entonces solamente la distribucion uniforma tiene
	    sentido): cada posicion es elegida con probabilidad $1/k$
	  - Se puede considerar solamente los algoritmos que no
	    consideran mas que una ves cada posicion. 
	  - De verdad, no algoritmo tiene de considerar mas
	    posiciones que $k-r+1$, entonces hay menos algoritmos
	    que de permutaciones sobre $k$ elementos. Para
	    simplificar la prueba, podemos exigir que los algoritmos
	    especifican una permutacion entera, pero no vamos a
	    contar las comparaciones despues que un de las $r$
	    valores fue encontrada.



   3. Decidir si un elemento pertenece en k listas ordenadas de tamano $n/k$
      * Cotas superiores
	 * Si una sola lista contiene la valor buscada
	   - $k$ busquedas en el peor caso deterministico, que da
	     $k\lg(n/k)$ comparaciones
	   - $k/2$ busquedas en (promedio y en) el peor caso
	     aleatorio, que da $k\lg(n/k)/2$ comparaciones

	 * Si $r<k$ listas contienen la valor buscada
	   - $k-r$ busquedas en el peor caso deterministico, que dan
	     $(k-r)\lg(n/(k-r))$ comparaciones
	   - $k/r$ busquedas en (promedio y en) el peor caso
	     aleatorio, que dan $(k/r)\lg(n/k)$ comparaciones

	 * Si $r=k$ listas contienen la valor buscada
	   - $k$ busquedas en el peor caso deterministico, en promedio y
	     en el peor caso aleatorio, que dan $k\lg(n/k)$ comparaciones

   4. Aplicacion: 

      algoritmos de interseccion de listas ordenadas.



 * Conclusion:

   * Relacion fuerte entre algoritmos aleatorizados y complejidad en
     promedio

   * El peor caso de un algoritmo aleatorio corresponde a la
     peor distribucion para un algoritmo deterministico.

 * Otras aplicaciones importantes de los algoritmos aleatorizados

   * "Online Algorithms", en particular paginamiento.

   * Algoritmos de aproximacion

  * Hashing

		   
**** *Relacion con Problemas NP-Dificiles*

     * Ejemplos de Problemas NP-Dificiles

	 1. *Maxcut*
	    - dado un grafe $G=(V,E)$
	    - encontrar una partition $(L,R)$ tq $L\cup R=V$ y que
	      *maximiza* la cantidad de aristas entre $L$ y $R$
	    - el problema es NP dificil
	    - se aproxima con un factor de dos con un algoritmo
	      aleatorizado en tiempo polinomial.
	    
	 2. *mincut*
	    - dado un grafe $G=(V,E)$
	    - encontrar una partition $(L,R)$ tq $L\cup R=V$ y que
	      minimiza la cantidad de aristas entre $L$ y $R$
	    - el problema es NP dificil

     * Relacion con la nocion de NP:

       - El arbol representando la ejecucion de un algoritmo
	 non-deterministico en tiempo polynomial (i.e. NP) se
	 decomposa en dos partes, de altura polynomial $p(n)$:
	 - una parte de *decisiones* non-deterministica (fan-out)
	 - una parte de *verificacion* deterministica (straight)
       - Si una solamente de las $2^{p(n))$ soluciones corresponde a
	 una solucion valida del problema, la aleatorizacion no
	 ayuda, pero si una proporcion constante
	 (e.g. 1/2,1/3,1/4,...) de las ramas corresponden a una
	 solucion correcta, existe un algoritmo aleatorizado que
	 resuelve el problema NP-dificil en tiempo polynomial *en
	 promedio*.




**** Complejidad de un algoritmo aleatorizado

       * Considera algoritmos con comparaciones
	 - algoritmos deterministicos se pueden ver como arboles de
	   decision.
	 - algoritmos aleatorios se pueden ver (de manera
	   intercambiable) como
	   - una distribucion sobre los arboles de decision,
	   - un arbol de decision con algunos nodos "aleatorios".

	 - La complejidad en una instancia de un algoritmo aleatorio
	   es el promedio de la complejidad (en este instancia) de
	   los algoritmos deterministicos que le compasan:

	   C((A_r)_r,I) = E_r( C(A_r,I) )

	 - La complejidad en el peor caso de un algoritmo aleatorio
	   es el peor caso del promedio de la complejidad de los
	   algoritmos deterministicos que le composan:

	   C((A_r)_r) = \max_I C((A_r)_r,I) =  \max_I  E_r( C(A_r,I) )

**** Primalidad
      * REFERENCIAS: 
       	- Primalidad: Capitulo 14.6 en "Randomized Algorithms", de Rajeev Motwani and Prabhakar Raghavan.
       	- http://en.wikipedia.org/wiki/Primality_test#Complexity

      * Algoritmo "Random Walks" para SAT
       	1. eliga cualquieras valores $x_1,\ldots,x_n$
       	2. si todas las  clausilas son  satisfechas,
	   - accepta
       	3. sino
	   - eliga una clausula non satisfecha (deterministicamente
	     o no)
	   - eliga una de las variables de esta closula.
       	4. Repite es $r$ veces.

      * PRIMES is in coNP

       	si $x\in coNP$, eliga non-deterministicamente una
       	decomposicion de $x$ y verificalo.

      * PRIMES is in NP (hence in NP\cap coNP)

       	In 1975, Vaughan Pratt showed that there existed a
       	certificate for primality that was checkable in polynomial
       	time, and thus that PRIMES was in NP, and therefore in NP Á
       	coNP.


      * PRIMES in coRP

       	The subsequent discovery of the Solovay-Strassen and
       	Miller-Rabin algorithms put PRIMES in coRP. 

      * PRIMES in $ZPP = RP \cap coRP$

       	In 1992, the Adleman-Huang algorithm reduced the complexity
       	to $ZPP = RP \cap coRP$, which superseded Pratt's result.

      * PRIMES in QP

       	The cyclotomy test of Adleman, Pomerance, and Rumely from
       	1983 put PRIMES in QP (quasi-polynomial time), which is not
       	known to be comparable with the classes mentioned above.

      * PRIMES in P

       	Because of its tractability in practice, polynomial-time
       	algorithms assuming the Riemann hypothesis, and other
       	similar evidence, it was long suspected but not proven that
       	primality could be solved in polynomial time. The existence
       	of the AKS primality test finally settled this long-standing
       	question and placed PRIMES in P. 

      * $PRIMES \in NC$? $PRIMES \in L$?

       	PRIMES is not known to be P-complete, and it is not known
       	whether it lies in classes lying inside P such as NC or L.

**** Clases de complejidad aleatorizada 			      :BONUS:

***** RP

      - "A *precise* polynomial-time bounded nondeterministic Turing
       	Machine", aka "maquina de Turing non-deterministica acotadada
       	polinomialemente *precisa*", es una maquina tal que su
       	ejecucion sobre las entradas de tamano n toman tiempo p(n)
       	*todas*.

      - "A *polynomial Monte-Carlo Turing machine*", aka una
       	"*maquina de Turing de Monte-Carlo* polynomial" para el
       	idioma $L$, es una tal maquina tal que
       	- si $x\in L$, al menos la mitad de los $2^{p(|x|)}$ caminos
	  acceptan $x$
       	- si $x\not\in L$, *todas* los caminos rechazan $x$.

      - La definicion corresponde exactamente a la definicion de
       	algoritmos de Monte-Carlo. La classe de idiomas reconocidos
       	por una *maquina de Turing de Monte-Carlo* polynomial es
       	$RP$.

      - $P \subset BP \subset NP$:
       	- un algoritmo en $P$ accepta con todos sus caminos cuando
	  una palabra $x$ es en $L$, que es "al menos" la mitad.
       	- un algoritmo en $NP$ accepta con al menos un camino: un
	  algoritmo en $RP$ accepta con al menos la mitad de sus
	  caminos.

      

***** ZPP

      - $ZPP = RP \cap coRP$
      - $Primes \in ZPP$

***** PP

      - Maquina que, si $x\in L$, accepta en la mayoridad de sus
       	entradas.
      - PP probablemente no en NP
      - PP probablemente no en RP

***** BPP

      $$BPP = \left\{ L, \forall x, \left\{
      \begin{array}{l}
         x\in L \Rightarrow 3/4 \mbox{ de los caminos acceptan $x$} \\
	 x\not\in L \Rightarrow 3/4 \mbox{ de los caminos rechazan $x$ } \\
      \end{array}\right.\right\}$$
      
      $RP \subset BPP \subset PP$

      $BPP = coBPP$


*** Nociones de *aproximabilidad* (2 semanas = 4 charlas)
    :LOGBOOK:
    - State "DONE"       from ""           [2010-10-07 Thu 10:50]
    :END:
    - problemas que son o no aproximables
**** (Motivacion) 

     Aproximacion de problemas de optimizacion NP-dificiles

    - Que problemas NP dificiles conocen?
      - colorisacion de grafos
      - ciclo hamiltonian
      - Recubrimiento de Vertices (Vertex Cover)
      - Bin Packing
      - Problema de la Mochila
      - Vendedor viajero (Traveling Salesman)

    - Que hacer cuando se necessita una solucion en tiempo
      polinomial?

      - Consideramos los problemas NP completos de decision,
	generalmente de optimizacion.  Si se necesita una soluciono
	en tiempo polinomial, se puede considerar una aproximacion.

**** 4.2.1 *$p(n)$-aproximacion*

***** Definicion

      * Dado un problema de minimizacion, un algoritmo $A$ es un
       	*$p(n)$-aproximacion* si
       	$\forall n \max \frac{ C_A(x) }{ C_{OPT}(x) } \leq p(n)$

      * Dado un problema de maximizacion, un algoritmo $A$ es un
       	*$p(n)$-aproximacion* si
       	$\forall n \max \frac{ C_{OPT}(x) }{ C_A(x) } \leq p(n)$

       - Notas: 
	 1) Aqui consideramos la cualidad de la solucion, NO la
	    complejidad del algoritmo. Usualmente el problema es
	    NP-dificil, y el algoritmo de aproximacion es de
	    complejidad polinomial.
	 2) Las razones estas $\geq 1$ (elijamos las definiciones para
	    eso)
	 3) A veces consideramos tambien $C_A(x)-C_{OPT}(x)$
	    (minimizacion) y $C_{OPT}(x)-C_A(x)$: eso se llama un
	    "esquema de aproximacion polinomial" y vamos a verlo mas
	    tarde.

***** Ejemplo: Bin Packing (un problema que es 2-aproximable)

     - DEFINICION

       	Dado $n$ valores $x_1, \ldots, x_n$, $0\leq x_i\leq 1/2$,
       	cual es la menor cantidad de cajas de tamano 1 necesarias
       	para empaquetarlas?

     - Algoritmo "Greedy"
       - Considerando los $x_i$ en orden,
       - llenar la caja actual todo lo posible,
       - pasar ala siguiente caja.

     - Analisis
       - Este algoritmo tiene complejidad lineal $O(n)$.
       - Greedy da una 2-aproximacion.
       - (se puede mostrar facilamente en las instancias donde el
	 algoritmo optima llene completamente todas las cajas.)

     - Ademas, hay mejores aproximaciones, 
       1. Best-fit tiene performance ratio de 1.7 en el peor caso
       2. [Best-Fit Bin-Packing with Random Order (1997), Kenyon]

	  Best-fit is the best known algorithm for on-line
	  binpacking, in the sense that no algorithm is known to
	  behave better both in the worst case (when Best-fit has
	  performance ratio 1.7) and in the average uniform case,
	  with items drawn uniformly in the interval [0; 1] (then
	  Best-fit has expected wasted space O(n 1=2 (log n) 3=4
	  )). In practical applications, Best-fit appears to perform
	  within a few percent of optimal. In this paper, in the
	  spirit of previous work in computational geometry, we study
	  the expected performance ratio, taking the worst-case
	  multiset of items L, and assuming that the elements of L
	  are inserted in random order, with all permutations equally
	  likely. We show a lower bound of 1:08 : : : and an upper
	  bound of 1:5 on the random order performance ratio of
	  Best-fit. The upper bound contrasts with the result that in
	  the worst case, any (deterministic or randomized) on-line
	  bin-packing algorithm has performance ratio at least
	  1:54 : : :. 1

       3. Un otro paper donde particionan los $x_i$ en tres classes,
	  placeando los $x_i$ mas grande primero, buscando el
	  placamiento optimal de los $x_i$ promedio, y usando un
	  algoritmo greedy para los $x_i$ pequenos. [Karpinski?]

       4. la distribucion de los tamanos de las cajas hacen la
	  instancia dificil o facil.


***** Ejemplo: Recubrimiento de Vertices (Vertex Cover)

     - DEFINICION:

       Dado un grafo $G=(V,E)$, cual es el menor $V'\subseteq V$ tal
       que $V'$ sea un vertex cover de $G$.
       (o sea $V'$ mas pequeno tq $\forall e=(u,v)\in E, u\in V'o v\in V')

     - Algoritmo de aproximacion:

       - $V'\leftarrow \emptyset$
       - while $E\neq \emptyset$
	 - sea $(u,v) \in E$
	 - $V'\leftarrow V'\cup {u,v}$
	 - $E \leftarrow E \ {(x,y), x=u o x=v o y=u o y=v }$
       - return V

     - Discusion: es una 2-aproximacion o no?

     - LEMA: El algoritmo es una 2-aproximacion.

       - PRUEBA:
	 + Cada par $u,v$ que la aproximacion elige esta conectada,
	   entonces $u$ o $v$ estas en cualquier soluciono optima de
	   Vertex Cover.
	 + Como se eliminan las aristas incidentes en $u$ y $v$, los
	   siguientes pares que se eligen no tienen interseccion con
	   el actual, entonces cada 2 nodos que el algoritmo elige,
	   uno pertenece a la solucion optima.
	 + quod erat demonstrandum, (QED).



***** Ejemplo: Vendedor viajero (Traveling Salesman)

     - DEFINICION:

       Dado $G=(V,E)$ dirigido y $C:E\rightarrow R^+$ una funcion de
       costos, encontrar un recorrido que toque cada ciudad una vez, y
       minimice la suma de los costos de las aristas recorridas.

     - LEMA: 	Si $c$ satisface la desigualdad triangular
       $\forall x,y,z   c(x,y)+x(y,z) \geq c(x,z)$,
       hay una 2-aproximacion.

       - PROOF: 
	 - Algoritmo de Aproximacion (con desigualdad triangular)
	   - construir un arbol cobertor minimo (MST)
	     - se puede hacer en tiempo polinomial, con programacion lineal.
	     - C_{MST} \leq C_{OPT}
	   - producimos un recorrido en profundidad DFS del MST:
	     - C_{DFS} = 2C_{MST} \leq 2 C_{OPT}
	     - (factor dos porque el camino de vuelta puede ser al
	       maximo de tamano igual al tamano del camino de ida)
	   - eliminamos los nodos repetidos del camino, que no crece
	     el costo
	     - $C_A\leq 2 C_{OPT}$
	 - quod erat demonstradndum, QED.

     - LEMA: Si $c$ no satisface la desigualdad triangular, el
       problema de vendedor viajero no es aproximables en tiempo
       polinomial (a menos que P=NP$).

       - PROOF:   
	 - Supongamos que existe una p(n)-aproximacion de tiempo
	   polinomial.
	 - Dado un grafo G=(V,E)
	 - Construimos un grafo G'=(V,E') tal que 
	   - E'=V^2 (grafo completo), y
	   - c(u,v) =  1 si (u,v)\in E
		       n p(n) sino
	 - Si no hay un Ciclo Hamiltonian en E,
	   - todas las soluciones usan al menos una arista que no es
	     en E
	   - entonces todas las soluciones tienen costo mas que
	     $np(n)$
	 - Si hay un Ciclo Hamiltonian en E
	   - tenemos una aproximacion de una soluciono de costo menos
	     que $(n-1)p(n)$                 QED

***** Ejemplo: Vertex Cover con pesos


     - DEFINICION

       Dado G=(V,E) y c:V\rightarrowR^+, se quiere un V^*\subseteq V que cubra
       E y que minimice \sum_{v\in V^*} c(v).

     - Este problema es NP Completo.

     - LEMA: Vertex Cover con pesos es 2-aproximable
     - PROOF:
       1. Sea variables $x(v)\in {0,1}$, $\forall v\in V$
	   - el costo de $V^*$ sera $\sum x(v) c(v)$ 
	   - objetivo: $\min \sum_{v\in V} x(v) c(v)$
	     donde $0\leq x(v) \leq 1 \forall v\in V$
	   - $x(u)+x(v)\geq 1 \forall u,v \in E$
       2. $x(v) \in Z$ sere programacion entera, que es NP Completa
	  $x(v) \in R$ sere programacion lineal, que es polinomial
	  el valor $\sum x(v) c(v)$ que produce el programo lineal
	  es inferior a la mejor solucion al problema de VC.
       3. Algoritmo
	  - resolver el problema de programacion lineal
	    - nos da $x(v_1),\ldots,x(v_n)$
	  - $V^* \leftarrow\emptyset$
	  - for $v\in V$
	    - si $x(v)\geq 1/2$
	      - $V^* \leftarrow V^* \cup {v}$
	  - return $V^*$
       4. Propiedades
	  - $V^*$ es un vertex cover:
	    - si $\forall(u,v)\in E, x(u)+x(v)\geq 1$
	    - entonces, $x(u)\geq 1/2 o x(v)/geq 1/2$
	    - entonces, $u\in V^* o v\in V^*$
	  - $V^*$ es una 2-aproximacion:
	    - $c(V^*) = \sum_v y(v) c(v)$
	      donde $y(v) = 1 si x(v) \geq 1/2$, y $0$ sino
	    - entonces $y(v) \leq 2 x(v)$
	    - $\sum y(v) c(v) \leq \sum 2 x(v) c(v) \leq 2 OPT$
	    - $C(V^*)         \leq 2 OPT$
       5. QED


**** 4.2.2 PTAS y FPTAS
***** Definiciones

     * Esquema de aproximacion poliniomial *PTAS*
       Un *esquema de aproximacion polinomial* para un problema es un
       algoritmo $A$ que recibe como input una instancia del problema
       y un para-metro $\varepsilon>0$ y produce una $(1+\varepsilon)$
       aproximacion. Para todo $\varepsilon$ fijo, el tiempo de $A$
       debe ser polinomial en $n$, el tamano de la instancia.

     * Ejemplos de complejidades: Cuales tienen sentidos?
       - [ ] $O(n^{2/3})$
       - [ ] $O(\frac{1}{\varepsilon^2} n^2)$
       - [ ] $O(2^\varepsilon n)$
       - [ ] $O(2^{1/\varepsilon} n)$
	     
     * Esquema de aproximacion completamente polinomial *FPTAS*

       Un *esquema de aproximacion completamente polinomial* es un
       PTAS donde el tiempo del algoritmo es polinomial en $n$ *y en
       $1/\varepsilon$*.

***** Ejemplo: Problema de la Mochila    
+ Definicion
  + Dado 	  
    - $n$ pesos $p_1,\ldots,p_n\geq 0$,
    - $n$ valores $v_1,\ldots,v_n\geq 0$,
    - un peso total maximo $P$
  + Queremos encontrar $S\subset [1..n[$ tal que 
    - $\sum_{i\in S} p_i \leq P$ 
    - $\sum_{i\in S} v_i$ sea maximal.

+ Solucion Exacta

  + Dado $L=\{y_1,,\ldots y_m\}$, 
    - definimos $L+x=\{y_1+x,\ldots,y_m+x\}$.

  + Algoritmo:
    - $L \leftarrow \{\emptyset\}$
    - for $i\leftarrow 1$ to $n$
      - $L \leftarrow$ merge $(L,L+x_i)$
      - prune$(L,P)$      (remudando las valores $> P$)
    - return $\max(L)$

+ Solucion aproximada (inspirada del algoritmo exacto)

  + Operacion "Recorte"
     - Definimos la operacion de *recorte* de una lista L con
       parametro $\delta$:
       - Dado $y,z \in L$, $z$ *represente* a $y$ si
	 - $y /(1+\delta)   \leq   z   \leq y$ 
     - vamos a eliminar de $L$ todos los $y$ que sean representados
       por alguno $z$ no eliminado de $L$.

     - Recortar$(L,\delta)$
       - Sea $L=\{y_1,\ldots,y_m\}$
       - $L'\leftarrow \{y_1\}$
       - $\mathit{Last} \leftarrow y_1$
       - for $i\leftarrow 2$ to $m$
	 - si $y_i > \mathit{Last}(1+\delta)$
	   - $L \leftarrow L'.\{y_i\}$
	   - $\mathit{Last}\leftarrow y_i$
       - return $L'$

  + Algoritmo de Aproximacion:

       - $L \leftarrow \{\emptyset\}$
       - for $i\leftarrow 1$ to $n$
	 - $L \leftarrow \mathit{merge} (L,L+x_i)$
	 - $\mathit{prune}(L,t)$      (remudando las valores $> t$)
	 - *$L \leftarrow \mathit{recortar}(L,\epsilon/2n)$*
       - return max(L)

  + Analysis

     - El resultado es una $(1+\epsilon)$-aproximacion:
       1. retorne una solucion valida, tal que 
	  - $\sum(S')\leq t$ para algun $S'\subset S$
       2. en el paso $i$, para todo $z\in L_{OPT}$,
	  existe un $y\in L_A$ tal que $z$ representa a $y$.
	  - Luego de los $n$ pasos, el $z^*$ optimo en $L_{OPT}$
	    tiene un representante $y^*\in L_A$ tal que 
	    $$z^*/(1+\epsilon/2n)^n \leq y^* \leq z^*$$
       3. Para mostrar que el algoritmo es una $(1+\epsilon)$
	  aproximacion, 
	  - hay que mostrar que
	    - $z^*/(1+\epsilon) \leq y^*$
	  - entonces, debemos mostrar que 
	    - $(1+\epsilon/2n)^n \leq 1+\epsilon$
	  - Eso se muestra con puro calculo:
	    - $(1+\epsilon/2n)^n \leq? 1+\epsilon$
	    - $e^{n\lg(1+\epsilon/2n) }$
	    - $\leq e^{ n \epsilon /2n }$
	    - $= e^{ \epsilon/2 }$
	    - $\leq? e^{\ln(1+\epsilon)}$
	    - eso es equivalente a elegir \epsilon tal
	      que $\epsilon/2 \leq \ln(1+\epsilon)$
	    - i.e. cualquier tal que $0<\epsilon\leq 1$
       4. El algoritmo es polinomial (en todos los parametros)
	  - despues de recortar dedos $y_i,y_{i+1}\in L$
	    se cumple $y_{i+1}>y_i(1+\delta)$
	    y el ultimo elemento es $\leq t$
	  - entonces, la lista contiene $0,1$
	    y luego a lo mas $\lfloor \log_{(1+\delta)} t \rfloor$
	  - entonces el largo de L en cada iteracion no supera
	    $2+ \frac{ \log t }{ \log( 1+\varepsilon/2n) }$
	  - Nota que $\ln(1+x)$
	    - $= - \ln (1/(1+x))$
	    - $= - \ln ( (1+x-x)/(1+x) )$
	    - $= - \ln ( 1 - x/(1+x) )$
	    - $= - \ln (1+y) \geq -y$
	    - $\geq - (-x/(1+x)) = x/(1+x)$
	  - Entonces
	    - $2+ \frac{ \ln t }{ \ln 1 + \epsilon/2n }$
	    - $\leq 2 + ((1+\epsilon/2n) 2n \ln t )/\epsilon$
	    - $= ( 2n\ln t )/ \epsilon + \ln t + 2$
	    - $= O(n \lg t /\epsilon)$
	  - Entonces cada iteracion toma $O(n \lg t /\epsilon)$ operaciones
	  - Las $n$ iteraciones en total toman 
	    - $O(n^2 \lg t /\epsilon)$ operaciones

*** Algoritmos *paralelos* y distribuidos (2 semanas = 4 charlas)
    - Medidas de complejidad
    - Tecnicas de diseno
**** PREREQUISITOS
     - Chap 12 of "Introduction to Algorithms, A Creative Approach",
       Udi Manber, p. 375
     - http://www.catonmat.net/blog/mit-introduction-to-algorithms-part-thirteen/
**** Modelos de paralelismo y modelo PRAM

    * Instrucciones
      - SIMD: Single Instruccion, Multiple Data
      - MIMD:  Multiple Instruccion, Multiple Data
    * Memoria
      - compartida
      - distribuida

    * 2*2 combinaciones posibles:

	|------+--------------------+-----------------------------------------------------|
	|      | Memoria compartida | Memoria distribuida                                 |
	|------+--------------------+-----------------------------------------------------|
	| SIMD | PRAM               | redes de interconexion (weak computer units) |
	|      |                    | (hipercubos, meshes, etc...)                        |
	|------+--------------------+-----------------------------------------------------|
	| MIMD | Threads            | procesamiento distribuido (strong computer units),  |
	|      |                    | Bulk Synchronous Process, etc...                    |
	|------+--------------------+-----------------------------------------------------|

    * En este curso consideramos en particular el modelo PRAM

***** Modelo PRAM

    * Mucha unidad de CPU, una sola memoria RAM

      cada procesador tiene un identificador unico, y puede utilizarlo
      en el programa

    * Ejemplo:
      
      + if p mod 2 = 0 then
       	+ A[p] += A[p-1]
      + else
       	+ A[p] += A[p+1]
      + b \leftarrow A[p];
      + A[p]\leftarrow b;

    * Problema: el resultado no es bien definido, puede tener
      *conflictos* si los procesadores estan asinchronos. Las soluciones a
      este problemas dan varios submodelos del modelo PRAM:

      1. EREW Exclusive Read. Exclusive Write

      2. CREW Concurrent Read, Exclusive Write

      3. CRCW Concurrent Read, Concurent Write
	 En este caso hay variantes tambien:
	 - todos deben escribir lo mismo
	 - arbitrario resultado
	 - priorizado
	 - alguna f() de lo que se escribe

	   
***** Como medir el "trade-off" entre recursos (cantidad de procesadores) y tiempo?

     * DEFINICION:

       - $T^*(n)$ es el *Tiempo secuencial* del mejor algoritmo no
	 paralelo en una entrada de tamano $n$ (i.e. usando $1$
	 procesador).

       - $T_A(n,p)$ es el *Tiempo paralelo* del algoritmo paralelo
	 $A$ en una entrada de tamano $n$ usando $p$ procesadores.

       - El *Speedup* del algoritmo $A$ es definido por

	 $S_A(n,p) = \frac{ T^*(n) }{ T_A(n,p) } \leq p$ 

	 Un algoritmo es mas efectivo cuando $S(p)=p$, que se llama
	 *speedup perfecto*.

       - La *Eficiencia* del algoritmo $A$ es definida por

	 $E_A(n,p) = \frac{ S_A(n,p) }{ p }=\frac{T^*(n)}{pT_A(n,p)}$ 

	 El caso optima es cuando $E_A(n,p)=1$, cuando el algoritmo
	 paralelo hace la misma cantidad de trabajo que el algoritmo
	 secuencial. El objetivo es de *maximizar la eficiencia*.

	 (Nota estas definiciones en la pisara, vamos a usarlas despues.)
**** LEMMA de Brent, Trabajo y Consecuencias
***** PROBLEMA: Calcular Max(A[1,...,N])

     1. Solucion Secuencial

       	* Algoritmo:
	  - $m \leftarrow 1$
	  - for $i\leftarrow 2$ to $n$
	    - if $A[i]>A[m]$ then $m\leftarrow i$
	  - return $A[m]$

       	* Se puede ver como un arbol de evaluacion con una sola rama
	  de largo $n$ y $n$ hojas.

       	* Complejidad: 
	  - tiempo $O(n)$, con $1$ procesador, entonces:
	  - $T^*(n)=n$.

     2. Solucion Parallela con $n$ procesadores

       	* Algoritmo:
	  - $M[p] \leftarrow A[p]$
	  - for $l\leftarrow 0$ to $\lceil \lg p \rceil -1$
	    - if $p \mod 2^{l+1}=0$ y $ p+2^l<n$
	      - $M[p]\leftarrow \max( M[p],M[p+2^l])$
	  - if $p=0$
	    - $max \leftarrow M[0]$

       	* Se puede ver como un arbol balanceado de altura $\lg n$ con
	  $n$ hojas.

       	* Complejidad: 

	  - tiempo $O(\lg n)$ con $n$ procesador, i.e. en nuestra notaciones:
	  - $T(n,n) = \lg n$
	  - $S(n,n) = \frac{n}{\lg n} $
	  - $E(n,n) = \frac{n}{n\lg n} = {1\over\lg n} $

       	* Nota: no se puede hacer mas rapido, pero hay mucho
	  procesadores poco usados: quizas se puede calcular el max
	  en el mismo tiempo, pero usando menos procesadores?

     3. Solucion general con $p$ procesadores

       	* Idea:

	  - reduce la cantidad de procesadores, y hace "load
	    balancing" sobre $n/\lg n$ procesadores.
	  - Divida el input en $n/\lg n$ grupos, 
	  - asigna cada grupo de $\lg n$ elementos a un procesador.
	  - En la primera fase, cada procesador encontra el max de su grupo
	  - En la segunda fase, utiliza el algoritmo precedente.

       	* Complejidad: 

	  - tiempo $O(\lg n)$ con $n$ procesador, i.e. en nuestra notaciones:
	    - $T(n, {n\over\lg n} ) = 2 \lg n \in O(\lg n)$
	      - $T(n,p)= {n\over p} + \lg p$
	    - $S(n,p) = \frac{n}{{n\over p} + \lg p} 
	      = p ( 1 - \frac {p\lg p}{n+p\lg p})
	      \rightarrow p$ si $n\rightarrow \infty$ y p
	    - $E(n,p) = \frac{n}{{n\over\lg n}\lg n} = 1/2 $

       	* Discusion:
	  
	  - El parametro de $\lg n$ procesadores es optima?

	    - Para que? Que significa ser optima? 
	      - en energia
	      - en el contexto donde los procesadores libres pueden
	       	ser usados para otras tareas.
	    - Si, es optimo para la eficiencia, se puede ver
	      estudiando el grafo en funcion de $p$.

	  - Eso es un algoritmo EREW, CREW, o CRCW?

	    - EREW (Exclusive Read. Exclusive Write): no dos
	      procesadores lean o escriben en la misma cedula al
	      mismo tiempo.

	  - Nota: 

	    - Hay un algoritmo CRCW que puede calcular el max en $O(1)$
	      tiempo en paralelo, ilustrando el poder del modelo
	      CRCW (y el costo de las restricciones del modelo EREW)
	      [ REFERENCIA: Section 12.3.2 of "Introduction to Algorithms, A
	      Creative Approach", Udi Manber, p. 382]]
***** LEMA de Brent

     El algoritmo previo illustra un principo mas general, llamado el
     "Lemma de Brent":

     Si un algoritmo 
     - consigue un tiempo T(n,p)=C, entonces 
     - consigue tiempo T(n,p/s)= sC \forall s>1 
     - (bajo algunas condiciones, tal que hay suficientamente memoria
       para cada procesador)

***** DEFINICION "Trabajo"

      - Usando el Lema de Brent, podemos exprimir el rendimiento de
	los algoritmos paralelos con solamente dos medidas:
       
	- $T(n)$, el tiempo del mejor algoritmo paralelo usando
	  cualquier cantidad de procesadores que quiere.

	  Nota las diferencias con
	  + $T^*(n)$, el tiempo del mejor algoritmo secuencial, y 
	  + $T_A(n,n)$, el tiempo del algoritmo $A$ con $n$ procesadores.

	- $W(n)$, la suma del total trabajo ejecutado por todo los
	  procesadores (i.e. superficia del arbol de calculo, a
	  contras de su altura (tiempo) o hancho (cantidad de
	  procesadores).

      - INTERACCION: Cual son estas valores para el algoritmo de Max?
	- $T(n)=$?
	- $W(n)=$?

      - INTERACCION: Puedes ver como desde $T(n)$, $W(n)$ se puede deducir
	las valores de
	- $T(n,p)$? (solucion en el corolario)
	- $S(n,p)$? (trivial desde $T(n,p)$) 
	- $E(n,p)$? (solucion en el corolario)
	  - 

***** COROLARIO

       - Con el lema de Brent podemos obtener:

	 - $$T(n,p) = T(n) + \frac{ W(n) }{ p }$$

	 - $$E(n,p) = \frac{ T^*(n) }{ pT(n) + W(n) }$$

***** EJEMPLO

     * Para el calculo del maximo:
       - $T(n) = \lg n$
       - $W(n) = n$

     * Entonces
       - se puede obtener 
	 - $T_B(n,p) = \lg n + {n \over p}$
	 - $$E(n,p) = \frac{ n }{ p\lg n + n }$$

     * (Nota que eso es solamente una cota superior, nuestro
       algoritmo da un mejor tiempo.)




**** PROBLEMA: Ranking en listas

     1. DEFINICION
	
	- dado una lista, calcula el rango para cada elemento.

	- En el caso de una lista tradicional, no se puede hacer
	  mucho mejor que lineal.

	- Consideramos una lista en un arreglo $A$, 
	  - donde cada elemento $A[i]$ tiene un puntero al siguiente
	    elemento, $N[i]$, y
	  - calculamos su rango $R[i]$ en un arreglo $R$.

     2. DoublingRank()
	- $R[p] \leftarrow 0$
	- if $N[p] =  null$
	  - $R[p] \leftarrow 1$
	- for $d\leftarrow 1$ to $\lceil\lg n\rceil$
	  - if $N[p]\neq NULL$
	    - if $R[N[p]]>0$ 
	      - $R[p] \leftarrow R[N[p]]+2^d$
	    - $N[p] \leftarrow N[N[p]]$

     3. Analisis

	- $T(n) = \lg n$
	- $W(n) = n + W(n/2) \in O(n)$
	- $T(n,p) = T(n) + W(n)/p = \lg n + n/p$
	- $p^*$
	  $T(n) = W(n) / p^*$
	  $p n/\lg n$
	- $E(n,p^*) = \frac{T^*(n)}{ p^* T(n) + W(n) }
		   = \frac{ n }{ n/\lg n \lg n +n} 
		   \in \Theta(1)$

     4. El algoritmo es EREW o CREW?

	- es EREW si los procesadores estan sincronizados, com en
          RAM aqua.


**** PROBLEMA: Prefijos en paralelo ("Parallel Prefix")

    * DEFINICION: Problema "Prefijo en Paralelo"

      Dado $x_1,\ldots, x_n$ y un operador asociativo $\times$,
      calcular 
      - $y_1 = x_1$
      - $y_2 = x_1\times x_2$
      - $y_2 = x_1\times x_2 \times x_3$
      - ...
      - $y_n = x_1\times \ldots \times x_n$

    * Solucion Secuencial

      Hay una solucion obvio en tiempo $O(n)$.

***** Solucion paralela 1

      * Concepto:

	- Hipotesis: sabemos solucionarlo con $n/2$ elementos
	- Caso de base: $n=1$ es simple.
	- Induccion:
	  1. recursivamente calculamos en paralelo:
	     - todos los prefijos de $\{x_1,\ldots,x_{n/2}\}$ con
	       $n/2$ procesadores.
	     - todos los prefijos de $\{x_{n/2},\ldots,x_n\}$ con
	       $n/2$ procesadores.
	  2. en paralelo agregamos $x_{n/2}$ a los prefijos de
	     $\{x_{n/2},\ldots,x_n\}$

      * Observacion: en cual modelo de parallelismo es el ultimo paso?

      * ParallelPrefix1(i,j)
	- if $i_p=j_p$
	  - return x_{i_p}
	- $m_p \leftarrow \lfloor \frac{i_p + j_p}{2} \rfloor$;
	- if $p\leq m$ then
	  - algo( i_p, m_p )
	- else
	  - algo(m+1, j_p)
	  - y_p \leftarrow y_m . y_p

      * Otra forma de escribir el algoritmo (de p. 384 de
        "Introduction to Algorithms, A Creative Approach", Udi
        Manber):

	 + ParallelPrefix1(left,right)
	   - if $(right-left) = 1$
	     - $x[right] \leftarrow x[left] . x[right]$
	   - else
	     - $middle \leftarrow (left+right-1)/2$
	     - do in paralel
	       - ParallelPrefix1(left,middle) \{assigned to $\{P_1 to P_{n/2}\}$\}
	       - ParallelPrefix1(middle+1,right) \{assigned to $\{P_{n/2+1} to P_n\}$\}
	     - for $i \leftarrow middle+1$ to $right$ do in paralel
	       - $x[i] \leftarrow x[middle] . x[i]$


      * Notas:

	- este solucion *no* es EREW (Exclusive Read and Write), porque
	  los procesadores pueden leer $y_m$ al mismo tiempo.
	- este soluciono es CREW (Concurrent Read, Exclusive Write).

       - Complejidad: 

	 - $T_{A_1}(n,n) = 1+ T(n/2,n/2) = \lg n$

	   (El mejor tiempo en paralelo con cualquier cantidad de
	   procesadores.)

	 - $W_{A_1}(n) = n + 2 W_{A_1}(n/2) - n\lg n$

	 - $T_{A_1}(n,p) = T(n) + W_{A_1}(n)/p = \lg n + (n\lg n)/p$

	 - Calculamos $p^*$, la cantidad optima de procesadores
	   para minimizar el tiempo: 
	   - $T(n) = W_{A_1}(n) /p^*$
	   - $p^* = \frac{ W(n) }{ T(n) } = n$

	 - Calculamos la eficiencia

	   - $E_{A_1}(n,p^*) 
	     = \frac{ T^*(n) }{ p^* T(n)+W(n) }
	     = \frac{ n }{ n\lg n } 
	     = {1 \over \lg n}$

	   - es poco eficiente =(

	   - Podriamos tener un algoritmo con 
	     - la eficiencia del algoritmo secuencial 
	     - el tiempo del algoritmo paralelo?


***** Solucion paralela 2: mismo tiempo, mejor eficiencia

      - Idea:
	
	El concepto es de dividir de manera diferente: par y impar
	(en vez de largo o pequeno).

      - Concepto:
	1. Calcular en paralelo $x_{2i-1}.x_{2i}$ en $x_{2i}$
	   para $\forall i, 1\leq i \leq n/2$.
	2. Recursivamente, calcular todos los prefijos de 
	   $E=\{ x_2,x_4,\ldots, x_{2i},\ldots, x_n \}$
	3. Calcular en paralelo los prefijos en posiciones impares,
	   multiplicando los prefijos de $E$ por una sola valor. 

      - algo2(i,j)

	- for $d\leftarrow 1$ to $(\lg n)-1$
	  - if $p=0 \mod 2^{d+1}$
	    - if $p+2^d<n$ 
	      - $x_{p+2^d} \leftarrow x_p . x_{p+2^d}$
	- for $d\leftarrow 1$ to $(\lg n)-1$
	  - if $p=0 \mod 2^{d+1}$
	    - if $p-2^d>0$ 
	      - $x_{p-2^d} \leftarrow x_{p-2^d}.x_p$

      - visualizacion 

	0. 
	1. [0,1]
	2. 
	3. [2,3] [0,3]
	4. 
	5. [4,5]
	6. 
	7. [6,7] [4,7] [0,7]
	8. 
	9.  [8,9]
	10.
	11. [10,11] [8,11]
	12. 
	13. [12,13]
	14. 
	15. [14,15] [12,15] [8,15] [0,15]

      - Notas:
	- Este algoritmo es EREW

      - Analisis
	- $T_2(n) = 2\lg n$
	- $W(n) = n + W(n.2) = n$
	- $T_2(n,p) = T(n) + W(n)/p = 2\lg n + \frac{n}{p}$

	- Calculamos la cantidad optima de procesadores para obtener
	  el tiempo optima:
	  - $T_2(n) = W(n) / p^*$ entonces
	  - $p^* = {W(n) \over T(n)} = {n\over\lg n}$

	- $E_2(n,p^*) = { T^*(n) \over p^* T(n)+ W(n)}  \in O(1)$
     
	 
**** Moralidad del Parallelismo:

     1. Cual es la consecuencia del Lemma de Brent?

	- Concentrarse en $T(n)$ y $E(n)$
	- Pero saber aplicar el Lemma de Brent para programmar $T(n,p)$

     2. Cual (otra) tecnica veamos?
	
	- Mejorar la efficiencia con una parte secuencial (en
          parallelo) del algoritmo.

*** Conclusion Unidad
    * Vimos
      1. Aleatorizacion
      2. Aproximabilidad
      3. Paralelizacion / Distribucio
    * Contexto
      + son *extenciones* del contenido del curso
      + hay muchas otras
	- cryptografia
	- quantum computing
	- parameterized complexity
	- ...
      + La metodologia entre todas tiene una parte en comun:
	- Formalismo
	  - Cotas superiores
	  - Cotas inferiores
	- Adecuacion a la practica.
** CONCLUSION del curso

    * Unidades:

      1. Conceptos basicos
      2. Memoria Secundaria
      3. Tecnicas Avanzadas
      4. Extensiones

    * Temas

      - Implementacion y Experimentacion.

      - cotas inferiores 
	- lemma del ave
	- minimo y maximo de un arreglo
	- busqueda en un arreglo con distintas probabilidades de acceso
	- lemma del minimax (para aleatorizacion)
	- theorema de Yao-von Neuman

      - analisis 
	- en promedio 
	  - de algoritmos deterministicos
	  - de algoritmos aleatorizados
	- "adaptativa" para
	  - torre de Hanoi y "Disk Pile" problema
	  - busqueda ordenada (y codificacion de enteros)
	  - problemas en linea
	- en Memoria Externa
	  - Diccionarios
	  - Colas de Prioridades
	  - Ordenamiento
	- en dominio discreto y finito (afuera del modelo de comparacion)
	  - inter/extra polacion
	  - skiplists
	  - hash
	  - radix sort
	- de algoritmos en linea ("online")
	  - "competitive analysis"
	- de esquemas de aproximacion
	  - "bin packing"
	  - "Vertex Cover"
	  - "Traveling Salesman"
	  - "Backpack"
	- amortizada
	  - enumeracion de enteros en binario
	  - min max
	- en el modelo PRAM
	  - max
	  - ranking en listas
	  - prefijos


	    
	    
	    
	    
   
	    
	    
	    
